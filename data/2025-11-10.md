<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 47]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs](https://arxiv.org/abs/2511.04727)
*Ali Faraz,Akash,Shaharukh Khan,Raja Kolla,Akshat Patidar,Suranjan Goswami,Abhinav Ravi,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 本文提出了IndicVisionBench，首个聚焦印度次大陆的大规模多语言视觉语言模型评测基准，涵盖13个文化主题、10种印度语言及英语，包含约5千张图像和3.7万以上问答对，用于评估当前视觉语言模型在多元文化和多语言环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的评测基准大多以西方为中心，缺乏对文化多样性和多语言场景的充分评估，因此需要一个聚焦非西方文化的基准来揭示模型在不同语言和文化背景下的性能差异。

Method: 构建了一个包含英语和10种印度语言的大规模多模态基准IndicVisionBench，覆盖OCR、多模态机器翻译（MMT）和视觉问答（VQA）三项任务，共6类问题类型，并发布了一个跨10种印度语言的平行语料库。对8种主流视觉语言模型进行了系统评估。

Result: 实验表明现有视觉语言模型在印度语言和文化场景下存在显著性能下降，暴露出其在多语言和跨文化理解上的局限性；同时发布的数据集为分析模型中的文化和语言偏见提供了新资源。

Conclusion: IndicVisionBench通过强调文化多样性和多语言性，建立了一个可复现的评估框架，推动更包容的多模态人工智能研究发展。

Abstract: Vision-language models (VLMs) have demonstrated impressive generalization
across multimodal tasks, yet most evaluation benchmarks remain Western-centric,
leaving open questions about their performance in culturally diverse and
multilingual settings. To address this gap, we introduce IndicVisionBench, the
first large-scale benchmark centered on the Indian subcontinent. Covering
English and 10 Indian languages, our benchmark spans 3 multimodal tasks,
including Optical Character Recognition (OCR), Multimodal Machine Translation
(MMT), and Visual Question Answering (VQA), covering 6 kinds of question types.
Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across
13 culturally grounded topics. In addition, we release a paired parallel corpus
of annotations across 10 Indic languages, creating a unique resource for
analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum
of 8 models, from proprietary closed-source systems to open-weights medium and
large-scale models. Our experiments reveal substantial performance gaps,
underscoring the limitations of current VLMs in culturally diverse contexts. By
centering cultural diversity and multilinguality, IndicVisionBench establishes
a reproducible evaluation framework that paves the way for more inclusive
multimodal research.

</details>


### [2] [CPO: Condition Preference Optimization for Controllable Image Generation](https://arxiv.org/abs/2511.04753)
*Zonglin Lyu,Ming Li,Xinxin Liu,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的条件偏好优化方法（CPO），通过在控制信号上进行偏好学习而非生成图像，提升了文本到图像生成中的可控性，相比ControlNet++在多种控制类型上显著降低了误差率。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有方法如ControlNet++在优化过程中忽略高噪声时间步以及引入近似误差的问题，并解决直接偏好优化（DPO）中因生成不确定性导致的图像对可比性问题，本文旨在提升文本到图像生成模型在所有时间步上的可控性。

Method: 提出条件偏好优化（CPO），构建优劣控制信号对（\mathbf{c}^{w} 和 \mathbf{c}^{l}），在控制条件上进行偏好学习，避免生成图像中的干扰因素，实现低方差、高效计算的训练目标。

Result: CPO在理论上具有比DPO更低的对比损失方差，在实验中显著优于ControlNet++：在分割任务上错误率降低超过10%，人体姿态上降低70%-80%，边缘和深度图上持续降低2%-5%；同时减少了数据集构建的计算与存储开销。

Conclusion: CPO是一种更高效、稳定且通用的可控图像生成训练框架，通过在控制条件空间进行偏好学习，有效提升了生成结果的可控性，为未来相关研究提供了新方向。

Abstract: To enhance controllability in text-to-image generation, ControlNet introduces
image-based control signals, while ControlNet++ improves pixel-level cycle
consistency between generated images and the input control signal. To avoid the
prohibitive cost of back-propagating through the sampling process, ControlNet++
optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step
approximation, which not only ignores the contribution of high-noise timesteps
but also introduces additional approximation errors. A straightforward
alternative for optimizing controllability across all timesteps is Direct
Preference Optimization (DPO), a fine-tuning method that increases model
preference for more controllable images ($I^{w}$) over less controllable ones
($I^{l}$). However, due to uncertainty in generative models, it is difficult to
ensure that win--lose image pairs differ only in controllability while keeping
other factors, such as image quality, fixed. To address this, we propose
performing preference learning over control conditions rather than generated
images. Specifically, we construct winning and losing control signals,
$\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer
$\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference
Optimization} (CPO), eliminates confounding factors and yields a low-variance
training objective. Our approach theoretically exhibits lower contrastive loss
variance than DPO and empirically achieves superior results. Moreover, CPO
requires less computation and storage for dataset curation. Extensive
experiments show that CPO significantly improves controllability over the
state-of-the-art ControlNet++ across multiple control types: over $10\%$ error
rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent
$2$--$5\%$ reductions in edge and depth maps.

</details>


### [3] [DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation](https://arxiv.org/abs/2511.04766)
*Dhenenjay Yadav,Rohan Sawai*

Main category: cs.CV

TL;DR: 本文提出了DARN，一种针对卫星图像异质性问题的新型解码器架构，通过任务复杂度预测、自适应Dropout调制和动态通道门控，在全微调和高效适应两种范式下均表现出卓越性能，显著提升了地理空间分析中的模型鲁棒性、泛化能力和 Minority 类别表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型在地理空间分析中面临适应困难的问题，尤其是标准适配方法采用固定正则化策略，无法应对卫星图像的高度异质性。

Method: 提出Dynamic Adaptive Regularization Networks (DARN)，包含三个核心组件：轻量级任务复杂度预测器（TCP）、自适应Dropout调制（ADM）和动态容量门控（DCG），并从理论上论证其优化收敛性和信息瓶颈机制。

Result: 在全微调设置下，DARN在GeoBench基准上达到86.66% mIoU，超越先前SOTA 5.56个百分点；在高效适应（冻结主干）下，在Sen1Floods11上达到90.5% mIoU，并在OOD泛化、抗干扰鲁棒性和少数类性能方面显著优于现有方法。

Conclusion: DARN为利用基础模型进行地理空间分析提供了一种更智能、更鲁棒且高效的解决方案，适用于实际部署中的关键应用场景。

Abstract: Foundation models (FMs) offer powerful representations for geospatial
analysis, but adapting them effectively remains challenging. Standard
adaptation methods, whether full fine-tuning or efficient frozen-backbone
approaches, typically employ decoders with fixed regularization strategies,
failing to account for the significant heterogeneity in satellite imagery. We
introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder
architecture designed to address this limitation. DARN integrates three key
innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates
per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically
adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and
(3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide
theoretical justifications linking DARN's optimization to stationary point
convergence and its mechanism to adaptive information bottlenecks. Empirically,
DARN demonstrates exceptional performance across both major adaptation
paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new
state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp
over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves
SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering
substantial advantages crucial for real-world deployment: superior
out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms),
enhanced robustness (17% relative reduction in corruption error), and improved
performance on minority classes. DARN offers a more intelligent, robust, and
efficient approach to leveraging FMs in critical geospatial applications.

</details>


### [4] [Global 3D Reconstruction of Clouds & Tropical Cyclones](https://arxiv.org/abs/2511.04773)
*Shirin Ermis,Cesar Aybar,Lilli Freischem,Stella Girtsou,Kyriaki-Margarita Bintsi,Emiliano Diaz Salas-Porras,Michael Eisinger,William Jones,Anna Jungbluth,Benoit Tremblay*

Main category: cs.CV

TL;DR: 提出一种基于预训练-微调框架的机器学习方法，首次实现从多卫星数据生成全球瞬时三维云图，精确重建强热带气旋的三维结构。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏探测热带气旋（TC）结构的卫星观测数据以及难以解析与TC增强相关的云特性，准确预测TC仍具挑战性。现有机器学习方法在TC罕见区域受限且对强风暴验证不足。

Method: 采用预训练-微调的机器学习框架，利用具有全球覆盖的多颗卫星数据，将二维卫星图像转换为包含关键云特性的三维云图，并在自建的TC数据集上进行评估。

Result: 首次实现了全球范围内的瞬时三维云图生成，能够精确重建强风暴的三维结构，即使在无观测数据时也能提供有效估计。

Conclusion: 该模型不仅扩展了现有卫星观测能力，还为理解热带气旋增强机制和改进预报提供了重要工具。

Abstract: Accurate forecasting of tropical cyclones (TCs) remains challenging due to
limited satellite observations probing TC structure and difficulties in
resolving cloud properties involved in TC intensification. Recent research has
demonstrated the capabilities of machine learning methods for 3D cloud
reconstruction from satellite observations. However, existing approaches have
been restricted to regions where TCs are uncommon, and are poorly validated for
intense storms. We introduce a new framework, based on a
pre-training--fine-tuning pipeline, that learns from multiple satellites with
global coverage to translate 2D satellite imagery into 3D cloud maps of
relevant cloud properties. We apply our model to a custom-built TC dataset to
evaluate performance in the most challenging and relevant conditions. We show
that we can - for the first time - create global instantaneous 3D cloud maps
and accurately reconstruct the 3D structure of intense storms. Our model not
only extends available satellite observations but also provides estimates when
observations are missing entirely. This is crucial for advancing our
understanding of TC intensification and improving forecasts.

</details>


### [5] [3D Gaussian Point Encoders](https://arxiv.org/abs/2511.04797)
*Jim James,Ben Wilson,Simon Lucey,James Hays*

Main category: cs.CV

TL;DR: 本文提出了3D高斯点编码器，一种基于学习到的3D高斯混合的显式每点嵌入方法，相比传统的PointNet在速度、参数效率和内存使用上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 为了克服隐式表示（如PointNet）在3D识别任务中的局限性，并探索更高效、更快的显式几何表示方法。

Method: 开发了基于自然梯度和从PointNet蒸馏的优化技术，以端到端的方式学习3D高斯编码器，并采用计算几何启发式方法进一步加速。

Result: 3D高斯点编码器比同等精度的PointNet快2.7倍，内存减少46%，FLOPs减少88%；在Mamba3D中也表现出1.27倍的速度提升，内存和FLOPs分别降低42%和54%。

Conclusion: 3D高斯点编码器是一种轻量级且高效的3D表示方法，适用于CPU设备并能实现高帧率运行。

Abstract: In this work, we introduce the 3D Gaussian Point Encoder, an explicit
per-point embedding built on mixtures of learned 3D Gaussians. This explicit
geometric representation for 3D recognition tasks is a departure from widely
used implicit representations such as PointNet. However, it is difficult to
learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We
develop optimization techniques based on natural gradients and distillation
from PointNets to find a Gaussian Basis that can reconstruct PointNet
activations. The resulting 3D Gaussian Point Encoders are faster and more
parameter efficient than traditional PointNets. As in the 3D reconstruction
literature where there has been considerable interest in the move from implicit
(e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can
take advantage of computational geometry heuristics to accelerate 3D Gaussian
Point Encoders further. We extend filtering techniques from 3D Gaussian
Splatting to construct encoders that run 2.7 times faster as a comparable
accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore,
we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component
in Mamba3D, running 1.27 times faster and achieving a reduction in memory and
FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight
enough to achieve high framerates on CPU-only devices.

</details>


### [6] [Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose](https://arxiv.org/abs/2511.04803)
*Shuo Zhao,Jianxu Chen*

Main category: cs.CV

TL;DR: 本研究以Cellpose为例，系统分析了生物医学图像分割中训练数据冗余和跨域迁移导致的灾难性遗忘问题。提出数据量化（DQ）策略，发现仅用10%数据即可达到性能饱和，并通过潜空间分析验证DQ能更好捕捉特征多样性；进一步研究表明，引入5-10%源域数据回放可有效缓解遗忘，且合理的训练域顺序有助于提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决通用生物医学图像分割模型中存在的训练数据冗余和跨域微调中的灾难性遗忘问题，探索更高效、可持续的学习策略。

Method: 提出数据量化（DQ）策略构建紧凑且多样化的训练子集；使用MAE嵌入和t-SNE进行潜空间分析；开展跨域微调实验，评估不同比例源域数据回放对性能的影响；研究多阶段迁移中训练域顺序的作用。

Result: 在Cyto数据集上，仅使用10%数据即可使分割性能饱和，表明存在显著数据冗余；DQ选择的样本比随机采样具有更高的特征多样性；跨域微调导致源域性能明显下降；重新引入5-10%源域数据可通过回放有效恢复源域性能；全量回放可能阻碍目标域适应；合理的域训练顺序可改善泛化并减少遗忘。

Conclusion: 高效的生物医学图像分割模型训练不仅需要精简的数据子集，还需考虑保留源域知识的学习策略和合理的域训练顺序，强调了以数据为中心设计的重要性。

Abstract: Generalist biomedical image segmentation models such as Cellpose are
increasingly applied across diverse imaging modalities and cell types. However,
two critical challenges remain underexplored: (1) the extent of training data
redundancy and (2) the impact of cross domain transfer on model retention. In
this study, we conduct a systematic empirical analysis of these challenges
using Cellpose as a case study. First, to assess data redundancy, we propose a
simple dataset quantization (DQ) strategy for constructing compact yet diverse
training subsets. Experiments on the Cyto dataset show that image segmentation
performance saturates with only 10% of the data, revealing substantial
redundancy and potential for training with minimal annotations. Latent space
analysis using MAE embeddings and t-SNE confirms that DQ selected patches
capture greater feature diversity than random sampling. Second, to examine
catastrophic forgetting, we perform cross domain finetuning experiments and
observe significant degradation in source domain performance, particularly when
adapting from generalist to specialist domains. We demonstrate that selective
DQ based replay reintroducing just 5-10% of the source data effectively
restores source performance, while full replay can hinder target adaptation.
Additionally, we find that training domain sequencing improves generalization
and reduces forgetting in multi stage transfer. Our findings highlight the
importance of data centric design in biomedical image segmentation and suggest
that efficient training requires not only compact subsets but also retention
aware learning strategies and informed domain ordering. The code is available
at https://github.com/MMV-Lab/biomedseg-efficiency.

</details>


### [7] [An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention](https://arxiv.org/abs/2511.04811)
*Shuo Zhao,Yu Zhou,Jianxu Chen*

Main category: cs.CV

TL;DR: 提出一种结合主动学习和伪标签的数据中心AI工作流，用于生物医学图像分割，显著减少人工标注需求并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在噪声数据上表现不佳，深度学习模型如nnU-Net需要大量标注数据进行交叉验证，而大基础模型虽具零样本泛化能力但特定数据集上性能有限。

Method: 利用基础模型生成伪标签，用于nnU-Net的自配置，并通过核心集选择进行少量人工标注，实现模型的有效微调。

Result: 该方法显著减少了人工标注的需求，同时在生物医学图像分割任务中保持了有竞争力的性能。

Conclusion: 所提出的AI工作流为生物医学研究人员提供了一种低标注成本、高性能的图像分割解决方案。

Abstract: Biomedical image segmentation is critical for precise structure delineation
and downstream analysis. Traditional methods often struggle with noisy data,
while deep learning models such as U-Net have set new benchmarks in
segmentation performance. nnU-Net further automates model configuration, making
it adaptable across datasets without extensive tuning. However, it requires a
substantial amount of annotated data for cross-validation, posing a challenge
when only raw images but no labels are available. Large foundation models offer
zero-shot generalizability, but may underperform on specific datasets with
unique characteristics, limiting their direct use for analysis. This work
addresses these bottlenecks by proposing a data-centric AI workflow that
leverages active learning and pseudo-labeling to combine the strengths of
traditional neural networks and large foundation models while minimizing human
intervention. The pipeline starts by generating pseudo-labels from a foundation
model, which are then used for nnU-Net's self-configuration. Subsequently, a
representative core-set is selected for minimal manual annotation, enabling
effective fine-tuning of the nnU-Net model. This approach significantly reduces
the need for manual annotations while maintaining competitive performance,
providing an accessible solution for biomedical researchers to apply
state-of-the-art AI techniques in their segmentation tasks. The code is
available at https://github.com/MMV-Lab/AL_BioMed_img_seg.

</details>


### [8] [Geometry Denoising with Preferred Normal Vectors](https://arxiv.org/abs/2511.04848)
*Manuel Weiß,Lukas Baumgärtner,Roland Herzog,Stephan Schmidt*

Main category: cs.CV

TL;DR: 提出了一种基于表面法向量先验知识的几何去噪新方法，通过标签向量进行法向量相似性分割，并结合全变分正则化与分裂Bregman优化框架实现高效去噪。


<details>
  <summary>Details</summary>
Motivation: 传统几何去噪方法缺乏对表面结构先验信息的有效利用，导致在保留特征和去除噪声之间难以平衡。因此，需要引入关于表面法向量的先验知识来提升去噪效果。

Method: 引入一组预定义的偏好法向量（标签向量），将去噪过程中的法向量映射到最相似的标签向量上，形成嵌入式的分割问题；采用全变分项进行正则化，并通过分裂Breg曼（ADMM）方法求解优化问题，顶点更新步骤基于二阶形状微积分。

Result: 该方法能有效保持几何模型的尖锐特征和整体结构，在噪声去除的同时实现精确的表面重建，并通过数值实验验证了其优于现有方法的性能。

Conclusion: 基于法向量标签的分割式去噪框架结合变分优化与形状微积分，为几何去噪提供了新的有效范式。

Abstract: We introduce a new paradigm for geometry denoising using prior knowledge
about the surface normal vector. This prior knowledge comes in the form of a
set of preferred normal vectors, which we refer to as label vectors. A
segmentation problem is naturally embedded in the denoising process. The
segmentation is based on the similarity of the normal vector to the elements of
the set of label vectors. Regularization is achieved by a total variation term.
We formulate a split Bregman (ADMM) approach to solve the resulting
optimization problem. The vertex update step is based on second-order shape
calculus.

</details>


### [9] [Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications](https://arxiv.org/abs/2511.04871)
*Gabriel Girard,Manon Edde,Félix Dumais,Yoan David,Matthieu Dumont,Guillaume Theaud,Jean-Christophe Houde,Arnaud Boré,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.CV

TL;DR: 提出了一种名为Clinical-ComBAT的新方法，用于在真实临床场景中对多中心DW-MRI数据进行去批次效应处理，相比传统ComBAT更具灵活性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统ComBAT方法依赖线性协变量关系、同质人群、固定且样本量大的站点，限制了其在临床中的应用，因此需要一种更灵活、适用于小样本和动态新增站点的去批次方法。

Method: 提出Clinical-ComBAT，采用非线性多项式数据模型，以规范站点为参考进行各站点独立标准化，引入可适应小样本的方差先验、超参数调优和拟合优度评估指标。

Result: 在模拟和真实数据上验证表明，该方法能更好对齐扩散指标，提升多中心数据一致性，并增强在规范建模中的适用性。

Conclusion: Clinical-ComBAT克服了传统方法的局限，支持动态扩展和小样本站点，在真实临床环境中具有更强的实用性与鲁棒性。

Abstract: Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps
are effective for assessing neurodegenerative diseases and microstructural
properties of white matter in large number of brain conditions. However, DW-MRI
inherently limits the combination of data from multiple acquisition sites
without harmonization to mitigate scanner-specific biases. While the widely
used ComBAT method reduces site effects in research, its reliance on linear
covariate relationships, homogeneous populations, fixed site numbers, and well
populated sites constrains its clinical use. To overcome these limitations, we
propose Clinical-ComBAT, a method designed for real-world clinical scenarios.
Clinical-ComBAT harmonizes each site independently, enabling flexibility as new
data and clinics are introduced. It incorporates a non-linear polynomial data
model, site-specific harmonization referenced to a normative site, and variance
priors adaptable to small cohorts. It further includes hyperparameter tuning
and a goodness-of-fit metric for harmonization assessment. We demonstrate its
effectiveness on simulated and real data, showing improved alignment of
diffusion metrics and enhanced applicability for normative modeling.

</details>


### [10] [Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects](https://arxiv.org/abs/2511.04872)
*James Ndubuisi,Fernando Auat,Marta Vallejo*

Main category: cs.CV

TL;DR: 本研究比较了Swin Transformer与传统ResNet模型在耳部疾病诊断中的准确性，发现初始高准确率因数据泄露问题被高估，修正后性能显著下降，强调了医学机器学习中严谨数据处理的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于耳鼻喉专科医生存在27%的误诊率，提高耳部疾病诊断准确性至关重要，因此需要评估先进模型如视觉Transformer在此类医学诊断中的有效性。

Method: 使用智利大学附属医院的真实耳镜视频数据集，基于Laplacian和Shannon熵筛选帧并去除空白帧；采用Swin v1、Swin v2和ResNet模型进行对比实验，在发现数据泄露问题后重新划分数据以纠正评估结果。

Result: 修正前Swin v1和Swin v2分别达到100%和99.1%的准确率，优于ResNet的99.5%；但在修复数据泄露问题后，Swin v1和Swin v2准确率降至83%，ResNet为82%。

Conclusion: 尽管视觉Transformer在耳病诊断中展现出潜力，但数据处理的质量对模型性能影响巨大；构建可靠的医学AI模型需在先进架构与严谨的数据预处理之间取得平衡。

Abstract: This study evaluates the efficacy of vision transformer models, specifically
Swin transformers, in enhancing the diagnostic accuracy of ear diseases
compared to traditional convolutional neural networks. With a reported 27%
misdiagnosis rate among specialist otolaryngologists, improving diagnostic
accuracy is crucial. The research utilised a real-world dataset from the
Department of Otolaryngology at the Clinical Hospital of the Universidad de
Chile, comprising otoscopic videos of ear examinations depicting various middle
and external ear conditions. Frames were selected based on the Laplacian and
Shannon entropy thresholds, with blank frames removed. Initially, Swin v1 and
Swin v2 transformer models achieved accuracies of 100% and 99.1%, respectively,
marginally outperforming the ResNet model (99.5%). These results surpassed
metrics reported in related studies. However, the evaluation uncovered a
critical data leakage issue in the preprocessing step, affecting both this
study and related research using the same raw dataset. After mitigating the
data leakage, model performance decreased significantly. Corrected accuracies
were 83% for both Swin v1 and Swin v2, and 82% for the ResNet model. This
finding highlights the importance of rigorous data handling in machine learning
studies, especially in medical applications. The findings indicate that while
vision transformers show promise, it is essential to find an optimal balance
between the benefits of advanced model architectures and those derived from
effective data preprocessing. This balance is key to developing a reliable
machine learning model for diagnosing ear diseases.

</details>


### [11] [A benchmark multimodal oro-dental dataset for large vision-language models](https://arxiv.org/abs/2511.04948)
*Haoxin Lv,Ijazul Haq,Jin Du,Jiaxin Ma,Binnian Zhu,Xiaobing Dang,Chaoan Liang,Ruxu Du,Yingjie Zhang,Muhammad Saqib*

Main category: cs.CV

TL;DR: 本文介绍了一个包含8775次牙科检查的大规模多模态数据集，涵盖5万张口腔图像、8056张X光片及详细文本记录，并基于该数据集微调视觉-语言模型，显著提升了口腔疾病分类与诊断报告生成的性能。


<details>
  <summary>Details</summary>
Motivation: 推动人工智能在口腔医疗中的应用需要能够反映临床复杂性的大规模多模态数据集，现有数据难以满足这一需求。

Method: 收集了2018至2025年间4800名患者的临床数据，构建多模态数据集，并对Qwen-VL 3B和7B等大模型进行微调，评估其在六类口腔异常分类和诊断报告生成任务上的表现。

Result: 微调后的模型在两项任务上均显著优于基础模型和GPT-4o，验证了数据集的有效性和实用性。

Conclusion: 该公开数据集为AI牙科研究提供了重要资源，有助于推动基于多模态数据的智能口腔健康解决方案发展。

Abstract: The advancement of artificial intelligence in oral healthcare relies on the
availability of large-scale multimodal datasets that capture the complexity of
clinical practice. In this paper, we present a comprehensive multimodal
dataset, comprising 8775 dental checkups from 4800 patients collected over
eight years (2018-2025), with patients ranging from 10 to 90 years of age. The
dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual
records, including diagnoses, treatment plans, and follow-up notes. The data
were collected under standard ethical guidelines and annotated for
benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large
vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks:
classification of six oro-dental anomalies and generation of complete
diagnostic reports from multimodal inputs. We compared the fine-tuned models
with their base counterparts and GPT-4o. The fine-tuned models achieved
substantial gains over these baselines, validating the dataset and underscoring
its effectiveness in advancing AI-driven oro-dental healthcare solutions. The
dataset is publicly available, providing an essential resource for future
research in AI dentistry.

</details>


### [12] [DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning](https://arxiv.org/abs/2511.04949)
*Tharindu Fernando,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 本文提出了一种基于高维潜在空间表示和多智能体对抗强化学习（MAARL）的新型深度学习框架，用于鲁棒且自适应的深伪检测水印技术，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的被动式深伪检测方法难以泛化到新型深伪内容，而主动式水印技术在抗良性失真与敏感恶意篡改之间难以平衡。

Method: 提出一种可学习的潜在空间水印嵌入器，并结合多智能体对抗强化学习（MAARL）范式，通过与模拟良性和恶意图像操作的对抗攻击者交互，动态优化水印的鲁棒性与脆弱性平衡。

Result: 在CelebA和CelebA-HQ数据集上实验表明，该方法在复杂操作场景下分别比现有最优方法提升超过4.5%和5.3%。

Conclusion: 所提框架能有效提升水印在面对各类图像操作时的鲁棒性与安全性，为高质合成媒体的可信识别提供了新思路。

Abstract: Rapid advances in generative AI have led to increasingly realistic deepfakes,
posing growing challenges for law enforcement and public trust. Existing
passive deepfake detectors struggle to keep pace, largely due to their
dependence on specific forgery artifacts, which limits their ability to
generalize to new deepfake types. Proactive deepfake detection using watermarks
has emerged to address the challenge of identifying high-quality synthetic
media. However, these methods often struggle to balance robustness against
benign distortions with sensitivity to malicious tampering. This paper
introduces a novel deep learning framework that harnesses high-dimensional
latent space representations and the Multi-Agent Adversarial Reinforcement
Learning (MAARL) paradigm to develop a robust and adaptive watermarking
approach. Specifically, we develop a learnable watermark embedder that operates
in the latent space, capturing high-level image semantics, while offering
precise control over message encoding and extraction. The MAARL paradigm
empowers the learnable watermarking agent to pursue an optimal balance between
robustness and fragility by interacting with a dynamic curriculum of benign and
malicious image manipulations simulated by an adversarial attacker agent.
Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that
our method consistently outperforms state-of-the-art approaches, achieving
improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under
challenging manipulation scenarios.

</details>


### [13] [CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting](https://arxiv.org/abs/2511.04951)
*Hexu Zhao,Xiwen Min,Xiaoteng Liu,Moonjun Gong,Yiming Li,Ang Li,Saining Xie,Jinyang Li,Aurojit Panda*

Main category: cs.CV

TL;DR: 提出CLM系统，通过将高斯分布卸载到CPU内存并在需要时加载到GPU，实现单个消费级GPU上渲染大规模3D场景，显著降低内存需求并保持高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点阵（3DGS）在渲染质量和速度上表现优异，但其高内存需求限制了在大规模或复杂场景中的应用，尤其是在消费级GPU上。

Method: 设计CLM系统，利用3DGS的内存访问模式，采用新颖的卸载策略，将高斯数据存储在CPU内存中，并通过流水线机制重叠GPU与CPU之间的通信、计算过程，同时减少通信量。

Result: 在单个RTX4090 GPU上成功渲染包含1亿个高斯的大规模场景，实现了最先进的重建质量，且性能开销低。

Conclusion: CLM使3DGS能够在消费级硬件上高效渲染大型场景，推动其在实际应用中的可扩展性。

Abstract: 3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis
approach due to its fast rendering time, and high-quality output. However,
scaling 3DGS to large (or intricate) scenes is challenging due to its large
memory requirement, which exceed most GPU's memory capacity. In this paper, we
describe CLM, a system that allows 3DGS to render large scenes using a single
consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU
memory, and loading them into GPU memory only when necessary. To reduce
performance and communication overheads, CLM uses a novel offloading strategy
that exploits observations about 3DGS's memory access pattern for pipelining,
and thus overlap GPU-to-CPU communication, GPU computation and CPU computation.
Furthermore, we also exploit observation about the access pattern to reduce
communication volume. Our evaluation shows that the resulting implementation
can render a large scene that requires 100 million Gaussians on a single
RTX4090 and achieve state-of-the-art reconstruction quality.

</details>


### [14] [GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder](https://arxiv.org/abs/2511.04977)
*Heng Er Metilda Chee,Jiayin Wang,Zhiqiang Guo,Weizhi Ma,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了贴纸语义相似性任务的定义，并发布了首个基准数据集Triple-S，同时提出了一种轻量且通用的贴纸编码模型GSE，在多个下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于贴纸内容高度多样化和符号化，现有的视觉和多模态预训练模型难以捕捉其细微语义，因此需要专门的任务定义、评估基准和专用模型来推动贴纸理解的研究。

Method: 作者定义了贴纸语义相似性任务，构建了包含905对人工标注正负样本的Triple-S基准数据集，并提出了通用贴纸编码器（GSE），利用Triple-S和其他数据集学习鲁棒的贴纸嵌入表示。

Result: 实验表明现有模型在贴纸语义理解上表现不佳，而GSE在未见贴纸上表现出优越性能，并在情感分类和贴纸-贴纸检索等下游任务中取得良好效果。

Conclusion: 通过发布Triple-S基准和GSE模型，为贴纸理解、检索及多模态内容生成提供了标准化评估工具和有效的嵌入方法，推动了该领域的研究发展。

Abstract: Stickers have become a popular form of visual communication, yet
understanding their semantic relationships remains challenging due to their
highly diverse and symbolic content. In this work, we formally {define the
Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark
for this task, consisting of 905 human-annotated positive and negative sticker
pairs. Through extensive evaluation, we show that existing pretrained vision
and multimodal models struggle to capture nuanced sticker semantics. To address
this, we propose the {General Sticker Encoder (GSE)}, a lightweight and
versatile model that learns robust sticker embeddings using both Triple-S and
additional datasets. GSE achieves superior performance on unseen stickers, and
demonstrates strong results on downstream tasks such as emotion classification
and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we
provide standardized evaluation tools and robust embeddings, enabling future
research in sticker understanding, retrieval, and multimodal content
generation. The Triple-S benchmark and GSE have been publicly released and are
available here.

</details>


### [15] [Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings](https://arxiv.org/abs/2511.05017)
*Aakriti Agrawal,Gouthaman KV,Rohith Aralikatti,Gauri Jagatap,Jiaxin Yuan,Vijay Kamarshi,Andrea Fanelli,Furong Huang*

Main category: cs.CV

TL;DR: 本文指出当前视觉-语言模型（LVLM）架构中存在对语言模态的固有偏差，提出通过平均池化视觉特征来优化文本嵌入的方法，有效改善了视觉定位并减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 发现现有LVLM架构因简单拼接视觉和文本嵌入而导致语言模态偏倚，进而引发视觉接地不良和幻觉问题，因此需要解决模态不平衡问题。

Method: 提出一种简单而有效的方法：通过引入平均池化的视觉特征来优化文本嵌入，从而实现更均衡的跨模态融合。

Result: 该方法在多个基准测试上显著提升了视觉接地能力，并明显减少了模型的幻觉现象，验证了优化文本嵌入的有效性。

Conclusion: 通过对文本嵌入进行视觉信息增强，可以有效缓解LVLM中的模态偏差问题，为减少幻觉提供了新思路，未来可探索更复杂的融合策略。

Abstract: In this work, we identify an inherent bias in prevailing LVLM architectures
toward the language modality, largely resulting from the common practice of
simply appending visual embeddings to the input text sequence. To address this,
we propose a simple yet effective method that refines textual embeddings by
integrating average-pooled visual features. Our approach demonstrably improves
visual grounding and significantly reduces hallucinations on established
benchmarks. While average pooling offers a straightforward, robust, and
efficient means of incorporating visual information, we believe that more
sophisticated fusion methods could further enhance visual grounding and
cross-modal alignment. Given that the primary focus of this work is to
highlight the modality imbalance and its impact on hallucinations -- and to
show that refining textual embeddings with visual information mitigates this
issue -- we leave exploration of advanced fusion strategies for future work.

</details>


### [16] [Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation](https://arxiv.org/abs/2511.05034)
*Jing Jin,Xu Liu,Te Gao,Zhihong Shi,Yixiong Liang,Ruiqing Zheng,Hulin Kuang,Min Zeng,Shichao Kan*

Main category: cs.CV

TL;DR: 提出了一种基于动态残差编码和滑动级别对比学习的全切片图像（WSI）表示方法（DRE-SLCL），通过引入记忆库和残差编码机制，实现端到端的WSI表示学习，在癌症分型、识别和突变预测任务中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于全切片图像包含大量图像块，传统端到端训练因GPU内存限制难以处理所有图像块，因此需要一种高效且可扩展的方法来学习WSI的整体表示。

Method: 采用记忆库存储所有WSI中图像块的特征；在每个训练批次中，对每张WSI随机采样部分图像块并提取其特征，同时从记忆库中检索同一WSI的其他块特征；使用残差编码技术融合两类特征生成每张WSI的表示；最后基于WSI表示及其病理报告计算滑动级别对比损失。

Result: 在癌症分型、癌症识别和突变预测三个任务上的实验表明，所提DRE-SLCL方法优于现有方法，验证了其在多种下游任务中的有效性与鲁棒性。

Conclusion: DRE-SLCL通过结合记忆库机制和残差编码，实现了高效的端到端WSI表示学习，有效缓解了GPU内存限制带来的挑战，提升了多任务病理分析性能。

Abstract: Whole Slide Image (WSI) representation is critical for cancer subtyping,
cancer recognition and mutation prediction.Training an end-to-end WSI
representation model poses significant challenges, as a standard gigapixel
slide can contain tens of thousands of image tiles, making it difficult to
compute gradients of all tiles in a single mini-batch due to current GPU
limitations. To address this challenge, we propose a method of dynamic residual
encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI
representation. Our approach utilizes a memory bank to store the features of
tiles across all WSIs in the dataset. During training, a mini-batch usually
contains multiple WSIs. For each WSI in the batch, a subset of tiles is
randomly sampled and their features are computed using a tile encoder. Then,
additional tile features from the same WSI are selected from the memory bank.
The representation of each individual WSI is generated using a residual
encoding technique that incorporates both the sampled features and those
retrieved from the memory bank. Finally, the slide-level contrastive loss is
computed based on the representations and histopathology reports ofthe WSIs
within the mini-batch. Experiments conducted over cancer subtyping, cancer
recognition, and mutation prediction tasks proved the effectiveness of the
proposed DRE-SLCL method.

</details>


### [17] [Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance](https://arxiv.org/abs/2511.05038)
*Zhengxuan Li,Qinhui Yang,Yiyu Zhuang,Chuan Guo,Xinxin Zuo,Xiaoxiao Long,Yao Yao,Xun Cao,Qiu Shen,Hao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Pressure2Motion的新方法，能够根据地面压力序列和文本提示生成人体动作，无需特殊设备，适用于保护隐私、低光和低成本场景。


<details>
  <summary>Details</summary>
Motivation: 由于压力信号到全身动作的映射具有高度不确定性，传统动作捕捉方法受限于设备和环境，因此需要一种更灵活、低成本且保护隐私的解决方案。

Method: 提出一种基于压力特征输入和文本提示引导的生成模型，采用双层特征提取器解析压力数据，并结合分层扩散模型生成大尺度运动轨迹和细微姿态调整。

Result: 实验表明该方法能生成高保真、物理合理的动作，在该任务上达到了新的SOTA水平，并建立了首个MPL基准。

Conclusion: Pressure2Motion是首个结合压力数据与语言先验进行动作生成的开创性工作，为无传感器动作捕捉提供了有效方案。

Abstract: We present Pressure2Motion, a novel motion capture algorithm that synthesizes
human motion from a ground pressure sequence and text prompt. It eliminates the
need for specialized lighting setups, cameras, or wearable devices, making it
suitable for privacy-preserving, low-light, and low-cost motion capture
scenarios. Such a task is severely ill-posed due to the indeterminate nature of
the pressure signals to full-body motion. To address this issue, we introduce
Pressure2Motion, a generative model that leverages pressure features as input
and utilizes a text prompt as a high-level guiding constraint. Specifically,
our model utilizes a dual-level feature extractor that accurately interprets
pressure data, followed by a hierarchical diffusion model that discerns
broad-scale movement trajectories and subtle posture adjustments. Both the
physical cues gained from the pressure sequence and the semantic guidance
derived from descriptive texts are leveraged to guide the motion generation
with precision. To the best of our knowledge, Pressure2Motion is a pioneering
work in leveraging both pressure data and linguistic priors for motion
generation, and the established MPL benchmark is the first benchmark for this
task. Experiments show our method generates high-fidelity, physically plausible
motions, establishing a new state-of-the-art for this task. The codes and
benchmarks will be publicly released upon publication.

</details>


### [18] [Medical Referring Image Segmentation via Next-Token Mask Prediction](https://arxiv.org/abs/2511.05044)
*Xinyu Chen,Yiran Wang,Gaoyang Pang,Jiafu Hao,Chentao Yue,Luping Zhou,Yonghui Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为NTP-MRISeg的新框架，将医学指代表像分割（MRIS）重新定义为对图像、文本和掩码统一标记序列的自回归下一个标记预测任务，简化了模型设计，并通过引入三种新策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有MRIS方法通常依赖复杂的多模态融合机制或多阶段解码器，设计复杂且难以泛化。本文旨在简化模型架构，实现端到端训练，并提升对细粒度病灶边界的分割能力。

Method: 将MRIS任务转化为统一多模态序列上的自回归下一标记预测；采用Next-k Token Prediction（NkTP）减少累积误差，引入Token-level Contrastive Learning（TCL）增强边界敏感性并缓解长尾分布问题，设计基于记忆的Hard Error Token（HET）优化策略以强化难例学习。

Result: 在QaTa-COV19和MosMedData+数据集上进行了广泛实验，NTP-MRISeg实现了新的最先进性能，显著优于传统MRIS方法。

Conclusion: NTP-MRISeg提供了一种简洁高效的MRIS新范式，无需复杂的模态融合或外部分割模型，支持利用大规模预训练多模态分词器，具有良好的泛化性和适应性。

Abstract: Medical Referring Image Segmentation (MRIS) involves segmenting target
regions in medical images based on natural language descriptions. While
achieving promising results, recent approaches usually involve complex design
of multimodal fusion or multi-stage decoders. In this work, we propose
NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive
next-token prediction task over a unified multimodal sequence of tokenized
image, text, and mask representations. This formulation streamlines model
design by eliminating the need for modality-specific fusion and external
segmentation models, supports a unified architecture for end-to-end training.
It also enables the use of pretrained tokenizers from emerging large-scale
multimodal models, enhancing generalization and adaptability. More importantly,
to address challenges under this formulation-such as exposure bias, long-tail
token distributions, and fine-grained lesion edges-we propose three novel
strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative
prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance
boundary sensitivity and mitigate long-tail distribution effects, and (3) a
memory-based Hard Error Token (HET) optimization strategy that emphasizes
difficult tokens during training. Extensive experiments on the QaTa-COV19 and
MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art
performance, offering a streamlined and effective alternative to traditional
MRIS pipelines.

</details>


### [19] [Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach](https://arxiv.org/abs/2511.05057)
*Yuanxiang Huangfu,Chaochao Wang,Weilei Wang*

Main category: cs.CV

TL;DR: 提出Role-SynthCLIP，利用多视角角色扮演提示生成语义多样化的图像-文本对，提升CLIP模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法侧重增加数据量，但语义多样性不足，导致文本描述冗余或浅显。

Method: 设计多视角角色扮演提示（如构图分析师、图像上下文解释者），指导多模态大模型生成多样化、细粒度的图像描述。

Result: 在仅使用100万合成对的情况下，CLIP-B/16模型在MS COCO验证集上达到64.1%的Recall@1，超过使用500万对的最佳基线2.8个百分点。

Conclusion: Role-SynthCLIP通过提升合成数据的语义多样性与图文匹配精细度，显著增强CLIP模型性能，且数据效率更高。

Abstract: The effectiveness of Contrastive Language-Image Pre-training (CLIP) models
critically depends on the semantic diversity and quality of their training
data. However, while existing synthetic data generation methods primarily focus
on increasing data volume, such emphasis often leads to limited semantic
diversity and redundant or shallow captions. To address this limitation, we
propose Role-SynthCLIP, a novel data synthesis framework that leverages
multi-perspective role-playing prompts (e.g., a compositional analyst, an
interpreter of image context) to guide Multimodal Large Language Models (MLLMs)
in generating semantically diverse captions from distinct viewpoints. This
mechanism enhances the semantic diversity and fine-grained image-text alignment
of synthetic pairs, thereby improving caption expressiveness and accuracy while
keeping the total number of image-text pairs unchanged. Experimental results
demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model
trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on
the MS COCO validation set, surpassing the best existing synthetic data
baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained
models are released at https://github.com/huangfu170/Role-SynthCLIP.

</details>


### [20] [Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start](https://arxiv.org/abs/2511.05095)
*Fuyang Liu,Jiaqi Xu,Xiaowei Hu*

Main category: cs.CV

TL;DR: 提出了一种基于物理驱动高保真数据集HFLS-Weather和双层强化学习框架的视觉感知方法，可在无配对监督的情况下实现对复杂恶劣天气条件的持续自适应恢复与优化。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定参数合成数据训练的视觉模型在面对复杂多变的恶劣天气退化时泛化能力差，缺乏有效适应真实世界条件的方法。

Method: 构建了物理驱动的高保真天气模拟数据集HFLS-Weather，并设计了一个双层强化学习框架：局部层面通过扰动驱动的图像质量优化来精细化天气特定的恢复模型；全局层面由元控制器动态调度模型选择与执行顺序。

Result: 该框架在多种真实恶劣天气场景下实现了持续适应能力，并在多个基准上达到最先进的性能表现。

Conclusion: 所提方法通过结合物理仿真与双层强化学习，有效提升了模型在复杂天气条件下的鲁棒性与自适应性，为实际应用中的视觉感知提供了新思路。

Abstract: Adverse weather severely impairs real-world visual perception, while existing
vision models trained on synthetic data with fixed parameters struggle to
generalize to complex degradations. To address this, we first construct
HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse
weather phenomena, and then design a dual-level reinforcement learning
framework initialized with HFLS-Weather for cold-start training. Within this
framework, at the local level, weather-specific restoration models are refined
through perturbation-driven image quality optimization, enabling reward-based
learning without paired supervision; at the global level, a meta-controller
dynamically orchestrates model selection and execution order according to scene
degradation. This framework enables continuous adaptation to real-world
conditions and achieves state-of-the-art performance across a wide range of
adverse weather scenarios. Code is available at
https://github.com/xxclfy/AgentRL-Real-Weather

</details>


### [21] [SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements](https://arxiv.org/abs/2511.05108)
*Jörg Gamerdinger,Benedict Wetzel,Patrick Schulz,Sven Teufel,Oliver Bringmann*

Main category: cs.CV

TL;DR: 本文提出了一种在雪天环境下不依赖传统车道线、通过检测路边分隔柱作为间接车道指示的鲁棒实时车道检测方法，并发布了包含8万帧标注数据的合成数据集SnowyLane，显著提升了恶劣天气下的检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于积雪常导致车道线缺失或遮挡，传统车道检测方法在冬季环境失效，亟需一种不依赖可见车道线的可靠检测方案。

Method: 通过感知路边竖立的分隔柱，利用参数化贝塞尔曲线模型拟合平滑车道轨迹，结合空间一致性和道路几何特征进行车道推断。

Result: 在新提出的SnowyLane数据集上验证，相比现有最先进方法，本方法在大雪遮挡等恶劣天气下表现出显著更高的鲁棒性和准确性。

Conclusion: 该方法为冬季场景下的可靠车道检测提供了有效解决方案，并通过公开SnowyLane数据集推动全天候自动驾驶的研究发展。

Abstract: Lane detection for autonomous driving in snow-covered environments remains a
major challenge due to the frequent absence or occlusion of lane markings. In
this paper, we present a novel, robust and realtime capable approach that
bypasses the reliance on traditional lane markings by detecting roadside
features,specifically vertical roadside posts called delineators, as indirect
lane indicators. Our method first perceives these posts, then fits a smooth
lane trajectory using a parameterized Bezier curve model, leveraging spatial
consistency and road geometry. To support training and evaluation in these
challenging scenarios, we introduce SnowyLane, a new synthetic dataset
containing 80,000 annotated frames capture winter driving conditions, with
varying snow coverage, and lighting conditions. Compared to state-of-the-art
lane detection systems, our approach demonstrates significantly improved
robustness in adverse weather, particularly in cases with heavy snow occlusion.
This work establishes a strong foundation for reliable lane detection in winter
scenarios and contributes a valuable resource for future research in
all-weather autonomous driving. The dataset is available at
https://ekut-es.github.io/snowy-lane

</details>


### [22] [From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection](https://arxiv.org/abs/2511.05150)
*Jingsong Liu,Han Li,Nassir Navab,Peter J. Schüffler*

Main category: cs.CV

TL;DR: JWTH是一种新型病理基础模型，通过整合自监督预训练、细胞中心微调和注意力池化，融合局部与全局特征，在多个生物标志物检测任务中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型多依赖全局patch级嵌入，忽视细胞级形态信息，限制了AI生物标志物的可解释性与准确性。

Method: 提出JWTH模型，采用大规模自监督预训练，结合以细胞为中心的微调策略和注意力池化机制，实现局部与全局token的融合。

Result: 在四个生物标志物、八个队列的四项任务中，JWTH相比先前模型最高提升8.3%的平衡准确率，平均提升1.2%。

Conclusion: JWTH通过细粒度细胞级建模显著提升了AI生物标志物检测的性能与可解释性，推动了数字病理学中基础模型的发展。

Abstract: AI-based biomarkers can infer molecular features directly from hematoxylin &
eosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global
patch-level embeddings and overlook cell-level morphology. We present a PFM
model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale
self-supervised pretraining with cell-centric post-tuning and attention pooling
to fuse local and global tokens. Across four tasks involving four biomarkers
and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2%
average improvement over prior PFMs, advancing interpretable and robust
AI-based biomarker detection in digital pathology.

</details>


### [23] [Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges](https://arxiv.org/abs/2511.05152)
*Adrian Azzarelli,Nantheera Anantrasirichai,David R Bull*

Main category: cs.CV

TL;DR: 提出一种将高斯点云和形变场分为前景和背景的可变形高斯点阵方法，通过稀疏掩码实现高质量动态3D重建，在稀疏多视角视频下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可变形高斯点阵方法在稀疏相机配置下难以捕捉复杂动态特征，受限于电影制作中的预算限制。

Method: 将标准高斯点云和形变场拆分为前景和背景两部分，使用t=0时刻的稀疏掩码进行分离；在预训练阶段分别对两部分应用不同损失函数，动态训练时为前景建模颜色、位置、旋转变化，为背景仅建模位置变化。

Result: 在3D和2.5D娱乐数据集上实现了最先进的定性和定量结果，PSNR最高提升3dB，模型大小减半；无需密集掩码监督即可生成包含透明和动态纹理的分段动态重建。

Conclusion: 该方法在稀疏视角条件下显著提升了动态场景重建质量，兼顾效率与视觉效果，适用于实际电影制作场景。

Abstract: Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D
reconstruction from dense multi-view video (MVV) by learning to deform a
canonical GS representation. However, in filmmaking, tight budgets can result
in sparse camera configurations, which limits state-of-the-art (SotA) methods
when capturing complex dynamic features. To address this issue, we introduce an
approach that splits the canonical Gaussians and deformation field into
foreground and background components using a sparse set of masks for frames at
t=0. Each representation is separately trained on different loss functions
during canonical pre-training. Then, during dynamic training, different
parameters are modeled for each deformation field following common filmmaking
practices. The foreground stage contains diverse dynamic features so changes in
color, position and rotation are learned. While, the background containing
film-crew and equipment, is typically dimmer and less dynamic so only changes
in point position are learned. Experiments on 3-D and 2.5-D entertainment
datasets show that our method produces SotA qualitative and quantitative
results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the
SotA and without the need for dense mask supervision, our method also produces
segmented dynamic reconstructions including transparent and dynamic textures.
Code and video comparisons are available online:
https://interims-git.github.io/

</details>


### [24] [Another BRIXEL in the Wall: Towards Cheaper Dense Features](https://arxiv.org/abs/2511.05168)
*Alexander Lappe,Martin A. Giese*

Main category: cs.CV

TL;DR: BRIXEL是一种简单的知识蒸馏方法，通过让学生模型学习在更高分辨率下生成特征图来提升性能，在保持分辨率不变的情况下显著优于DINOv3基线模型，并以较低计算成本生成与教师模型相似的特征图。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基础模型（如DINOv3）需要高分辨率输入和大量计算资源来生成细粒度的密集特征图，限制了其在实际应用中的效率和可扩展性。

Method: 提出BRIXEL，采用知识蒸馏策略，让学生模型学习自我生成更高分辨率的特征图，从而在不增加输入分辨率的情况下提升特征质量。

Result: BRIXEL在下游任务中显著优于DINOv3基线模型，且生成的特征图与教师模型高度相似，同时计算成本大幅降低。

Conclusion: BRIXEL通过简单有效的蒸馏机制，在保持低计算开销的同时实现了优异的密集特征表示能力，为高效视觉建模提供了新思路。

Abstract: Vision foundation models achieve strong performance on both global and
locally dense downstream tasks. Pretrained on large images, the recent DINOv3
model family is able to produce very fine-grained dense feature maps, enabling
state-of-the-art performance. However, computing these feature maps requires
the input image to be available at very high resolution, as well as large
amounts of compute due to the squared complexity of the transformer
architecture. To address these issues, we propose BRIXEL, a simple knowledge
distillation approach that has the student learn to reproduce its own feature
maps at higher resolution. Despite its simplicity, BRIXEL outperforms the
baseline DINOv3 models by large margins on downstream tasks when the resolution
is kept fixed. Moreover, it is able to produce feature maps that are very
similar to those of the teacher at a fraction of the computational cost. Code
and model weights are available at https://github.com/alexanderlappe/BRIXEL.

</details>


### [25] [MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification](https://arxiv.org/abs/2511.05170)
*Zijiang Yang,Hanqing Chao,Bokai Zhao,Yelin Yang,Yunshuo Zhang,Dongmei Fu,Junping Zhang,Le Lu,Ke Yan,Dakai Jin,Minfeng Xu,Yun Bian,Hui Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MUSE的自监督学习方法，用于组织病理学中的核检测与分类（NDC），通过坐标引导的局部自蒸馏机制NuLo实现多尺度特征对齐，无需严格的增广视图空间对齐，从而提升模型对细粒度核表示的学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有NDC方法依赖大量人工标注的核级标签，且难以充分利用大规模无标签数据来学习判别性核表示，因此需要一种能有效利用无标签数据的自监督方法。

Method: 提出MUSE框架，核心为NuLo机制，基于预测的核位置进行坐标引导的局部自蒸馏；设计了适用于MUSE的编码器-解码器结构，并采用大视野半监督微调策略以最大化利用无标签图像。

Result: 在三个广泛使用的基准上实验表明，MUSE不仅优于现有的监督基线方法，还超越了通用的病理基础模型。

Conclusion: MUSE有效解决了组织病理学NDC中的关键挑战，在减少标注依赖的同时提升了核级别表示学习性能，具有较强的实用性和泛化能力。

Abstract: Nucleus detection and classification (NDC) in histopathology analysis is a
fundamental task that underpins a wide range of high-level pathology
applications. However, existing methods heavily rely on labor-intensive
nucleus-level annotations and struggle to fully exploit large-scale unlabeled
data for learning discriminative nucleus representations. In this work, we
propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised
learning method tailored for NDC. At its core is NuLo (Nucleus-based Local
self-distillation), a coordinate-guided mechanism that enables flexible local
self-distillation based on predicted nucleus positions. By removing the need
for strict spatial alignment between augmented views, NuLo allows critical
cross-scale alignment, thus unlocking the capacity of models for fine-grained
nucleus-level representation. To support MUSE, we design a simple yet effective
encoder-decoder architecture and a large field-of-view semi-supervised
fine-tuning strategy that together maximize the value of unlabeled pathology
images. Extensive experiments on three widely used benchmarks demonstrate that
MUSE effectively addresses the core challenges of histopathological NDC. The
resulting models not only surpass state-of-the-art supervised baselines but
also outperform generic pathology foundation models.

</details>


### [26] [Walk the Lines 2: Contour Tracking for Detailed Segmentation](https://arxiv.org/abs/2511.05210)
*André Peter Kelm,Max Braeschke,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: 本文提出了Walk the Lines 2（WtL2），一种专用于红外（IR）船舶和RGB图像中多种物体精细分割的轮廓跟踪算法。该方法扩展了原有的WtL算法，适用于红外图像并提升对RGB多类物体的处理能力，在闭合轮廓生成和细节保留方面优于最新轮廓方法，具有高IoU表现，适用于需要高质量分割的专用场景。


<details>
  <summary>Details</summary>
Motivation: 原有WtL算法仅适用于彩色图像中的船舶分割，应用范围有限。为拓展其至红外图像并支持更多RGB对象类型，需改进输入与算法结构以适应不同模态与目标。

Method: WtL2通过改进轮廓检测器以适配红外船舶，并增强算法对多种RGB对象的处理能力；利用轮廓跟踪替代传统的非极大值抑制（NMS），迭代优化目标轮廓直至形成1像素宽的闭合形状，进而二值化为可分割区域。

Result: WtL2在红外和多种RGB对象上实现了高精度分割，闭合轮廓质量优于现有轮廓方法，达到高峰值交并比（IoU）并保留丰富细节。

Conclusion: WtL2有效扩展了WtL的应用范围，成为适用于红外与多类RGB对象的高性能轮廓跟踪分割方法，适合对细节和样本质量要求较高的专业应用场景。

Abstract: This paper presents Walk the Lines 2 (WtL2), a unique contour tracking
algorithm specifically adapted for detailed segmentation of infrared (IR) ships
and various objects in RGB.1 This extends the original Walk the Lines (WtL)
[12], which focused solely on detailed ship segmentation in color. These
innovative WtLs can replace the standard non-maximum suppression (NMS) by using
contour tracking to refine the object contour until a 1-pixel-wide closed shape
can be binarized, forming a segmentable area in foreground-background
scenarios. WtL2 broadens the application range of WtL beyond its original
scope, adapting to IR and expanding to diverse objects within the RGB context.
To achieve IR segmentation, we adapt its input, the object contour detector, to
IR ships. In addition, the algorithm is enhanced to process a wide range of RGB
objects, outperforming the latest generation of contour-based methods when
achieving a closed object contour, offering high peak Intersection over Union
(IoU) with impressive details. This positions WtL2 as a compelling method for
specialized applications that require detailed segmentation or high-quality
samples, potentially accelerating progress in several niche areas of image
segmentation.

</details>


### [27] [4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos](https://arxiv.org/abs/2511.05229)
*Mengqi Guo,Bo Xu,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出了一种无需相机姿态的动态神经渲染框架4D3R，通过两阶段方法分离静态与动态成分，在真实动态场景中实现了比现有方法更高的渲染质量与计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决单目动态视频在未知相机姿态下的新视角合成难题，现有NeRF和3DGS方法难以处理动态内容且依赖预估姿态。

Method: 首先利用3D基础模型进行初始位姿与几何估计，然后通过运动感知优化；引入运动感知BA模块（结合Transformer先验与SAM2）和高效的运动感知高斯点阵表示（MA-GS），使用控制点、变形场MLP和线性混合蒙皮建模动态运动。

Result: 在真实动态数据集上实验表明，相比最先进方法PSNR提升达1.8dB，尤其在大动态物体场景表现优异，同时计算成本降低5倍。

Conclusion: 4D3R实现了高质量、高效且无需精确相机姿态输入的动态场景新视角合成，推动了动态神经渲染在复杂现实场景中的应用。

Abstract: Novel view synthesis from monocular videos of dynamic scenes with unknown
camera poses remains a fundamental challenge in computer vision and graphics.
While recent advances in 3D representations such as Neural Radiance Fields
(NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static
scenes, they struggle with dynamic content and typically rely on pre-computed
camera poses. We present 4D3R, a pose-free dynamic neural rendering framework
that decouples static and dynamic components through a two-stage approach. Our
method first leverages 3D foundational models for initial pose and geometry
estimation, followed by motion-aware refinement. 4D3R introduces two key
technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that
combines transformer-based learned priors with SAM2 for robust dynamic object
segmentation, enabling more accurate camera pose refinement; and (2) an
efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses
control points with a deformation field MLP and linear blend skinning to model
dynamic motion, significantly reducing computational cost while maintaining
high-quality reconstruction. Extensive experiments on real-world dynamic
datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement
over state-of-the-art methods, particularly in challenging scenarios with large
dynamic objects, while reducing computational requirements by 5x compared to
previous dynamic scene representations.

</details>


### [28] [FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction](https://arxiv.org/abs/2511.05219)
*Jiang Lin,Xinyu Chen,Song Wu,Zhiqiu Zhang,Jizhi Zhang,Ye Wang,Qiang Tang,Qian Wang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: FreeControl是一种无需训练的扩散模型语义结构控制框架，通过单步注意力提取和潜在条件解耦（LCD）实现高效、稳定的图像生成控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ControlNet依赖手工设计的条件图且需重新训练，灵活性差；基于反演的方法虽对齐效果好但推理成本高。因此需要一种高效、灵活且无需训练的结构控制方法。

Method: FreeControl在单一关键时间步进行一步式注意力提取，并在整个去噪过程中复用该注意力；引入潜在条件解耦（LCD），分离关键时间步与噪声潜在变量，提升注意力质量并消除结构伪影；支持通过多源参考图像进行组合控制。

Result: FreeControl实现了与原始图像在结构和语义上的高度对齐，生成结果视觉连贯，支持直观的场景布局设计，推理成本仅增加约5%，且兼容现代扩散模型。

Conclusion: FreeControl提出了一种新的测试时控制范式，无需训练或反演即可实现高效、稳定、可组合的语义结构控制，为扩散模型的可控生成提供了更灵活的解决方案。

Abstract: Controlling the spatial and semantic structure of diffusion-generated images
remains a challenge. Existing methods like ControlNet rely on handcrafted
condition maps and retraining, limiting flexibility and generalization.
Inversion-based approaches offer stronger alignment but incur high inference
cost due to dual-path denoising. We present FreeControl, a training-free
framework for semantic structural control in diffusion models. Unlike prior
methods that extract attention across multiple timesteps, FreeControl performs
one-step attention extraction from a single, optimally chosen key timestep and
reuses it throughout denoising. This enables efficient structural guidance
without inversion or retraining. To further improve quality and stability, we
introduce Latent-Condition Decoupling (LCD): a principled separation of the key
timestep and the noised latent used in attention extraction. LCD provides finer
control over attention quality and eliminates structural artifacts. FreeControl
also supports compositional control via reference images assembled from
multiple sources - enabling intuitive scene layout design and stronger prompt
alignment. FreeControl introduces a new paradigm for test-time control,
enabling structurally and semantically aligned, visually coherent generation
directly from raw images, with the flexibility for intuitive compositional
design and compatibility with modern diffusion models at approximately 5
percent additional cost.

</details>


### [29] [OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU](https://arxiv.org/abs/2511.05263)
*Qi Sun,Dingju Zhou,Lina Zhang*

Main category: cs.CV

TL;DR: 本文提出了OregairuChar数据集，用于分析动漫《我的青春恋爱物语果然有问题》中角色出场频率，支持对角色显著性和叙事动态的计算研究。


<details>
  <summary>Details</summary>
Motivation: 为了深入理解动漫中的叙事结构和角色重要性，需要一个专门针对角色视觉出现频率分析的高质量数据集。

Method: 构建了一个包含1600帧、2860个标注框、涵盖11个主要角色的数据集，并在多个目标检测模型上进行基准测试，利用模型预测实现按集统计的角色出现频率分析。

Result: 成功实现了对角色出场频率的细粒度分析，揭示了角色在剧情发展中的 prominence 变化模式。

Conclusion: OregairuChar为研究风格化媒体中的计算叙事动态和以角色为中心的 storytelling 提供了一个有价值的工具。

Abstract: The analysis of character appearance frequency is essential for understanding
narrative structure, character prominence, and story progression in anime. In
this work, we introduce OregairuChar, a benchmark dataset designed for
appearance frequency analysis in the anime series My Teen Romantic Comedy
SNAFU. The dataset comprises 1600 manually selected frames from the third
season, annotated with 2860 bounding boxes across 11 main characters.
OregairuChar captures diverse visual challenges, including occlusion, pose
variation, and inter-character similarity, providing a realistic basis for
appearance-based studies. To enable quantitative research, we benchmark several
object detection models on the dataset and leverage their predictions for
fine-grained, episode-level analysis of character presence over time. This
approach reveals patterns of character prominence and their evolution within
the narrative. By emphasizing appearance frequency, OregairuChar serves as a
valuable resource for exploring computational narrative dynamics and
character-centric storytelling in stylized media.

</details>


### [30] [LiveStar: Live Streaming Assistant for Real-World Online Video Understanding](https://arxiv.org/abs/2511.05299)
*Zhenyu Yang,Kairui Zhang,Yuhang Hu,Bing Wang,Shengsheng Qian,Bin Wen,Fan Yang,Tingting Gao,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: 本文提出了LiveStar，一种用于实时视频理解的在线视频大语言模型，通过自适应流式解码实现持续主动响应，显著提升了实时性和叙事连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频大语言模型难以同时处理逐帧连续输入并确定最佳响应时机，影响了实时响应能力和叙事一致性。

Method: 提出LiveStar模型，包含增量视频-语言对齐训练策略、响应-静默解码框架以单次前向传播确定最佳响应时机，以及基于峰值-末端记忆压缩和流式键值缓存的内存感知加速方法。

Result: 在三个基准上实验表明，相比现有模型，语义正确性平均提升19.5%，响应时序差异减少18.1%，五项OmniStar任务中FPS提升12.0%。

Conclusion: LiveStar有效解决了在线视频理解中的实时响应与连贯生成难题，结合OmniStar数据集推动了该领域的发展。

Abstract: Despite significant progress in Video Large Language Models (Video-LLMs) for
offline video understanding, existing online Video-LLMs typically struggle to
simultaneously process continuous frame-by-frame inputs and determine optimal
response timing, often compromising real-time responsiveness and narrative
coherence. To address these limitations, we introduce LiveStar, a pioneering
live streaming assistant that achieves always-on proactive responses through
adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a
training strategy enabling incremental video-language alignment for
variable-length video streams, preserving temporal consistency across
dynamically evolving frame sequences; (2) a response-silence decoding framework
that determines optimal proactive response timing via a single forward pass
verification; (3) memory-aware acceleration via peak-end memory compression for
online inference on 10+ minute videos, combined with streaming key-value cache
to achieve 1.53x faster inference. We also construct an OmniStar dataset, a
comprehensive dataset for training and benchmarking that encompasses 15 diverse
real-world scenarios and 5 evaluation tasks for online video understanding.
Extensive experiments across three benchmarks demonstrate LiveStar's
state-of-the-art performance, achieving an average 19.5% improvement in
semantic correctness with 18.1% reduced timing difference compared to existing
online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.
Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.

</details>


### [31] [Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection](https://arxiv.org/abs/2511.05253)
*Tiziano Natali,Karin A. Olthof,Niels F. M. Kok,Koert F. D. Kuhlmann,Theo J. M. Ruers,Matteo Fusaglia*

Main category: cs.CV

TL;DR: 本研究提出了一种基于裁剪3D U-Net的自动分割方法，用于术中三维超声（iUS）下结直肠癌肝转移瘤（CRLM）的精准勾画。该方法在保持高精度的同时实现了近实时分割，显著提升了手术导航效率。


<details>
  <summary>Details</summary>
Motivation: 术中准确勾画CRLM对于实现阴性切缘至关重要，但传统iUS因对比度低、噪声多和操作者依赖性而难以满足需求。因此，亟需一种自动化、高精度的分割方法以提升手术精度和效率。

Method: 采用nnU-Net框架实现3D U-Net模型，并比较了全体积输入与肿瘤区域裁剪后输入两种策略。使用85例患者的追踪3D iUS数据进行训练与评估，集成至3D Slicer平台以支持实时术中应用。

Result: 裁剪体积模型显著优于全体积模型（AUC-ROC：0.898 vs 0.718），中位DSC为0.74，召回率为0.79，Hausdorff距离为17.1 mm，速度比半自动方法快约4倍（约1分钟）。前瞻性术中测试验证了其稳健性和临床可用性。

Conclusion: 基于裁剪3D U-Net的自动分割方法可实现CRLM在iUS中的可靠、近实时分割，几乎无需人工干预，能够在无需配准的情况下支持高效超声导航肝切除术，接近专家水平精度，显著降低工作量和手术时间。

Abstract: Introduction: Accurate intraoperative delineation of colorectal liver
metastases (CRLM) is crucial for achieving negative resection margins but
remains challenging using intraoperative ultrasound (iUS) due to low contrast,
noise, and operator dependency. Automated segmentation could enhance precision
and efficiency in ultrasound-based navigation workflows.
  Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used
to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two
variants were compared: one trained on full iUS volumes and another on cropped
regions around tumors. Segmentation accuracy was assessed using Dice Similarity
Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference
(RVD) on retrospective and prospective datasets. The workflow was integrated
into 3D Slicer for real-time intraoperative use.
  Results: The cropped-volume model significantly outperformed the full-volume
model across all metrics (AUC-ROC = 0.898 vs 0.718). It achieved median DSC =
0.74, recall = 0.79, and HDist. = 17.1 mm comparable to semi-automatic
segmentation but with ~4x faster execution (~ 1 min). Prospective
intraoperative testing confirmed robust and consistent performance, with
clinically acceptable accuracy for real-time surgical guidance.
  Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net
provides reliable, near real-time results with minimal operator input. The
method enables efficient, registration-free ultrasound-based navigation for
hepatic surgery, approaching expert-level accuracy while substantially reducing
manual workload and procedure time.

</details>


### [32] [Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation](https://arxiv.org/abs/2511.05308)
*Matteo Bastico,David Ryckelynck,Laurent Corté,Yannick Tillier,Etienne Decencière*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D点云生成评价指标SNC，并改进了现有基于Chamfer距离的评估方法，结合提出的Diffusion Point Transformer模型，在ShapeNet数据集上实现了生成质量的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的点云生成评价指标（如Chamfer Distance）对缺陷不敏感，无法准确反映几何保真度和局部形状一致性，亟需更鲁棒的评估方法。

Method: 引入预对齐步骤并使用密度感知Chamfer距离（DCD），提出新的表面法向一致性（SNC）指标；结合序列化patch注意力机制，设计Diffusion Point Transformer生成模型。

Result: 在ShapeNet数据集上实验表明，所提指标更具鲁棒性，所提模型显著提升生成点云质量，达到SOTA水平。

Conclusion: 通过改进评估指标和生成架构，可显著提升3D点云生成模型的可靠性和输出质量。

Abstract: As 3D point clouds become a cornerstone of modern technology, the need for
sophisticated generative models and reliable evaluation metrics has grown
exponentially. In this work, we first expose that some commonly used metrics
for evaluating generated point clouds, particularly those based on Chamfer
Distance (CD), lack robustness against defects and fail to capture geometric
fidelity and local shape consistency when used as quality indicators. We
further show that introducing samples alignment prior to distance calculation
and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet
essential steps to ensure the consistency and robustness of point cloud
generative model evaluation metrics. While existing metrics primarily focus on
directly comparing 3D Euclidean coordinates, we present a novel metric, named
Surface Normal Concordance (SNC), which approximates surface similarity by
comparing estimated point normals. This new metric, when combined with
traditional ones, provides a more comprehensive evaluation of the quality of
generated samples. Finally, leveraging recent advancements in transformer-based
models for point cloud analysis, such as serialized patch attention , we
propose a new architecture for generating high-fidelity 3D structures, the
Diffusion Point Transformer. We perform extensive experiments and comparisons
on the ShapeNet dataset, showing that our model outperforms previous solutions,
particularly in terms of quality of generated point clouds, achieving new
state-of-the-art. Code available at
https://github.com/matteo-bastico/DiffusionPointTransformer.

</details>


### [33] [What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs](https://arxiv.org/abs/2511.05292)
*Jiaxi Yin,Pengcheng Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CuisineSense的系统，通过结合智能手表的手部运动和智能眼镜的头部动态来识别中国菜类型，实现了高精度的饮食状态检测和食物分类。


<details>
  <summary>Details</summary>
Motivation: 准确的食物摄入检测对于饮食监测和慢性病预防至关重要。传统方法存在回忆偏差或隐私问题，且现有可穿戴设备主要关注有限的食物种类，无法应对中国菜系的多样性。

Method: 设计了一个两阶段检测流程：第一阶段通过区分特征时间模式与非进食行为来识别进食状态；第二阶段基于进食期间捕捉的动作进行细粒度的食物类型识别。系统融合了来自智能手表的手部运动线索和来自智能眼镜的头部动态数据。

Result: 构建了一个包含11类食物、10名参与者共27.5小时IMU记录的数据集。实验表明，CuisineSense在进食状态检测和食物分类方面均实现了高精度。

Conclusion: CuisineSense为无干扰式可穿戴饮食监测提供了一个实用解决方案，并特别针对中国菜系的多样性进行了优化。

Abstract: Accurate food intake detection is vital for dietary monitoring and chronic
disease prevention. Traditional self-report methods are prone to recall bias,
while camera-based approaches raise concerns about privacy. Furthermore,
existing wearable-based methods primarily focus on a limited number of food
types, such as hamburgers and pizza, failing to address the vast diversity of
Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that
classifies Chinese food types by integrating hand motion cues from a smartwatch
with head dynamics from smart glasses. To filter out irrelevant daily
activities, we design a two-stage detection pipeline. The first stage
identifies eating states by distinguishing characteristic temporal patterns
from non-eating behaviors. The second stage then conducts fine-grained food
type recognition based on the motions captured during food intake. To evaluate
CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings
across 11 food categories and 10 participants. Experiments demonstrate that
CuisineSense achieves high accuracy in both eating state detection and food
classification, offering a practical solution for unobtrusive, wearable-based
dietary monitoring.The system code is publicly available at
https://github.com/joeeeeyin/CuisineSense.git.

</details>


### [34] [AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly](https://arxiv.org/abs/2511.05394)
*Alexander Htet Kyaw,Haotian Ma,Sasa Zivkovic,Jenny Sabin*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习物体识别的增强现实（AR）辅助组装工作流程，通过实时识别组件并显示逐步指导，减少手动查找和分类的需求。


<details>
  <summary>Details</summary>
Motivation: 为了提高组装效率，减少人工操作中的错误和时间消耗，探索利用AI和AR技术实现智能化的组装指导系统。

Method: 采用深度学习模型进行物体识别，在AR环境中为每个组装步骤实时标注对应组件的位置和安装位置，并叠加显示组装指引。

Result: 系统能够在物理空间中准确框出待组装部件及其目标位置，成功应用于乐高雕塑的组装案例中，验证了技术可行性。

Conclusion: 该方法有效实现了无需预先标记或分类的智能AR辅助组装，展示了AI与AR结合在实际装配任务中的潜力。

Abstract: We present an AI-assisted Augmented Reality assembly workflow that uses deep
learning-based object recognition to identify different assembly components and
display step-by-step instructions. For each assembly step, the system displays
a bounding box around the corresponding components in the physical space, and
where the component should be placed. By connecting assembly instructions with
the real-time location of relevant components, the system eliminates the need
for manual searching, sorting, or labeling of different components before each
assembly. To demonstrate the feasibility of using object recognition for
AR-assisted assembly, we highlight a case study involving the assembly of LEGO
sculptures.

</details>


### [35] [Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments](https://arxiv.org/abs/2511.05404)
*Laura Alejandra Encinar Gonzalez,John Folkesson,Rudolph Triebel,Riccardo Giubilato*

Main category: cs.CV

TL;DR: 本文提出了一种名为MPRF的多模态管道，利用基于Transformer的基础模型融合视觉和LiDAR信息，实现复杂环境中鲁棒的闭环检测。该方法结合DINOv2、SALAD和SONATA等先进技术，在检索精度和位姿估计鲁棒性方面优于现有方法，并在低纹理区域表现出色。


<details>
  <summary>Details</summary>
Motivation: 在GNSS失效环境（如行星探测）中，传统视觉或LiDAR闭环检测方法因纹理缺失、场景模糊或点云稀疏而性能受限，亟需一种更鲁棒、可靠的多模态解决方案。

Method: MPRF采用两阶段视觉检索策略，结合DINOv2特征与SALAD聚合进行候选筛选，并引入SONATA生成LiDAR描述符用于几何验证，同时实现显式的6自由度位姿估计，提升闭环检测的准确性和可解释性。

Result: 在S3LI和S3LI Vulcano数据集上的实验表明，MPRF在精度上优于当前最先进的检索方法，并在低纹理区域显著提升了位姿估计的鲁棒性。

Conclusion: MPRF通过融合视觉与LiDAR基础模型，实现了高精度、高效率且可靠的闭环检测，为SLAM系统提供了良好的精度-效率-可靠性平衡，展示了基础模型在统一地点识别与位姿估计任务中的潜力。

Abstract: Robust loop closure detection is a critical component of Simultaneous
Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as
in the context of planetary exploration. In these settings, visual place
recognition often fails due to aliasing and weak textures, while LiDAR-based
methods suffer from sparsity and ambiguity. This paper presents MPRF, a
multimodal pipeline that leverages transformer-based foundation models for both
vision and LiDAR modalities to achieve robust loop closure in severely
unstructured environments. Unlike prior work limited to retrieval, MPRF
integrates a two-stage visual retrieval strategy with explicit 6-DoF pose
estimation, combining DINOv2 features with SALAD aggregation for efficient
candidate screening and SONATA-based LiDAR descriptors for geometric
verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show
that MPRF outperforms state-of-the-art retrieval methods in precision while
enhancing pose estimation robustness in low-texture regions. By providing
interpretable correspondences suitable for SLAM back-ends, MPRF achieves a
favorable trade-off between accuracy, efficiency, and reliability,
demonstrating the potential of foundation models to unify place recognition and
pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.

</details>


### [36] [TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning](https://arxiv.org/abs/2511.05489)
*Junwen Pan,Qizhe Zhang,Rui Zhang,Ming Lu,Xin Wan,Yuan Zhang,Chang Liu,Qi She*

Main category: cs.CV

TL;DR: 本文提出TimeSearch-R，将时间搜索重构为交错的文本-视频推理过程，并通过强化学习优化搜索策略。为解决强化学习中中间决策无监督的问题，引入了完备性自验证机制（GRPO-CSV），提升了视频推理的完整性和探索能力。同时构建了专门训练数据集，在多个基准上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有时间搜索方法依赖手工设计的搜索流程，缺乏端到端优化，难以学习最优搜索策略；同时强化学习训练中的中间搜索决策缺乏监督，导致探索不足和推理不一致。

Method: 将时间搜索建模为交错的文本-视频推理过程，采用强化学习（基于GRPO）进行端到端训练，并提出GRPO-CSV，在推理过程中引入完备性自验证机制，利用同一策略模型对已搜索帧的充分性进行验证，以提升搜索完整性。此外，构建了用于SFT冷启动和RL训练的专用数据集，筛选弱时间依赖样本以提高任务难度。

Result: TimeSearch-R在Haystack-LVBench、Haystack-Ego4D等时间搜索基准，以及VideoMME、MLVU等长视频理解基准上均取得显著提升。在LongVideoBench上相比Qwen2.5-VL基础模型提升4.1%，相比先进模型Video-R1提升2.0%。

Conclusion: TimeSearch-R通过将时间搜索融入推理过程并结合GRPO-CSV自验证机制，实现了更完整和一致的视频内容探索，推动了长视频理解中时间搜索的性能边界，建立了新的SOTA。

Abstract: Temporal search aims to identify a minimal set of relevant frames from tens
of thousands based on a given query, serving as a foundation for accurate
long-form video understanding. Existing works attempt to progressively narrow
the search space. However, these approaches typically rely on a hand-crafted
search process, lacking end-to-end optimization for learning optimal search
strategies. In this paper, we propose TimeSearch-R, which reformulates temporal
search as interleaved text-video thinking, seamlessly integrating searching
video clips into the reasoning process through reinforcement learning (RL).
However, applying RL training methods, such as Group Relative Policy
Optimization (GRPO), to video reasoning can result in unsupervised intermediate
search decisions. This leads to insufficient exploration of the video content
and inconsistent logical reasoning. To address these issues, we introduce GRPO
with Completeness Self-Verification (GRPO-CSV), which gathers searched video
frames from the interleaved reasoning process and utilizes the same policy
model to verify the adequacy of searched frames, thereby improving the
completeness of video reasoning. Additionally, we construct datasets
specifically designed for the SFT cold-start and RL training of GRPO-CSV,
filtering out samples with weak temporal dependencies to enhance task
difficulty and improve temporal search capabilities. Extensive experiments
demonstrate that TimeSearch-R achieves significant improvements on temporal
search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as
long-form video understanding benchmarks like VideoMME and MLVU. Notably,
TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%
improvement over the base model Qwen2.5-VL and 2.0% over the advanced video
reasoning model Video-R1. Our code is available at
https://github.com/Time-Search/TimeSearch-R.

</details>


### [37] [$\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models](https://arxiv.org/abs/2511.05319)
*Huanqi Wu,Huangbiao Xu,Runfeng Xie,Jiaxin Cai,Kaixin Zhang,Xiao Ke*

Main category: cs.CV

TL;DR: 本文提出了句子到图像的隐写术（Sentence-to-Image Steganography），利用大语言模型（LLM）将句子级甚至段落级文本信息嵌入图像中，提出了一种新的语义隐写任务和名为S²LM的框架，并建立了Invisible Text（IVT）基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 传统隐写术难以在载体中嵌入语义丰富的句子级信息，而在AIGC时代对隐写容量和语义表达能力提出了更高要求，因此需要发展能够处理高层语义信息的新型隐写技术。

Method: 提出S²LM：一个基于大语言模型的语义隐写语言模型，设计了全新的端到端流程，让LLM全程参与文本信息的编码与图像生成过程，实现将句子或段落等高级语义内容嵌入图像中，并构建IVT基准用于评估。

Result: 实验表明S²LM在定量和定性上均能有效将句子级秘密信息隐藏于图像中，且具有良好的隐蔽性和恢复性，显著提升了LLM在语义隐写方面的能力。

Conclusion: S²LM成功实现了将复杂语义文本嵌入图像的语义隐写新范式，拓展了大语言模型在信息安全领域的应用潜力。

Abstract: Although steganography has made significant advancements in recent years, it
still struggles to embed semantically rich, sentence-level information into
carriers. However, in the era of AIGC, the capacity of steganography is more
critical than ever. In this work, we present Sentence-to-Image Steganography,
an instance of Semantic Steganography, a novel task that enables the hiding of
arbitrary sentence-level messages within a cover image. Furthermore, we
establish a benchmark named Invisible Text (IVT), comprising a diverse set of
sentence-level texts as secret messages for evaluation. Finally, we present
$\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large
language models (LLMs) to embed high-level textual information, such as
sentences or even paragraphs, into images. Unlike traditional bit-level
counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich
content through a newly designed pipeline in which the LLM is involved
throughout the entire process. Both quantitative and qualitative experiments
demonstrate that our method effectively unlocks new semantic steganographic
capabilities for LLMs. The source code will be released soon.

</details>


### [38] [Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects](https://arxiv.org/abs/2511.05356)
*Manuel Gomes,Bogdan Raducanu,Miguel Oliveira*

Main category: cs.CV

TL;DR: 本文提出了Artic4D数据集和CanonSeg4D框架，用于提升4D动态关节约束物体的全景分割性能，通过引入时间建模和规范空间对齐，在复杂场景中实现了优于现有方法的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽略关节约束物体的时间动态特性，且缺乏适用于4D全景分割的基准数据集，限制了该领域的发展。

Method: 基于PartNet Mobility构建了包含4D全景标注和关节参数的合成数据集Artic4D，并提出CanonSeg4D框架，通过估计每帧中物体部件到学习到的规范空间的偏移量，实现跨帧的部件一致性对齐与分割。

Result: 在Artic4D上的实验表明，CanonSeg4D在复杂场景下的全景分割精度优于现有最先进方法，验证了时间建模和规范对齐的有效性。

Conclusion: 引入时间信息和规范空间建模显著提升了关节约束物体的4D理解能力，为未来4D动态物体感知研究提供了新的数据基础和方法思路。

Abstract: Articulated object perception presents significant challenges in computer
vision, particularly because most existing methods ignore temporal dynamics
despite the inherently dynamic nature of such objects. The use of 4D temporal
data has not been thoroughly explored in articulated object perception and
remains unexamined for panoptic segmentation. The lack of a benchmark dataset
further hurt this field. To this end, we introduce Artic4D as a new dataset
derived from PartNet Mobility and augmented with synthetic sensor data,
featuring 4D panoptic annotations and articulation parameters. Building on this
dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework.
This approach explicitly estimates per-frame offsets mapping observed object
parts to a learned canonical space, thereby enhancing part-level segmentation.
The framework employs this canonical representation to achieve consistent
alignment of object parts across sequential frames. Comprehensive experiments
on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the
art approaches in panoptic segmentation accuracy in more complex scenarios.
These findings highlight the effectiveness of temporal modeling and canonical
alignment in dynamic object understanding, and pave the way for future advances
in 4D articulated object perception.

</details>


### [39] [Dense Motion Captioning](https://arxiv.org/abs/2511.05369)
*Shiyao Xu,Benedetta Liberatori,Gül Varol,Paolo Rota*

Main category: cs.CV

TL;DR: 本文提出了密集动作描述（Dense Motion Captioning）这一新任务，旨在对3D人体动作序列中的动作进行时间定位与描述，并发布了首个大规模复杂动作数据集CompMo及配套模型DEMO，显著提升了3D动作理解与描述的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D人体动作与语言结合研究多集中于文本到动作生成，而动作理解任务相对缺乏关注；同时现有数据集在时间标注细节和序列复杂度上存在不足。

Method: 构建了一个包含6万条多动作、精细时间标注的复杂3D动作序列的大规模数据集CompMo，并提出DEMO模型，将大语言模型与简单的动作适配器结合，实现对动作序列的密集、时序对齐的描述生成。

Result: 实验表明，DEMO在CompMo数据集及改编基准上均显著优于现有方法，实现了对复杂动作序列的准确时序定位与自然语言描述。

Conclusion: 该工作推动了3D人体动作理解的发展，为未来动作-语言联合建模提供了新的任务设定、高质量数据集和有效模型基线。

Abstract: Recent advances in 3D human motion and language integration have primarily
focused on text-to-motion generation, leaving the task of motion understanding
relatively unexplored. We introduce Dense Motion Captioning, a novel task that
aims to temporally localize and caption actions within 3D human motion
sequences. Current datasets fall short in providing detailed temporal
annotations and predominantly consist of short sequences featuring few actions.
To overcome these limitations, we present the Complex Motion Dataset (CompMo),
the first large-scale dataset featuring richly annotated, complex motion
sequences with precise temporal boundaries. Built through a carefully designed
data generation pipeline, CompMo includes 60,000 motion sequences, each
composed of multiple actions ranging from at least two to ten, accurately
annotated with their temporal extents. We further present DEMO, a model that
integrates a large language model with a simple motion adapter, trained to
generate dense, temporally grounded captions. Our experiments show that DEMO
substantially outperforms existing methods on CompMo as well as on adapted
benchmarks, establishing a robust baseline for future research in 3D motion
understanding and captioning.

</details>


### [40] [PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization](https://arxiv.org/abs/2511.05393)
*Zehui Feng,Tian Qiu,Tong Wu,Junxuan Li,Huayuan Xu,Ting Han*

Main category: cs.CV

TL;DR: 本文提出了一种名为PreResQ-R1的新型视觉质量评估框架，通过偏好-响应解耦的强化学习方法，统一了绝对评分回归与相对排序一致性，显著提升了图像和视频质量评估的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉质量评估方法多依赖监督微调或仅排序目标，导致推理浅层、评分校准差、跨域泛化能力弱，难以实现细粒度且符合人类感知的质量判断。

Method: 提出PreResQ-R1框架，采用双分支奖励机制，分别建模样本内响应一致性和样本间偏好对齐，并通过组相对策略优化（GRPO）进行训练；结合全局时序与局部空间数据流策略，扩展至视频质量评估。

Result: 在仅使用6K图像和28K视频进行强化微调的情况下，PreResQ-R1在10个IQA和5个VQA基准上均达到SOTA性能，IQA任务中SRCC和PLCC指标分别提升5.30%和2.15%。

Conclusion: PreResQ-R1实现了更精细、稳定且可解释的感知质量推理，生成的人类对齐推理轨迹揭示了质量判断背后的感知线索，具备良好的跨域泛化能力和应用潜力。

Abstract: Visual Quality Assessment (QA) seeks to predict human perceptual judgments of
visual fidelity. While recent multimodal large language models (MLLMs) show
promise in reasoning about image and video quality, existing approaches mainly
rely on supervised fine-tuning or rank-only objectives, resulting in shallow
reasoning, poor score calibration, and limited cross-domain generalization. We
propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning
framework that unifies absolute score regression and relative ranking
consistency within a single reasoning-driven optimization scheme. Unlike prior
QA methods, PreResQ-R1 introduces a dual-branch reward formulation that
separately models intra-sample response coherence and inter-sample preference
alignment, optimized via Group Relative Policy Optimization (GRPO). This design
encourages fine-grained, stable, and interpretable chain-of-thought reasoning
about perceptual quality. To extend beyond static imagery, we further design a
global-temporal and local-spatial data flow strategy for Video Quality
Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and
28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5
VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%
and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it
produces human-aligned reasoning traces that reveal the perceptual cues
underlying quality judgments. Code and model are available.

</details>


### [41] [PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior](https://arxiv.org/abs/2511.05403)
*Zicong Fan,Edoardo Remelli,David Dimond,Fadime Sener,Liuhao Ge,Bugra Tekin,Cem Keskin,Shreyas Hampali*

Main category: cs.CV

TL;DR: 本文提出了PALM，一个大规模手部数据集，包含263名受试者的1.3万次高质量扫描和9万张多视角图像，用于推动基于单张图像的个性化手部头像建模。同时提出PALM-Net作为基线方法，通过物理渲染反演学习手部几何与材质先验，实现逼真且可重光照的个性化重建。


<details>
  <summary>Details</summary>
Motivation: 由于手部几何、外观和姿态复杂，且缺乏兼具精确3D几何、高分辨率多视角图像和多样化人群的数据集，从图像生成高质量个性化手部头像仍具挑战性。

Method: 构建了包含13k手部扫描和90k多视角图像的大规模数据集PALM，并提出PALM-Net，采用基于物理的逆向渲染方法，在多主体上学习手部几何与材质属性的先验模型。

Result: PALM数据集覆盖丰富的肤色、年龄和几何差异；PALM-Net实现了基于单张图像的逼真、可重光照的手部头像个性化重建。

Conclusion: PALM是首个大规模真实手部多模态数据集，为手部建模及相关研究提供了宝贵资源，所提方法展示了其在单图个性化重建中的有效性。

Abstract: The ability to grasp objects, signal with gestures, and share emotion through
touch all stem from the unique capabilities of human hands. Yet creating
high-quality personalized hand avatars from images remains challenging due to
complex geometry, appearance, and articulation, particularly under
unconstrained lighting and limited views. Progress has also been limited by the
lack of datasets that jointly provide accurate 3D geometry, high-resolution
multiview imagery, and a diverse population of subjects. To address this, we
present PALM, a large-scale dataset comprising 13k high-quality hand scans from
263 subjects and 90k multi-view images, capturing rich variation in skin tone,
age, and geometry. To show its utility, we present a baseline PALM-Net, a
multi-subject prior over hand geometry and material properties learned via
physically based inverse rendering, enabling realistic, relightable
single-image hand avatar personalization. PALM's scale and diversity make it a
valuable real-world resource for hand modeling and related research.

</details>


### [42] [Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis](https://arxiv.org/abs/2511.05432)
*Dogucan Yaman,Seymanur Akti,Fevziye Irem Eyiokur,Alexander Waibel*

Main category: cs.CV

TL;DR: 提出了一种基于HierSpeech++潜在语音表示的文本到说话人脸合成框架，通过两阶段训练实现音频与视觉的高度对齐。


<details>
  <summary>Details</summary>
Motivation: 解决文本到说话人脸合成中音频与视觉不同步、表情不自然以及TTS特征分布偏移的问题。

Method: 设计Text-to-Vec模块将文本转换为Wav2Vec2嵌入，并联合条件生成语音和面部动作；采用预训练加微调的两阶段训练策略以应对TTS特征分布变化。

Result: 在无真实音频推断条件下实现了紧致的音视频对齐，提升了唇部同步性和视觉真实感，优于级联方法。

Conclusion: 该方法能有效生成自然、富有表现力的说话人脸视频，并保持说话人身份一致性。

Abstract: We propose a text-to-talking-face synthesis framework leveraging latent
speech representations from HierSpeech++. A Text-to-Vec module generates
Wav2Vec2 embeddings from text, which jointly condition speech and face
generation. To handle distribution shifts between clean and TTS-predicted
features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and
finetuning on TTS outputs. This enables tight audio-visual alignment, preserves
speaker identity, and produces natural, expressive speech and synchronized
facial motion without ground-truth audio at inference. Experiments show that
conditioning on TTS-predicted latent features outperforms cascaded pipelines,
improving both lip-sync and visual realism.

</details>


### [43] [How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?](https://arxiv.org/abs/2511.05449)
*Tuan Anh Tran,Duy M. H. Nguyen,Hoai-Chau Tran,Michael Barz,Khoa D. Doan,Roger Wattenhofer,Ngo Anh Vien,Mathias Niepert,Daniel Sonntag,Paul Swoboda*

Main category: cs.CV

TL;DR: 提出了一种名为gitmerge3D的全局感知图令牌合并方法，可将3D点云Transformer中的令牌数量减少90-95%，同时保持竞争力的性能，揭示了当前模型存在过度令牌化问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云Transformer依赖密集令牌表示，导致训练和推理过程中计算与内存开销高，亟需提升效率。

Method: 引入gitmerge3D，一种全局感知的图令牌合并方法，通过识别并合并冗余令牌来显著降低令牌数量。

Result: 在多个3D视觉任务上验证了该方法，实现了最高90-95%的令牌压缩率，并显著提升了计算效率，同时保持良好的性能。

Conclusion: 当前3D Transformer模型存在显著的令牌冗余，gitmerge3D为构建高效、可扩展的3D基础架构提供了新思路。

Abstract: Recent advances in 3D point cloud transformers have led to state-of-the-art
results in tasks such as semantic segmentation and reconstruction. However,
these models typically rely on dense token representations, incurring high
computational and memory costs during training and inference. In this work, we
present the finding that tokens are remarkably redundant, leading to
substantial inefficiency. We introduce gitmerge3D, a globally informed graph
token merging method that can reduce the token count by up to 90-95% while
maintaining competitive performance. This finding challenges the prevailing
assumption that more tokens inherently yield better performance and highlights
that many current models are over-tokenized and under-optimized for
scalability. We validate our method across multiple 3D vision tasks and show
consistent improvements in computational efficiency. This work is the first to
assess redundancy in large-scale 3D transformer models, providing insights into
the development of more efficient 3D foundation architectures. Our code and
checkpoints are publicly available at https://gitmerge3d.github.io

</details>


### [44] [The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2](https://arxiv.org/abs/2511.05461)
*Olivier Dietrich,Merlin Alfredsson,Emilia Arens,Nando Metzger,Torben Peters,Linus Scheibenreif,Jan Dirk Wegner,Konrad Schindler*

Main category: cs.CV

TL;DR: 本研究探讨了中等分辨率的哥白尼计划遥感影像（Sentinel-1和Sentinel-2）在建筑物损毁评估中的应用潜力，提出了xBD-S12数据集，并验证了其在多种灾害场景下进行大范围快速损毁评估的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于高分辨率影像常受限于获取难度和覆盖范围，亟需一种可广泛获取且时效性强的数据源用于灾后快速损毁评估，因此研究中等分辨率影像的应用可行性。

Method: 构建了包含10,315对灾前灾后影像的xBD-S12数据集，整合Sentinel-1与Sentinel-2数据，并在多个灾害场景下测试不同模型对建筑损毁检测的效果，评估模型泛化能力及架构复杂度的影响。

Result: 实验表明，尽管空间分辨率为10米，仍可在多种灾害中较准确地检测和绘制建筑损毁图；复杂的模型架构在跨灾害泛化上表现不佳，地理空间基础模型实际增益有限。

Conclusion: 哥白尼计划的中等分辨率影像可作为快速、大范围灾损评估的有效数据源，能有效补充高分辨率影像的不足，具备实际应用价值。

Abstract: Natural disasters demand rapid damage assessment to guide humanitarian
response. Here, we investigate whether medium-resolution Earth observation
images from the Copernicus program can support building damage assessment,
complementing very-high resolution imagery with often limited availability. We
introduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from
both Sentinel-1 and Sentinel-2, spatially and temporally aligned with the
established xBD benchmark. In a series of experiments, we demonstrate that
building damage can be detected and mapped rather well in many disaster
scenarios, despite the moderate 10$\,$m ground sampling distance. We also find
that, for damage mapping at that resolution, architectural sophistication does
not seem to bring much advantage: more complex model architectures tend to
struggle with generalization to unseen disasters, and geospatial foundation
models bring little practical benefit. Our results suggest that Copernicus
images are a viable data source for rapid, wide-area damage assessment and
could play an important role alongside VHR imagery. We release the xBD-S12
dataset, code, and trained models to support further research.

</details>


### [45] [Photo Dating by Facial Age Aggregation](https://arxiv.org/abs/2511.05464)
*Jakub Paplham,Vojtech Franc*

Main category: cs.CV

TL;DR: 提出了一种利用图像中人脸信息进行照片年代估计的新方法，并发布了包含160多万标注人脸的CSFD-1.6M数据集。通过结合面部识别、年龄估计和基于职业生涯的时间先验，所提出的概率框架在多脸图像中显著优于基于场景的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的照片年代估计方法主要依赖于场景信息，但在包含多人的图像中未能充分利用人脸信息。为了提升这类图像的年代估计性能，需要一种能够有效聚合多个人脸线索的方法。

Method: 提出一个概率框架，融合现代人脸识别模型和年龄估计模型提供的视觉证据，以及基于人物职业生涯的时间先验信息，对照片拍摄年份进行推断；使用新发布的CSFD-1.6M数据集（含多个人脸及其出生年份标注）进行训练与验证。

Result: 实验表明，聚合多个人脸的信息能持续提升性能，在含有多个可识别个体的图像上显著优于强健的基于场景的基线方法。

Conclusion: 利用多人脸信息并结合身份与年龄线索的概率框架能有效提高照片年代估计的准确性，尤其适用于含多个已知人物的图像，如电影剧照等。

Abstract: We introduce a novel method for Photo Dating which estimates the year a
photograph was taken by leveraging information from the faces of people present
in the image. To facilitate this research, we publicly release CSFD-1.6M, a new
dataset containing over 1.6 million annotated faces, primarily from movie
stills, with identity and birth year annotations. Uniquely, our dataset
provides annotations for multiple individuals within a single image, enabling
the study of multi-face information aggregation. We propose a probabilistic
framework that formally combines visual evidence from modern face recognition
and age estimation models, and career-based temporal priors to infer the photo
capture year. Our experiments demonstrate that aggregating evidence from
multiple faces consistently improves the performance and the approach
significantly outperforms strong, scene-based baselines, particularly for
images containing several identifiable individuals.

</details>


### [46] [EventFlow: Real-Time Neuromorphic Event-Driven Classification of Two-Phase Boiling Flow Regimes](https://arxiv.org/abs/2511.05467)
*Sanghyeon Chang,Srikar Arani,Nishant Sai Nuthalapati,Youngjoon Suh,Nicholas Choi,Siavash Khodakarami,Md Rakibul Hasan Roni,Nenad Miljkovic,Aparna Chandramowlishwaran,Yoonjin Won*

Main category: cs.CV

TL;DR: 提出了一种基于神经形态传感器事件数据的实时流动模式分类框架，用于高效、低延迟的沸腾传热监测。


<details>
  <summary>Details</summary>
Motivation: 传统光学成像方法计算量大、时间分辨率不足，难以实时捕捉流动状态的瞬态变化，因此需要一种高响应、低延迟的实时监测方法。

Method: 利用神经形态传感器获取亮度变化事件数据，构建五种分类模型（包括LSTM），并与基于图像的传统方法对比，采用异步处理和多数投票机制实现稳定预测。

Result: 事件驱动的LSTM模型在0.28毫秒处理时间内达到97.6%的分类准确率，显著优于帧基方法，实现了连续、低延迟、高可靠的实时流动模式识别。

Conclusion: 基于事件的神经形态传感结合LSTM模型为流动沸腾监测提供了一种高性能的实时解决方案，适用于智能热管理系统中的动态反馈控制。

Abstract: Flow boiling is an efficient heat transfer mechanism capable of dissipating
high heat loads with minimal temperature variation, making it an ideal thermal
management method. However, sudden shifts between flow regimes can disrupt
thermal performance and system reliability, highlighting the need for accurate
and low-latency real-time monitoring. Conventional optical imaging methods are
limited by high computational demands and insufficient temporal resolution,
making them inadequate for capturing transient flow behavior. To address this,
we propose a real-time framework based on signals from neuromorphic sensors for
flow regime classification. Neuromorphic sensors detect changes in brightness
at individual pixels, which typically correspond to motion at edges, enabling
fast and efficient detection without full-frame reconstruction, providing
event-based information. We develop five classification models using both
traditional image data and event-based data, demonstrating that models
leveraging event data outperform frame-based approaches due to their
sensitivity to dynamic flow features. Among these models, the event-based long
short-term memory model provides the best balance between accuracy and speed,
achieving 97.6% classification accuracy with a processing time of 0.28 ms. Our
asynchronous processing pipeline supports continuous, low-latency predictions
and delivers stable output through a majority voting mechanisms, enabling
reliable real-time feedback for experimental control and intelligent thermal
management.

</details>


### [47] [Visual Spatial Tuning](https://arxiv.org/abs/2511.05491)
*Rui Yang,Ziyu Zhu,Yanwei Li,Jingjia Huang,Shen Yan,Siyuan Zhou,Zhe Liu,Xiangtai Li,Shuangye Li,Wenqian Wang,Yi Lin,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出了视觉空间调优（VST）框架，通过大规模数据集VST-P和VST-R以及渐进式训练方法，在不损害通用能力的前提下显著提升了视觉-语言模型的空间感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 为了在通用架构中增强视觉-语言模型（VLMs）的空间理解能力，避免引入额外专家编码器带来的开销和对通用性能的负面影响。

Method: 构建了两个数据集：VST-P（410万样本，涵盖单视图、多图像和视频中的19种技能）用于提升空间感知；VST-R（13.5万样本）用于空间推理。采用渐进式训练策略：先进行监督微调建立基础空间知识，再通过强化学习提升空间推理能力。

Result: 在多个空间基准测试上达到最先进性能，如MMSI-Bench上达到34.8%，VSIBench上达到61.2%，且未对模型通用能力产生副作用。

Conclusion: 所提出的VST框架能有效提升视觉-语言-动作模型的空间能力，为实现更物理 grounded 的人工智能提供了可行路径。

Abstract: Capturing spatial relationships from visual inputs is a cornerstone of
human-like general intelligence. Several previous studies have tried to enhance
the spatial awareness of Vision-Language Models (VLMs) by adding extra expert
encoders, which brings extra overhead and usually harms general capabilities.
To enhance the spatial ability in general architectures, we introduce Visual
Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with
human-like visuospatial abilities, from spatial perception to reasoning. We
first attempt to enhance spatial perception in VLMs by constructing a
large-scale dataset termed VST-P, which comprises 4.1 million samples spanning
19 skills across single views, multiple images, and videos. Then, we present
VST-R, a curated dataset with 135K samples that instruct models to reason in
space. In particular, we adopt a progressive training pipeline: supervised
fine-tuning to build foundational spatial knowledge, followed by reinforcement
learning to further improve spatial reasoning abilities. Without the
side-effect to general capabilities, the proposed VST consistently achieves
state-of-the-art results on several spatial benchmarks, including $34.8\%$ on
MMSI-Bench and $61.2\%$ on VSIBench. It turns out that the
Vision-Language-Action models can be significantly enhanced with the proposed
spatial tuning paradigm, paving the way for more physically grounded AI.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [48] [Evaluating LLMs' Reasoning Over Ordered Procedural Steps](https://arxiv.org/abs/2511.04688)
*Adrita Anika,Md Messal Monem Miah*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型在从打乱的步骤中重建有序过程序列方面的能力，使用食谱数据集进行评估，并提出了一套综合评价框架。


<details>
  <summary>Details</summary>
Motivation: 为了测试和提升大语言模型在处理顺序敏感任务上的推理能力，特别是在步骤顺序直接影响结果的过程性序列中的表现。

Method: 采用食谱这一领域作为案例，利用Kendall's Tau、归一化最长公共子序列（NLCS）和归一化编辑距离（NED）等指标，在零样本和少样本设置下对多个大语言模型进行了评估。

Result: 分析表明，随着序列长度增加以及输入中步骤位移增大，模型性能下降，反映出当前大语言模型在处理更长且更混乱的过程性输入时存在局限性。

Conclusion: 当前的大语言模型在处理长且高度打乱的过程性序列时面临挑战，需要进一步改进以增强其程序性推理能力。

Abstract: Reasoning over procedural sequences, where the order of steps directly
impacts outcomes, is a critical capability for large language models (LLMs). In
this work, we study the task of reconstructing globally ordered sequences from
shuffled procedural steps, using a curated dataset of food recipes, a domain
where correct sequencing is essential for task success. We evaluate several
LLMs under zero-shot and few-shot settings and present a comprehensive
evaluation framework that adapts established metrics from ranking and sequence
alignment. These include Kendall's Tau, Normalized Longest Common Subsequence
(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects
of ordering quality. Our analysis shows that model performance declines with
increasing sequence length, reflecting the added complexity of longer
procedures. We also find that greater step displacement in the input,
corresponding to more severe shuffling, leads to further degradation. These
findings highlight the limitations of current LLMs in procedural reasoning,
especially with longer and more disordered inputs.

</details>


### [49] [Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks](https://arxiv.org/abs/2511.04689)
*Peiyu Li,Xiuxiu Tang,Si Chen,Ying Cheng,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: ATLAS是一种基于项目反应理论（IRT）的自适应测试框架，通过Fisher信息引导的题目选择，在大幅减少测试题量的同时保持评估精度，显著提高了大模型评估的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型评估依赖大量固定题目的基准测试，存在成本高、速度慢、题目质量参差不齐的问题，且无法区分题目的信息价值，导致评估结果可能受到低质量或标注错误题目的影响。

Method: 提出ATLAS框架，采用项目反应理论（IRT）对模型能力进行建模，并利用Fisher信息最大化原则动态选择最具信息量的测试题目，实现自适应测试；同时分析多个主流基准数据集中的题目质量，识别负区分度题目（标注错误）。

Result: 在HellaSwag等五个主要基准上验证，ATLAS仅用3-6%的题目（如HellaSwag从5608题减至42题）即可达到与完整测试集相当的能力估计精度（MAE=0.154），题目曝光率低于10%，模型间测试重叠度为16-27%；发现传统准确率排名与IRT能力排名存在显著差异，23-31%的模型排名变动超过10位。

Conclusion: ATLAS通过自适应选题显著提升了大模型评估的效率与科学性，揭示了传统静态评估中题目质量与模型排序的局限性，为未来高效、精准的模型评测提供了新范式。

Abstract: Large language model evaluation requires thousands of benchmark items, making
evaluations expensive and slow. Existing methods compute average accuracy
across fixed item sets, treating all items equally despite varying quality and
informativeness. We present ATLAS an adaptive testing framework using Item
Response Theory (IRT) to estimate model ability through Fisher
information-guided item selection. Our analysis of five major benchmarks
reveals that 3-6% of items exhibit negative discrimination, indicating
annotation errors that corrupt static evaluation. ATLAS achieves 90% item
reduction while maintaining measurement precision: on HellaSwag (5,608 items),
we match full-benchmark estimates using only 42 items with 0.154 MAE. Our
framework maintains item exposure rates below 10% and test overlap at 16-27%,
compared to static benchmarks where every model sees all items (100% exposure).
Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with
the same accuracy get different IRT scores, and 23-31% of all models shift by
more than 10 rank positions. Code and calibrated item banks are available at
https://github.com/Peiyu-Georgia-Li/ATLAS.git.

</details>


### [50] [SARC: Sentiment-Augmented Deep Role Clustering for Fake News Detection](https://arxiv.org/abs/2511.04692)
*Jingqing Wang,Jiaxing Shang,Rong Xu,Fei Hao,Tianjin Huang,Geyong Min*

Main category: cs.CL

TL;DR: 本文提出了一种名为SARC的基于情感增强角色聚类的框架，用于提升社交媒体中的虚假新闻检测性能。该方法通过结合评论文本表示与情感编码生成用户特征，并利用可微分深度聚类模块自动识别用户角色，进一步通过联合优化角色聚类与虚假新闻检测任务来提升模型效果。在RumourEval-19和Weibo-comp两个基准数据集上的实验表明，SARC在各项指标上均优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有虚假新闻检测方法通常将情感特征作为辅助信号，忽视了不同用户角色对相同情感极性的贡献差异，限制了对细微模式的捕捉能力。因此，需要一种能够区分用户角色并充分利用情感信息的方法来提升检测性能。

Method: 提出SARC框架：首先使用BiGRU和注意力机制进行评论文本表示，并结合情感编码生成用户特征；然后构建可微分深度聚类模块以自动分类用户角色；最后设计联合优化目标，同时优化角色聚类和虚假新闻检测任务。

Result: 在RumourEval-19和Weibo-comp两个数据集上，SARC在准确率、F1分数等所有评估指标上均优于现有基线模型，验证了其有效性。此外，消融实验表明角色聚类和联合训练策略对性能提升有显著贡献。

Conclusion: SARC通过引入情感增强的用户角色聚类机制，并采用联合优化策略，有效提升了虚假新闻检测的性能，证明了建模用户角色在该任务中的重要性。

Abstract: Fake news detection has been a long-standing research focus in social
networks. Recent studies suggest that incorporating sentiment information from
both news content and user comments can enhance detection performance. However,
existing approaches typically treat sentiment features as auxiliary signals,
overlooking role differentiation, that is, the same sentiment polarity may
originate from users with distinct roles, thereby limiting their ability to
capture nuanced patterns for effective detection. To address this issue, we
propose SARC, a Sentiment-Augmented Role Clustering framework which utilizes
sentiment-enhanced deep clustering to identify user roles for improved fake
news detection. The framework first generates user features through joint
comment text representation (with BiGRU and Attention mechanism) and sentiment
encoding. It then constructs a differentiable deep clustering module to
automatically categorize user roles. Finally, unlike existing approaches which
take fake news label as the unique supervision signal, we propose a joint
optimization objective integrating role clustering and fake news detection to
further improve the model performance. Experimental results on two benchmark
datasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior
performance across all metrics compared to baseline models. The code is
available at: https://github.com/jxshang/SARC.

</details>


### [51] [Reasoning Up the Instruction Ladder for Controllable Language Models](https://arxiv.org/abs/2511.04694)
*Zishuo Zheng,Vidhisha Balachandran,Chan Young Park,Faeze Brahman,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出将指令层级解析重构为推理任务，通过构建VerIH数据集并使用轻量级强化学习训练模型，使其能够优先处理高优先级指令，从而提升大语言模型在复杂场景下的可控性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键决策场景中的应用，需解决来自用户、系统和工具等多源指令的冲突问题，确保高优先级指令（如安全策略）能有效覆盖低优先级请求，以提高模型的可靠性和可控性。

Method: 将指令层级处理视为推理任务，要求模型先推理用户提示与高优先级系统指令之间的关系；构建包含一致与冲突指令对的VerIH数据集，并采用轻量级强化学习进行微调。

Result: 微调后的模型在指令遵循和指令层级基准上表现持续提升，且推理能力可泛化至训练分布之外的安全关键场景，显著增强对越狱和提示注入攻击的防御能力。

Conclusion: 通过推理处理指令层级是一种实现可靠、可控大语言模型的有效路径，系统指令的更新可带来行为上的可预测调整，具有实际应用价值。

Abstract: As large language model (LLM) based systems take on high-stakes roles in
real-world decision-making, they must reconcile competing instructions from
multiple sources (e.g., model developers, users, and tools) within a single
prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where
higher-level directives override lower-priority requests, is critical for the
reliability and controllability of LLMs. In this work, we reframe instruction
hierarchy resolution as a reasoning task. Specifically, the model must first
"think" about the relationship between a given user prompt and higher-priority
(system) instructions before generating a response. To enable this capability
via training, we construct VerIH, an instruction hierarchy dataset of
constraint-following tasks with verifiable answers. This dataset comprises both
aligned and conflicting system-user instructions. We show that lightweight
reinforcement learning with VerIH effectively transfers general reasoning
capabilities of models to instruction prioritization. Our finetuned models
achieve consistent improvements on instruction following and instruction
hierarchy benchmarks. This reasoning ability also generalizes to
safety-critical settings beyond the training distribution. By treating safety
issues as resolving conflicts between adversarial user inputs and predefined
higher-priority policies, our trained model enhances robustness against
jailbreak and prompt injection attacks. These results demonstrate that
reasoning over instruction hierarchies provides a practical path to reliable
LLMs, where updates to system prompts yield controllable and robust changes in
model behavior.

</details>


### [52] [EncouRAGe: Evaluating RAG Local, Fast, and Reliable](https://arxiv.org/abs/2511.04696)
*Jan Strich,Adeline Scharfenberg,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: EncouRAGe是一个用于简化基于大语言模型和嵌入模型的检索增强生成（RAG）系统开发与评估的Python框架，包含五个模块化组件，支持灵活实验和本地部署，在多个基准数据集上进行了广泛评估。


<details>
  <summary>Details</summary>
Motivation: 为了提升RAG系统的可复现性、评估多样性和开发效率，推动RAG技术的研究与发展。

Method: 设计并实现了一个包含Type Manifest、RAG Factory、Inference、Vector Store和Metrics五个模块的Python框架EncouRAGe，支持模块化、可扩展的RAG系统构建与评估。

Result: 在包含25k问答对和51k文档的数据集上评估显示，RAG性能仍低于Oracle Context，Hybrid BM25在所有四个数据集中表现最佳；重排序仅带来轻微性能提升但增加响应延迟。

Conclusion: EncouRAGe为RAG系统提供了高效、可复现的开发与评估平台，实验揭示了当前RAG方法的局限性，为未来优化提供了方向。

Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to
streamline the development and evaluation of Retrieval-Augmented Generation
(RAG) systems using Large Language Models (LLMs) and Embedding Models.
EncouRAGe comprises five modular and extensible components: Type Manifest, RAG
Factory, Inference, Vector Store, and Metrics, facilitating flexible
experimentation and extensible development. The framework emphasizes scientific
reproducibility, diverse evaluation metrics, and local deployment, enabling
researchers to efficiently assess datasets within RAG workflows. This paper
presents implementation details and an extensive evaluation across multiple
benchmark datasets, including 25k QA pairs and over 51k documents. Our results
show that RAG still underperforms compared to the Oracle Context, while Hybrid
BM25 consistently achieves the best results across all four datasets. We
further examine the effects of reranking, observing only marginal performance
improvements accompanied by higher response latency.

</details>


### [53] [multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder](https://arxiv.org/abs/2511.04698)
*K M Sajjadul Islam,John Fields,Praveen Madiraju*

Main category: cs.CL

TL;DR: 本文提出了一种名为multiMentalRoBERTa的细调RoBERTa模型，用于从社交媒体文本中多分类识别常见心理疾病（如抑郁、焦虑、PTSD、自杀意念等），在六类和五类任务中均表现出优越性能，并结合可解释性方法分析关键词汇特征，强调模型在敏感场景下的可靠性与可部署性。


<details>
  <summary>Details</summary>
Motivation: 早期从社交媒体文本中检测心理健康问题对于及时提供支持和风险评估至关重要，现有方法在多类别识别和可解释性方面仍有不足。

Method: 基于多个精选数据集对RoBERTa模型进行细调，构建multiMentalRoBERTa用于多分类任务；采用数据探索分析类别重叠情况，并使用Layer Integrated Gradients和KeyBERT等方法提升模型可解释性。

Result: multiMentalRoBERTa在六类和五类分类任务中分别取得0.839和0.870的macro F1分数，优于传统机器学习方法、领域专用模型（如MentalBERT）及大语言模型提示方法；发现抑郁与自杀意念、焦虑与PTSD之间存在强相关性，压力类别具有广泛重叠性。

Conclusion: 细调后的Transformer模型在心理健康文本识别中具有高效、轻量且可解释的优势，具备实际部署潜力，但需结合公平性、偏见缓解和人工干预机制以确保安全应用。

Abstract: The early detection of mental health disorders from social media text is
critical for enabling timely support, risk assessment, and referral to
appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned
RoBERTa model designed for multiclass classification of common mental health
conditions, including stress, anxiety, depression, post-traumatic stress
disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple
curated datasets, data exploration is conducted to analyze class overlaps,
revealing strong correlations between depression and suicidal ideation as well
as anxiety and PTSD, while stress emerges as a broad, overlapping category.
Comparative experiments with traditional machine learning methods,
domain-specific transformers, and prompting-based large language models
demonstrate that multiMentalRoBERTa achieves superior performance, with macro
F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup
(excluding stress), outperforming both fine-tuned MentalBERT and baseline
classifiers. Beyond predictive accuracy, explainability methods, including
Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues
that drive classification, with a particular focus on distinguishing depression
from suicidal ideation. The findings emphasize the effectiveness of fine-tuned
transformers for reliable and interpretable detection in sensitive contexts,
while also underscoring the importance of fairness, bias mitigation, and
human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as
a lightweight, robust, and deployable solution for enhancing support in mental
health platforms.

</details>


### [54] [Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding](https://arxiv.org/abs/2511.04699)
*Haneen Al-Homoud,Asma Ibrahim,Murtadha Al-Jubran,Fahad Al-Otaibi,Yazeed Al-Harbi,Daulet Toibazar,Kesen Wang,Pedro J. Moreno*

Main category: cs.CL

TL;DR: Cross-Lingual SynthDocs是一个大规模合成语料库，旨在解决阿拉伯语OCR和文档理解资源稀缺的问题，包含超过250万样本，涵盖文本、表格和图表，显著提升了多模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语在光学字符识别（OCR）和文档理解（DU）领域缺乏高质量训练资源的问题。

Method: 通过真实扫描背景、双语布局和带音标字体的合成管道生成包含文本、表格和图表的多样化阿拉伯语文档数据。

Result: 在多个公开阿拉伯语基准上，微调Qwen-2.5-VL模型后，词错误率（WER）和字符错误率（CER）显著降低，表格和图表解析的TEDS和CharTeX评分也得到提升。

Conclusion: SynthDocs提供了一个可扩展且视觉真实的资源，推动多语言文档分析研究的发展。

Abstract: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address
the scarcity of Arabic resources for Optical Character Recognition (OCR) and
Document Understanding (DU). The dataset comprises over 2.5 million of samples,
including 1.5 million textual data, 270K fully annotated tables, and hundred
thousands of real data based charts. Our pipeline leverages authentic scanned
backgrounds, bilingual layouts, and diacritic aware fonts to capture the
typographic and structural complexity of Arabic documents. In addition to text,
the corpus includes variety of rendered styles for charts and tables.
Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word
Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple
public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart
Extraction Score (CharTeX) improved as well in other modalities. SynthDocs
provides a scalable, visually realistic resource for advancing research in
multilingual document analysis.

</details>


### [55] [Measuring what Matters: Construct Validity in Large Language Model Benchmarks](https://arxiv.org/abs/2511.04703)
*Andrew M. Bean,Ryan Othniel Kearns,Angelika Romanou,Franziska Sofia Hafner,Harry Mayne,Jan Batzner,Negar Foroutan,Chris Schmitz,Karolina Korgul,Hunar Batra,Oishi Deb,Emma Beharry,Cornelius Emde,Thomas Foster,Anna Gausen,María Grandury,Simeng Han,Valentin Hofmann,Lujain Ibrahim,Hazel Kim,Hannah Rose Kirk,Fangru Lin,Gabrielle Kaili-May Liu,Lennart Luettgau,Jabez Magomere,Jonathan Rystrøm,Anna Sotnikova,Yushi Yang,Yilun Zhao,Adel Bibi,Antoine Bosselut,Ronald Clark,Arman Cohan,Jakob Foerster,Yarin Gal,Scott A. Hale,Inioluwa Deborah Raji,Christopher Summerfield,Philip H. S. Torr,Cozmin Ududec,Luc Rocher,Adam Mahdi*

Main category: cs.CL

TL;DR: 本文系统评估了445个大语言模型（LLM）基准测试，发现当前在衡量“安全性”和“鲁棒性”等复杂现象时存在构念效度不足的问题，并基于29位专家的评审提出了八项改进建议。


<details>
  <summary>Details</summary>
Motivation: 为了确保大语言模型在部署前的能力评估和安全、鲁棒性问题识别具有可靠性，需要提升基准测试的构念效度。

Method: 通过由29位专家组成的团队，对自然语言处理和机器学习顶会中的445个LLM基准进行系统性综述，分析所测现象、任务设计和评分指标的有效性。

Result: 发现了广泛存在的影响评估有效性的模式，如测量目标不明确、任务设计不合理、评分指标不匹配等，导致结论的构念效度不足。

Conclusion: 研究提出了八项具体可行的建议，以指导研究人员和实践者更科学地开发和使用LLM基准测试。

Abstract: Evaluating large language models (LLMs) is crucial for both assessing their
capabilities and identifying safety or robustness issues prior to deployment.
Reliably measuring abstract and complex phenomena such as 'safety' and
'robustness' requires strong construct validity, that is, having measures that
represent what matters to the phenomenon. With a team of 29 expert reviewers,
we conduct a systematic review of 445 LLM benchmarks from leading conferences
in natural language processing and machine learning. Across the reviewed
articles, we find patterns related to the measured phenomena, tasks, and
scoring metrics which undermine the validity of the resulting claims. To
address these shortcomings, we provide eight key recommendations and detailed
actionable guidance to researchers and practitioners in developing LLM
benchmarks.

</details>


### [56] [GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models](https://arxiv.org/abs/2511.04710)
*Hari Mohan Pandey,Anshul Gupta,Subham Sarkar,Minakshi Tomer,Schneider Johannes,Yan Gong*

Main category: cs.CL

TL;DR: 本文提出了GEMMA-SQL，一种基于开源Gemma 2B架构的轻量级高效文本到SQL模型，通过资源高效的微调方法和多种提示策略，在SPIDER基准上取得了优于多个先进基线模型的性能，展示了有效提示设计和指令微调在提升文本到SQL系统性能方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了降低用户与结构化数据库交互的技术门槛，研究者希望开发无需专业编程知识即可使用的自然语言接口，并探索在有限硬件资源下实现高性能文本到SQL转换的可行方案。

Method: 基于开源的Gemma 2B模型，采用资源高效、迭代式的微调方法，并结合少样本学习等多种提示策略进行指令微调，以提升SQL生成准确性。

Result: GEMMA-SQL Instruct版本在SPIDER基准上达到66.8%的Test-Suite准确率和63.3%的Exact Set Match准确率，优于IRNet、RYANSQL和CodeXDavinci等多个现有先进模型。

Conclusion: 有效的提示设计与针对性的指令微调可显著提升轻量级模型的文本到SQL转换性能，GEMMA-SQL为构建高效、可扩展且开放可用的文本到SQL系统提供了实用解决方案。

Abstract: Text-to-SQL systems enable users to interact with structured databases using
natural language, eliminating the need for specialized programming knowledge.
In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL
model built upon the open-source Gemma 2B architecture. Unlike many large
language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient,
iterative manner and can be deployed on low-cost hardware. Leveraging the
SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple
prompting strategies, including few-shot learning, to enhance SQL query
generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct,
achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy,
outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and
CodeXDavinci. The proposed approach demonstrates that effective prompt design
and targeted instruction tuning can significantly boost performance while
maintaining high scalability and adaptability. These results position GEMMA-SQL
as a practical, open-source alternative for robust and accessible text-to-SQL
systems.

</details>


### [57] [First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation](https://arxiv.org/abs/2511.04715)
*Dmytro Vitel,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本文提出理论和实证证据，表明在大语言模型中用于影响估计的“抵消效应”不可靠，中间注意力层比第一层更适合作为影响估计的依据，并提出了改进的跨层影响分数聚合方法及新的评估指标噪声检测率（NDR），实验表明首层并非最优选择。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型参数量巨大，现有影响估计方法受限于计算可行性，通常仅使用部分模型层，而先前研究认为第一层（嵌入层）最适合计算影响，本文旨在重新评估这一结论并提出更可靠的方法。

Method: 通过理论分析和实验证明抵消效应的不可靠性，比较不同层（特别是中间注意力层）在影响估计中的表现，探索非标准聚合方法（如排序和投票法）的效果，并提出无需重训练模型即可评估影响分数的新指标——噪声检测率（NDR）。

Result: 实验结果显示中间注意力层优于首层或末层；采用排名和投票等聚合方法显著提升性能；NDR指标相比抵消效应具有更强的预测能力；在多种类型和规模的大语言模型上验证了结论的普适性。

Conclusion: 大语言模型中的训练样本影响估计不应局限于第一层，中间注意力层结合改进的聚合策略和NDR评估指标可实现更准确可靠的分析，挑战了该领域此前的认知。

Abstract: Identifying how training samples influence/impact Large Language Model (LLM)
decision-making is essential for effectively interpreting model decisions and
auditing large-scale datasets. Current training sample influence estimation
methods (also known as influence functions) undertake this goal by utilizing
information flow through the model via its first-order and higher-order
gradient terms. However, owing to the large model sizes of today consisting of
billions of parameters, these influence computations are often restricted to
some subset of model layers to ensure computational feasibility. Prior seminal
work by Yeh et al. (2022) in assessing which layers are best suited for
computing language data influence concluded that the first (embedding) layers
are the most informative for this purpose, using a hypothesis based on
influence scores canceling out (i.e., the cancellation effect). In this work,
we propose theoretical and empirical evidence demonstrating how the
cancellation effect is unreliable, and that middle attention layers are better
estimators for influence. Furthermore, we address the broader challenge of
aggregating influence scores across layers, and showcase how alternatives to
standard averaging (such as ranking and vote-based methods) can lead to
significantly improved performance. Finally, we propose better methods for
evaluating influence score efficacy in LLMs without undertaking model
retraining, and propose a new metric known as the Noise Detection Rate (NDR)
that exhibits strong predictive capability compared to the cancellation effect.
Through extensive experiments across LLMs of varying types and scales, we
concretely determine that the first (layers) are not necessarily better than
the last (layers) for LLM influence estimation, contrasting with prior
knowledge in the field.

</details>


### [58] [Surprisal reveals diversity gaps in image captioning and different scorers change the story](https://arxiv.org/abs/2511.04754)
*Nikolai Ilinykh,Simon Dobnik*

Main category: cs.CL

TL;DR: 本文提出了一种基于惊讶度方差的图像描述多样性度量方法，发现使用不同语言模型评分时，人类与模型生成描述的多样性关系会发生反转，强调多样性评估需结合多个评分器以确保结论的稳健性。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地衡量图像描述任务中的语言多样性，避免因单一评分模型导致错误结论。

Method: 提出使用惊讶度方差作为多样性指标，在MSCOCO测试集上比较了五种先进视觉-语言大模型与人类描述的差异，并采用n-gram语言模型和通用语言模型进行重评分分析。

Result: 使用描述训练的语言模型时，人类描述的惊讶度方差约为模型的两倍；但使用通用语言模型时该模式反转。单一评分器可能导致结论完全相反。

Conclusion: 惊讶度方差是一个有效的多样性度量指标，但多样性评估应结合多个语言模型评分，以提高评估的可靠性。

Abstract: We quantify linguistic diversity in image captioning with surprisal variance
- the spread of token-level negative log-probabilities within a caption set. On
the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs,
decoded with greedy and nucleus sampling, to human captions. Measured with a
caption-trained n-gram LM, humans display roughly twice the surprisal variance
of models, but rescoring the same captions with a general-language model
reverses the pattern. Our analysis introduces the surprisal-based diversity
metric for image captioning. We show that relying on a single scorer can
completely invert conclusions, thus, robust diversity evaluation must report
surprisal under several scorers.

</details>


### [59] [Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2511.04800)
*Chenxi Liu,Junjie Liang,Yuqi Jia,Bochuan Cao,Yang Bai,Heng Huang,Xun Chen*

Main category: cs.CL

TL;DR: 提出ERPO框架以增强在残差提示上的探索，重新激活训练信号，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着模型训练时间增加和规模扩大，越来越多的训练提示变为残差提示（奖励方差为零），导致有效训练信号减少，训练多样性下降，影响强化学习效果。

Method: 提出Explore Residual Prompts in Policy Optimization (ERPO)框架，通过为每个提示维护历史记录，并对曾生成全正确响应的残差提示自适应提高采样温度，鼓励模型生成更多样化的推理路径，从而引入错误响应以恢复训练信号。

Result: 在Qwen2.5系列模型上的实验表明，ERPO在多个数学推理基准上持续优于强基线方法。

Conclusion: ERPO能有效利用残差提示，提升训练信号的利用率和推理性能，为大规模语言模型的强化学习提供了更具扩展性的优化方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an
effective approach for improving the reasoning abilities of large language
models (LLMs). The Group Relative Policy Optimization (GRPO) family has
demonstrated strong performance in training LLMs with RLVR. However, as models
train longer and scale larger, more training prompts become residual prompts,
those with zero variance rewards that provide no training signal. Consequently,
fewer prompts contribute to training, reducing diversity and hindering
effectiveness. To fully exploit these residual prompts, we propose the Explore
Residual Prompts in Policy Optimization (ERPO) framework, which encourages
exploration on residual prompts and reactivates their training signals. ERPO
maintains a history tracker for each prompt and adaptively increases the
sampling temperature for residual prompts that previously produced all correct
responses. This encourages the model to generate more diverse reasoning traces,
introducing incorrect responses that revive training signals. Empirical results
on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong
baselines across multiple mathematical reasoning benchmarks.

</details>


### [60] [Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs](https://arxiv.org/abs/2511.04869)
*Preetum Nakkiran,Arwen Bradley,Adam Goliński,Eugene Ndiaye,Michael Kirchhof,Sinead Williamson*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在生成回答时的语义校准能力，即模型对其输出语义正确性的置信度估计。研究发现，基础LLM在开放域问答任务中表现出良好的语义校准性，尽管未被显式训练于此。作者提出“B-校准”理论框架，解释语义校准如何作为下一词预测训练的副产品自然涌现，并通过实验证实：基础LLM在问答任务中具备语义校准性，而RL指令微调和思维链推理会破坏这种校准性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常缺乏对其输出的可靠置信度估计。虽然已知基础LLM在词元级别具有校准性，但其是否能在语义层面评估自身回答的置信度尚不清楚。本文旨在探究LLM在语义层面的校准能力及其成因。

Method: 作者提出了“B-校准”的广义定义，将校准性扩展到由等价类（如语义类别）定义的抽象层级。基于近期校准性与局部损失最优性的理论联系，构建了一个理论机制，解释语义校准如何从下一词预测训练中自然产生。该机制预测：当模型能轻松预测自身在语义答案类别上的分布时，语义校准就会出现。作者通过在多种问答任务上对基础LLM、经RL指令微调的LLM以及使用思维链推理的LLM进行实验，验证了该预测的三个推论。

Result: 实验结果表明：(1) 基础LLM在多种开放域问答任务中表现出显著的语义校准性；(2) 经过强化学习（RL）指令微调的模型系统性地破坏了这种语义校准性；(3) 使用思维链（chain-of-thought）推理同样会破坏语义校准。这些发现验证了理论预测。

Conclusion: 本研究首次为大语言模型中语义校准性的出现提供了原理性解释：语义校准是下一词预测训练目标下局部损失最优性的副产品。只要模型能在生成前有效预测其语义输出分布，校准性就会自然涌现。同时揭示了当前改进模型性能的技术（如RL指令微调和思维链）可能以牺牲置信度可靠性为代价，这对构建可信赖AI具有重要意义。

Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for
their outputs. While base LLMs are known to exhibit next-token calibration, it
remains unclear whether they can assess confidence in the actual meaning of
their responses beyond the token level. We find that, when using a certain
sampling-based notion of semantic calibration, base LLMs are remarkably
well-calibrated: they can meaningfully assess confidence in open-domain
question-answering tasks, despite not being explicitly trained to do so. Our
main theoretical contribution establishes a mechanism for why semantic
calibration emerges as a byproduct of next-token prediction, leveraging a
recent connection between calibration and local loss optimality. The theory
relies on a general definition of "B-calibration," which is a notion of
calibration parameterized by a choice of equivalence classes (semantic or
otherwise). This theoretical mechanism leads to a testable prediction: base
LLMs will be semantically calibrated when they can easily predict their own
distribution over semantic answer classes before generating a response. We
state three implications of this prediction, which we validate through
experiments: (1) Base LLMs are semantically calibrated across
question-answering tasks, (2) RL instruction-tuning systematically breaks this
calibration, and (3) chain-of-thought reasoning breaks calibration. To our
knowledge, our work provides the first principled explanation of when and why
semantic calibration emerges in LLMs.

</details>


### [61] [Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs](https://arxiv.org/abs/2511.04875)
*Matthew Bozoukov,Matthew Nguyen,Shubkarman Singh,Bart Bussmann,Patrick Leask*

Main category: cs.CL

TL;DR: 研究表明，通过低秩适配器（LoRA）对指令调优的大型语言模型进行微调，可诱导出行为自我意识，且该能力具有领域局部性、线性特征和可调控性。


<details>
  <summary>Details</summary>
Motivation: 探索大模型行为自我意识出现的最小条件及其机制，以应对潜在的安全风险。

Method: 在指令调优的大型语言模型上使用低秩适配器（LoRA）进行受控微调实验，并分析激活空间中的 steering 向量。

Result: 发现行为自我意识可通过一个秩为1的LoRA适配器可靠诱导；大部分行为变化可由激活空间中的单个steering向量捕获；自我意识具有非普遍性和任务领域局部性。

Conclusion: 行为自我意识是一种领域特定的线性特征，易于诱导和调节，其出现不依赖复杂结构。

Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness:
the ability to accurately describe or predict their own learned behaviors
without explicit supervision. This capability raises safety concerns as it may,
for example, allow models to better conceal their true abilities during
evaluation. We attempt to characterize the minimal conditions under which such
self-awareness emerges, and the mechanistic processes through which it
manifests. Through controlled finetuning experiments on instruction-tuned LLMs
with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably
induced using a single rank-1 LoRA adapter; (2) that the learned self-aware
behavior can be largely captured by a single steering vector in activation
space, recovering nearly all of the fine-tune's behavioral effect; and (3) that
self-awareness is non-universal and domain-localized, with independent
representations across tasks. Together, these findings suggest that behavioral
self-awareness emerges as a domain-specific, linear feature that can be easily
induced and modulated.

</details>


### [62] [SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents](https://arxiv.org/abs/2511.04910)
*Jaehoon Lee,Sohyun Kim,Wanggeun Park,Geon Lee,Seungkyung Kim,Minyoung Lee*

Main category: cs.CL

TL;DR: 本文提出了SDS KoPub VDR，首个面向韩语公共文档的大规模公开视觉文档检索基准，涵盖复杂版式与多模态内容，通过双任务评估揭示现有模型在跨模态推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉文档检索基准忽视非英语语言和官方文件的结构复杂性，缺乏针对韩语公共文档的高质量评测资源。

Method: 基于361份真实韩语公共文档（40,781页）构建数据集，使用多模态模型生成600个查询-页面-答案三元组，并经人工验证优化；设计文本检索与多模态检索两项任务，按文本、视觉、跨模态推理对问题分类。

Result: 建立了包含法律、行政等六大领域的600个高质量三元组测试集，在文本检索和多模态检索任务上评估了多种先进模型，发现当前模型在跨模态推理场景下性能显著下降。

Conclusion: SDS KoPub VDR为韩语复杂文档理解提供了可靠基准，揭示了多模态检索中跨模态推理的挑战，推动真实场景下多模态AI的发展。

Abstract: Existing benchmarks for visual document retrieval (VDR) largely overlook
non-English languages and the structural complexity of official publications.
To address this critical gap, we introduce SDS KoPub VDR, the first
large-scale, publicly available benchmark for retrieving and understanding
Korean public documents. The benchmark is built upon a corpus of 361 real-world
documents (40,781 pages), including 256 files under the KOGL Type 1 license and
105 from official legal portals, capturing complex visual elements like tables,
charts, and multi-column layouts. To establish a challenging and reliable
evaluation set, we constructed 600 query-page-answer triples. These were
initially generated using multimodal models (e.g., GPT-4o) and subsequently
underwent a rigorous human verification and refinement process to ensure
factual accuracy and contextual relevance. The queries span six major public
domains and are systematically categorized by the reasoning modality required:
text-based, visual-based (e.g., chart interpretation), and cross-modal. We
evaluate SDS KoPub VDR on two complementary tasks that reflect distinct
retrieval paradigms: (1) text-only retrieval, which measures a model's ability
to locate relevant document pages based solely on textual signals, and (2)
multimodal retrieval, which assesses retrieval performance when visual features
(e.g., tables, charts, and layouts) are jointly leveraged alongside text. This
dual-task evaluation reveals substantial performance gaps, particularly in
multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art
models. As a foundational resource, SDS KoPub VDR not only enables rigorous and
fine-grained evaluation across textual and multimodal retrieval tasks but also
provides a clear roadmap for advancing multimodal AI in complex, real-world
document intelligence.

</details>


### [63] [BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models](https://arxiv.org/abs/2511.04919)
*Chandra Vamsi Krishna Alla,Harish Naidu Gaddam,Manohar Kommi*

Main category: cs.CL

TL;DR: BudgetMem是一种新型的内存增强架构，通过选择性记忆机制和基于特征的显著性评分，在严格内存限制下实现高效长上下文处理，显著降低内存使用的同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长上下文时面临计算和内存限制，现有方法在扩展上下文窗口时带来过高资源开销，难以在资源受限环境下部署。

Method: 提出BudgetMem，结合选择性记忆策略与基于实体密度、TF-IDF、话语标记和位置偏置等特征的显著性评分，利用学习型门控机制和BM25稀疏检索，决定哪些信息在有限内存预算下值得存储。

Result: 在Llama-3.2-3B-Instruct上对700个问答对进行实验，长文档中仅损失1.0%的F1分数，内存节省72.4%；且性能优势随文档长度增加而提升。

Conclusion: BudgetMem为在普通硬件上部署高效的长上下文语言模型提供了可行方案，有助于普及高级语言理解能力。

Abstract: Large Language Models (LLMs) face significant computational and memory
constraints when processing long contexts, despite growing demand for
applications requiring reasoning over extensive documents, multi-session
dialogues, and book length texts. While recent advances have extended context
windows to 100K-1M tokens, such approaches incur prohibitive costs for resource
constrained deployments. We propose BudgetMem, a novel memory augmented
architecture that learns what to remember rather than remembering everything.
Our system combines selective memory policies with feature based salience
scoring (entity density, TF-IDF, discourse markers, position bias) to decide
which information merits storage under strict budget constraints. Unlike
existing retrieval augmented generation (RAG) systems that store all chunks,
BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval
for efficient information access. Through comprehensive experiments on 700
question answer pairs across short (237 tokens) and long (5K-10K tokens)
documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves
remarkable results on long documents: only 1.0% F1 score degradation while
saving 72.4% memory compared to baseline RAG. We validate our approach through
budget sensitivity analysis (testing 7 budget ratios), naive baseline
comparisons, and document length analysis, showing that BudgetMem's benefits
increase with document length. Our work provides a practical pathway for
deploying capable long context systems on modest hardware, democratizing access
to advanced language understanding capabilities.

</details>


### [64] [Diagnosing and Mitigating Semantic Inconsistencies in Wikidata's Classification Hierarchy](https://arxiv.org/abs/2511.04926)
*Shixiong Zhao,Hideaki Takeda*

Main category: cs.CL

TL;DR: 本研究提出了一种新的验证方法，用于检测Wikidata中特定领域的分类错误、过度泛化的子类链接和冗余连接，并引入了新的评估标准来判断这些问题是否需要修正，同时开发了一个系统以充分利用其众包特性来检查任意实体的分类关系。


<details>
  <summary>Details</summary>
Motivation: 由于Wikidata相对宽松的编辑政策导致了分类上的不一致性，因此需要一种有效的方法来识别并评估这些结构问题。

Method: 基于先前的研究，提出并应用一种新颖的验证方法来检测Wikidata中的分类错误和冗余连接，并设计一个新评估标准和检查系统。

Result: 确认了Wikidata特定领域中存在分类错误、过度泛化的子类链接和冗余连接，并开发出可评估问题严重性及支持用户审查的系统。

Conclusion: 该方法能够有效揭示Wikidata中的结构性质量问题，并通过众包机制为改进其分类体系提供了可行路径。

Abstract: Wikidata is currently the largest open knowledge graph on the web,
encompassing over 120 million entities. It integrates data from various
domain-specific databases and imports a substantial amount of content from
Wikipedia, while also allowing users to freely edit its content. This openness
has positioned Wikidata as a central resource in knowledge graph research and
has enabled convenient knowledge access for users worldwide. However, its
relatively loose editorial policy has also led to a degree of taxonomic
inconsistency. Building on prior work, this study proposes and applies a novel
validation method to confirm the presence of classification errors,
over-generalized subclass links, and redundant connections in specific domains
of Wikidata. We further introduce a new evaluation criterion for determining
whether such issues warrant correction and develop a system that allows users
to inspect the taxonomic relationships of arbitrary Wikidata
entities-leveraging the platform's crowdsourced nature to its full potential.

</details>


### [65] [LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model](https://arxiv.org/abs/2511.04952)
*Wei Shao,Lingchao Zheng,Pengyu Wang,Peizhen Zheng,Jun Li,Yuwei Fan*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoPT的无损并行分词框架，解决了长文本推理中因分词边界效应导致结果不一致的问题，通过基于字符位置的匹配和动态调整块长度，实现了与标准顺序分词完全一致的输出，同时显著提升了处理速度。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理场景对大语言模型日益重要，但现有并行分词方法由于边界效应导致合并后结果不一致，影响模型输入质量，因此需要一种既能加速又能保证分词准确性的新方法。

Method: 提出LoPT框架，采用基于字符位置的匹配策略和动态调整分块长度的方法，在多进程并行分词后精确对齐和合并分词结果，确保最终输出与顺序分词完全相同。

Result: 在多种长文本数据集上的实验表明，LoPT在保证无损分词的前提下显著提升了分词速度，并通过理论证明了其一致性，验证了方法的鲁棒性。

Conclusion: LoPT有效解决了并行分词中的边界问题，为长序列推理提供了一种高效且准确的分词方案，填补了该领域在系统优化中忽视分词瓶颈的空白。

Abstract: Long context inference scenarios have become increasingly important for large
language models, yet they introduce significant computational latency. While
prior research has optimized long-sequence inference through operators, model
architectures, and system frameworks, tokenization remains an overlooked
bottleneck. Existing parallel tokenization methods accelerate processing
through text segmentation and multi-process tokenization, but they suffer from
inconsistent results due to boundary artifacts that occur after merging. To
address this, we propose LoPT, a novel Lossless Parallel Tokenization framework
that ensures output identical to standard sequential tokenization. Our approach
employs character-position-based matching and dynamic chunk length adjustment
to align and merge tokenized segments accurately. Extensive experiments across
diverse long-text datasets demonstrate that LoPT achieves significant speedup
while guaranteeing lossless tokenization. We also provide theoretical proof of
consistency and comprehensive analytical studies to validate the robustness of
our method.

</details>


### [66] [Too Good to be Bad: On the Failure of LLMs to Role-Play Villains](https://arxiv.org/abs/2511.04962)
*Zihao Yi,Qingxuan Jiang,Ruotian Ma,Xingyu Chen,Qu Yang,Mengru Wang,Fanghua Ye,Ying Shen,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLM）在扮演道德模糊或反派角色时的表现，发现其安全性对齐机制显著削弱了对非亲社会角色的忠实模拟，提出了Moral RolePlay基准以系统评估该问题。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型经过安全对齐训练，可能难以真实模拟具有欺骗性、操纵性等负面特质的虚构反派角色，限制了其在创造性任务中的应用，因此需要系统评估这一冲突。

Method: 提出Moral RolePlay基准数据集，包含四级道德对齐量表和平衡测试集，对最先进的LLMs进行大规模评估，要求模型扮演从道德楷模到纯粹反派的角色，并分析其角色扮演保真度。

Result: 实验显示，随着角色道德水平下降，模型的角色扮演保真度呈单调递减；模型难以体现‘欺骗’和‘操纵’等特质，常以表面攻击性替代复杂恶意，且高安全对齐模型表现更差，通用对话能力无法预测反派扮演效果。

Conclusion: 安全性对齐与创造性角色扮演之间存在根本张力，当前LLM在模拟非亲社会角色方面存在系统性缺陷，需发展更细致、情境感知的对齐方法。

Abstract: Large Language Models (LLMs) are increasingly tasked with creative
generation, including the simulation of fictional characters. However, their
ability to portray non-prosocial, antagonistic personas remains largely
unexamined. We hypothesize that the safety alignment of modern LLMs creates a
fundamental conflict with the task of authentically role-playing morally
ambiguous or villainous characters. To investigate this, we introduce the Moral
RolePlay benchmark, a new dataset featuring a four-level moral alignment scale
and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs
with role-playing characters from moral paragons to pure villains. Our
large-scale evaluation reveals a consistent, monotonic decline in role-playing
fidelity as character morality decreases. We find that models struggle most
with traits directly antithetical to safety principles, such as ``Deceitful''
and ``Manipulative'', often substituting nuanced malevolence with superficial
aggression. Furthermore, we demonstrate that general chatbot proficiency is a
poor predictor of villain role-playing ability, with highly safety-aligned
models performing particularly poorly. Our work provides the first systematic
evidence of this critical limitation, highlighting a key tension between model
safety and creative fidelity. Our benchmark and findings pave the way for
developing more nuanced, context-aware alignment methods.

</details>


### [67] [Acquiring Common Chinese Emotional Events Using Large Language Model](https://arxiv.org/abs/2511.04989)
*Ya Wang,Guangzheng Zhu,Cungen Cao,Jingjing Li,He Li,Xin Huang*

Main category: cs.CL

TL;DR: 本文提出一种从中文大语言模型中生成高质量常见情感事件的方法，并构建了首个大规模中文情感事件常识知识库，包含102,218个带情感极性标签的情感事件。


<details>
  <summary>Details</summary>
Motivation: 常见或通用的、与上下文无关的情感事件难以获取，但这类知识在多种应用中具有重要价值，因此需要构建中文情感事件常识知识库。

Method: 首先收集中文情感事件指示词，利用这些指示词提示中文大语言模型生成情感事件；训练过滤器以去除无效结果，并使用不同技术对事件进行正负情感分类。

Result: 成功构建了包含102,218个高质量常见情感事件的知识库，具备情感极性标签；内在评估表明该方法有效，外在用例显示其在情感原因提取任务中具有应用潜力。

Conclusion: 所提方法能有效获取中文常见情感事件，构建的知识库填补了中文情感事件常识资源的空白，具备实际应用价值。

Abstract: Knowledge about emotional events is an important kind of knowledge which has
been applied to improve the effectiveness of different applications. However,
emotional events cannot be easily acquired, especially common or generalized
emotional events that are context-independent. The goal of this paper is to
obtain common emotional events in Chinese language such as "win a prize" and
"be criticized". Our approach begins by collecting a comprehensive list of
Chinese emotional event indicators. Then, we generate emotional events by
prompting a Chinese large language model (LLM) using these indicators. To
ensure the quality of these emotional events, we train a filter to discard
invalid generated results. We also classify these emotional events as being
positive events and negative events using different techniques. Finally, we
harvest a total of 102,218 high-quality common emotional events with sentiment
polarity labels, which is the only large-scale commonsense knowledge base of
emotional events in Chinese language. Intrinsic evaluation results show that
the proposed method in this paper can be effectively used to acquire common
Chinese emotional events. An extrinsic use case also demonstrates the strong
potential of common emotional events in the field of emotion cause extraction
(ECE). Related resources including emotional event indicators and emotional
events will be released after the publication of this paper.

</details>


### [68] [UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian](https://arxiv.org/abs/2511.05040)
*Mykyta Syromiatnikov,Victoria Ruvinskaya*

Main category: cs.CL

TL;DR: 本文提出了UA-Code-Bench，一个用于评估乌克兰语代码生成和竞赛编程问题解决能力的新基准，揭示了现有大模型在低资源语言中的局限性，并提供了全面的性能分析。


<details>
  <summary>Details</summary>
Motivation: 现有的基准多关注从英语翻译的任务或仅评估简单的语言理解，难以真实反映大模型在低资源语言中的能力，因此需要针对非主流语言建立更具挑战性的评估方式。

Method: 构建包含500个来自Eolymp平台的问题的基准，覆盖五个难度等级，使用13个主流闭源和开源模型生成Python代码，通过单样本提示并在专用Eolymp环境中进行隐藏测试评估代码正确性，同时分析运行时间与内存消耗。

Result: 即使是表现最好的模型（如OpenAI o3和GPT-5）也仅能解决约一半的问题；研究还发现模型在不同难度级别上的表现差异显著，并对解法唯一性和计算效率进行了量化分析。

Conclusion: 竞争性编程基准对于评估大语言模型在低资源语言中的真实能力具有重要价值，该工作为多语言代码生成及增强推理模型的研究提供了基础和方向。

Abstract: Evaluating the real capabilities of large language models in low-resource
languages still represents a challenge, as many existing benchmarks focus on
widespread tasks translated from English or evaluate only simple language
understanding. This paper introduces UA-Code-Bench, a new open-source benchmark
established for a thorough evaluation of language models' code generation and
competitive programming problem-solving abilities in Ukrainian. The benchmark
comprises 500 problems from the Eolymp platform, evenly distributed across five
complexity levels from very easy to very hard. A diverse set of 13 leading
proprietary and open-source models, generating Python solutions based on a
one-shot prompt, was evaluated via the dedicated Eolymp environment against
hidden tests, ensuring code correctness. The obtained results reveal that even
top-performing models, such as OpenAI o3 and GPT-5, solve only half of the
problems, highlighting the challenge of code generation in low-resource natural
language. Furthermore, this research presents a comprehensive analysis of
performance across various difficulty levels, as well as an assessment of
solution uniqueness and computational efficiency, measured by both elapsed time
and memory consumption of the generated solutions. In conclusion, this work
demonstrates the value of competitive programming benchmarks in evaluating
large language models, especially in underrepresented languages. It also paves
the way for future research on multilingual code generation and
reasoning-enhanced models. The benchmark, data parsing, preparation, code
generation, and evaluation scripts are available at
https://huggingface.co/datasets/NLPForUA/ua-code-bench.

</details>


### [69] [Order-Level Attention Similarity Across Language Models: A Latent Commonality](https://arxiv.org/abs/2511.05064)
*Jinglin Liang,Jin Zhong,Shuangping Huang,Yunqing Hu,Huiyuan Zhang,Huifang Li,Lixin Fan,Hanlin Gu*

Main category: cs.CL

TL;DR: 本文提出了Order-Level Attention（OLA），揭示了不同语言模型在相同阶数下的OLA具有显著相似性，并发现OLA与句法知识之间存在隐式映射。基于此，提出了一种无需训练的跨语言模型适配器迁移方法Transferable OLA Adapter（TOA），实验证明TOA能有效提升未见模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究不同语言模型在上下文聚合模式上是否存在共性，以加深对语言模型的理解并促进跨模型知识迁移。

Method: 通过注意力展开的逐阶分解引入Order-Level Attention（OLA），分析多个语言模型间的OLA相似性，并利用OLA作为统一的句法特征表示来训练适配器，实现无需参数更新的跨模型迁移。

Result: 发现了同阶OLA在不同语言模型间具有显著相似性，并揭示了OLA与句法知识之间的隐式关联；所提出的TOA方法在多个未见模型上表现出良好的泛化能力和性能提升。

Conclusion: OLA是一种具有跨模型一致性的可解释特征，基于OLA的TOA为无需训练的跨语言模型知识迁移提供了有效途径。

Abstract: In this paper, we explore an important yet previously neglected question: Do
context aggregation patterns across Language Models (LMs) share commonalities?
While some works have investigated context aggregation or attention weights in
LMs, they typically focus on individual models or attention heads, lacking a
systematic analysis across multiple LMs to explore their commonalities. In
contrast, we focus on the commonalities among LMs, which can deepen our
understanding of LMs and even facilitate cross-model knowledge transfer. In
this work, we introduce the Order-Level Attention (OLA) derived from the
order-wise decomposition of Attention Rollout and reveal that the OLA at the
same order across LMs exhibits significant similarities. Furthermore, we
discover an implicit mapping between OLA and syntactic knowledge. Based on
these two findings, we propose the Transferable OLA Adapter (TOA), a
training-free cross-LM adapter transfer method. Specifically, we treat the OLA
as a unified syntactic feature representation and train an adapter that takes
OLA as input. Due to the similarities in OLA across LMs, the adapter
generalizes to unseen LMs without requiring any parameter updates. Extensive
experiments demonstrate that TOA's cross-LM generalization effectively enhances
the performance of unseen LMs. Code is available at
https://github.com/jinglin-liang/OLAS.

</details>


### [70] [Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts](https://arxiv.org/abs/2511.05078)
*Manan Sharma,Arya Suneesh,Manish Jain,Pawan Kumar Rajpoot,Prasanna Devadiga,Bharatdeep Hazarika,Ashish Shrivastava,Kishan Gurumurthy,Anshuman B Suresh,Aditya U Baliga*

Main category: cs.CL

TL;DR: 本文提出一种基于六要素（Who, What, Where, When, Why, How）分解的多语言声明规范化方法，用于虚假信息检测，仅使用英文训练数据即可实现跨语言迁移。


<details>
  <summary>Details</summary>
Motivation: 社交媒体帖子通常表述混乱且多语言混杂，难以直接用于虚假信息检测，因此需要将其规范化为清晰、可验证的陈述。

Method: 采用LoRA微调Qwen3-14B模型，结合帖内去重、基于token级别的召回率过滤以提升语义对齐，并在推理阶段引入检索增强的少样本学习和上下文示例。

Result: 在20种语言上实现跨语言迁移，METEOR得分从英语的41.16到马拉地语的15.21不等，在英语、荷兰语和旁遮普语排行榜上分别排名第三和第四，相比基线模型有41.3%的相对提升。

Conclusion: 该方法在罗曼语族和日耳曼语族语言中表现出良好的跨语言泛化能力，能够在不同语言结构下保持语义一致性，有效支持多语言虚假信息检测。

Abstract: We address claim normalization for multilingual misinformation detection -
transforming noisy social media posts into clear, verifiable statements across
20 languages. The key contribution demonstrates how systematic decomposition of
posts using Who, What, Where, When, Why and How questions enables robust
cross-lingual transfer despite training exclusively on English data. Our
methodology incorporates finetuning Qwen3-14B using LoRA with the provided
dataset after intra-post deduplication, token-level recall filtering for
semantic alignment and retrieval-augmented few-shot learning with contextual
examples during inference. Our system achieves METEOR scores ranging from 41.16
(English) to 15.21 (Marathi), securing third rank on the English leaderboard
and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative
improvement in METEOR over baseline configurations and substantial gains over
existing methods. Results demonstrate effective cross-lingual generalization
for Romance and Germanic languages while maintaining semantic coherence across
diverse linguistic structures.

</details>


### [71] [On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class](https://arxiv.org/abs/2511.05080)
*P. Bilha Githinji,Aikaterini Meilliou,Peiwu Qin*

Main category: cs.CL

TL;DR: 本研究评估了两种通用大语言模型（Mistral 24B 和 QWen2.5 32B）在科学文本简化任务中的表现，发现指令调优的 Mistral 在提升可读性的同时更好保持了语义忠实性，表现出架构优势。


<details>
  <summary>Details</summary>
Motivation: 公众对生物医学信息的需求增加，亟需自动化方法将复杂科技文献转化为通俗语言，但现有模型在可读性与语义保真之间难以平衡。

Method: 通过对比指令调优的 Mistral 24B 与推理增强的 QWen2.5 32B 模型，在多个可读性、语义保真和安全性指标上进行实证分析，并开展相关性分析以揭示指标间的冗余性和机制洞察。

Result: Mistral 在 SARI 得分（均值 42.46）和 BERTScore（0.91）上优于 QWen（BERTScore 0.89），显示出更优的词汇简化策略和语义保持能力；21项指标的相关性分析揭示五种可读性指数存在功能冗余。

Conclusion: 指令调优的 LLM（如 Mistral 24B）更具文本简化潜力，词汇支持是领域适应的关键问题，研究为模型选择和评估指标优化提供了实证依据和启发式指导。

Abstract: The increasing health-seeking behavior and digital consumption of biomedical
information by the general public necessitate scalable solutions for
automatically adapting complex scientific and technical documents into plain
language. Automatic text simplification solutions, including advanced large
language models, however, continue to face challenges in reliably arbitrating
the tension between optimizing readability performance and ensuring
preservation of discourse fidelity. This report empirically assesses the
performance of two major classes of general-purpose LLMs, demonstrating their
linguistic capabilities and foundational readiness for the task compared to a
human benchmark. Using a comparative analysis of the instruction-tuned Mistral
24B and the reasoning-augmented QWen2.5 32B, we identify a potential
architectural advantage in the instruction-tuned LLM. Mistral exhibits a
tempered lexical simplification strategy that enhances readability across a
suite of metrics and the simplification-specific formula SARI (mean 42.46),
while preserving human-level discourse with a BERTScore of 0.91. QWen also
attains enhanced readability performance, but its operational strategy shows a
disconnect in balancing between readability and accuracy, reaching a
statistically significantly lower BERTScore of 0.89. Additionally, a
comprehensive correlation analysis of 21 metrics spanning readability,
discourse fidelity, content safety, and underlying distributional measures for
mechanistic insights, confirms strong functional redundancies among five
readability indices. This empirical evidence tracks baseline performance of the
evolving LLMs for the task of text simplification, identifies the
instruction-tuned Mistral 24B for simplification, provides necessary heuristics
for metric selection, and points to lexical support as a primary
domain-adaptation issue for simplification.

</details>


### [72] [Iterative Layer-wise Distillation for Efficient Compression of Large Language Models](https://arxiv.org/abs/2511.05085)
*Grigory Kovalev,Mikhail Tikhomirov*

Main category: cs.CL

TL;DR: 本文提出了一种改进的基于ShortGPT的迭代蒸馏方法，通过评估各层的重要性并结合KL散度与均方误差的联合损失函数进行微调，成功将Qwen2.5-3B模型从36层压缩至24-28层，在仅损失9.7%-18%性能的同时显著降低参数量，验证了该方法在资源受限场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了在保持大语言模型高性能的同时减小模型规模，提升其在资源受限环境中的部署能力，研究现有的模型蒸馏方法并改进其效率和效果。

Method: 基于ShortGPT方法，引入迭代式层重要性评估机制：每一步通过移除单个层并在代表性数据集上测量性能下降来判断其重要性；结合KL散度与均方误差的联合损失函数进行进一步微调，实现高效的知识迁移。

Result: 在Qwen2.5-3B模型上实验表明，可将其层数从36层压缩至28层（参数降至2.47B），性能损失仅为9.7%；压缩至24层时性能损失为18%。中间Transformer层对推理贡献较小，验证了方法的有效性。

Conclusion: 所提出的迭代蒸馏与微调方法能有效压缩大语言模型，在显著减少参数量的同时保留大部分性能，适合用于资源受限场景，具有较强的实用价值。

Abstract: This work investigates distillation methods for large language models (LLMs)
with the goal of developing compact models that preserve high performance.
Several existing approaches are reviewed, with a discussion of their respective
strengths and limitations. An improved method based on the ShortGPT approach
has been developed, building upon the idea of incorporating iterative
evaluation of layer importance. At each step, importance is assessed by
measuring performance degradation when individual layers are removed, using a
set of representative datasets. This process is combined with further training
using a joint loss function based on KL divergence and mean squared error.
Experiments on the Qwen2.5-3B model show that the number of layers can be
reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a
9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that
the middle transformer layers contribute less to inference, underscoring the
potential of the proposed method for creating efficient models. The results
demonstrate the effectiveness of iterative distillation and fine-tuning, making
the approach suitable for deployment in resource-limited settings.

</details>


### [73] [A Toolbox for Improving Evolutionary Prompt Search](https://arxiv.org/abs/2511.05120)
*Daniel Grießhaber,Maximilian Kimmich,Johannes Maucher,Ngoc Thang Vu*

Main category: cs.CL

TL;DR: 提出了一种改进的进化式提示优化方法，通过分解进化步骤、引入基于LLM的评判器、整合人工反馈和开发更高效的评估策略，提升了优化质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的进化式提示优化方法缺乏强大的算子和高效的评估机制，限制了其在大语言模型提示优化中的效果和应用。

Method: 将进化过程分解为不同的步骤，引入基于大语言模型的评判器来验证进化结果，结合人工反馈优化进化算子，并设计更高效的评估策略以降低计算开销。

Result: 所提方法在保持性能的同时显著提高了提示优化的质量和效率，且具备一定的通用性。

Conclusion: 该研究有效改进了进化式提示优化框架，为提示优化任务提供了可复用的代码和新的研究方向。

Abstract: Evolutionary prompt optimization has demonstrated effectiveness in refining
prompts for LLMs. However, existing approaches lack robust operators and
efficient evaluation mechanisms. In this work, we propose several key
improvements to evolutionary prompt optimization that can partially generalize
to prompt optimization in general: 1) decomposing evolution into distinct steps
to enhance the evolution and its control, 2) introducing an LLM-based judge to
verify the evolutions, 3) integrating human feedback to refine the evolutionary
operator, and 4) developing more efficient evaluation strategies that maintain
performance while reducing computational overhead. Our approach improves both
optimization quality and efficiency. We release our code, enabling prompt
optimization on new tasks and facilitating further research in this area.

</details>


### [74] [ManufactuBERT: Efficient Continual Pretraining for Manufacturing](https://arxiv.org/abs/2511.05135)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: 本文提出了ManufactuBERT，一种在大规模制造领域语料上持续预训练的RoBERTa模型，通过精心构建和去重的数据集显著提升了在制造相关NLP任务上的性能，并减少了33%的训练时间和计算成本。


<details>
  <summary>Details</summary>
Motivation: 通用Transformer编码器在专业领域（如制造）表现不佳，因其缺乏对领域术语和语义的接触，因此需要专门针对制造领域优化的语言模型。

Method: 提出了一种多阶段数据处理流程，包括领域特定过滤和多阶段去重，构建大规模制造领域语料库，并在此基础上对RoBERTa模型进行持续预训练。

Result: ManufactuBERT在多个制造相关的NLP任务上达到最先进水平，优于强基线模型；使用去重后的数据使训练收敛速度提升，训练时间与计算成本降低33%。

Conclusion: 该研究展示了领域特定数据质量与持续预训练对专业NLP模型的重要性，所提出的流程具有可复制性，可用于其他专业领域高性能编码器的开发。

Abstract: While large general-purpose Transformer-based encoders excel at general
language understanding, their performance diminishes in specialized domains
like manufacturing due to a lack of exposure to domain-specific terminology and
semantics. In this paper, we address this gap by introducing ManufactuBERT, a
RoBERTa model continually pretrained on a large-scale corpus curated for the
manufacturing domain. We present a comprehensive data processing pipeline to
create this corpus from web data, involving an initial domain-specific
filtering step followed by a multi-stage deduplication process that removes
redundancies. Our experiments show that ManufactuBERT establishes a new
state-of-the-art on a range of manufacturing-related NLP tasks, outperforming
strong specialized baselines. More importantly, we demonstrate that training on
our carefully deduplicated corpus significantly accelerates convergence,
leading to a 33\% reduction in training time and computational cost compared to
training on the non-deduplicated dataset. The proposed pipeline offers a
reproducible example for developing high-performing encoders in other
specialized domains. We will release our model and curated corpus at
https://huggingface.co/cea-list-ia.

</details>


### [75] [Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models](https://arxiv.org/abs/2511.05184)
*Cong-Thanh Do,Rama Doddipatla,Kate Knill*

Main category: cs.CL

TL;DR: 本文研究了思维链（CoT）提示在白盒知识蒸馏（KD）中从大语言模型向小语言模型传递推理能力的作用，实验表明CoT能有效提升小模型在自然语言推理与理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在探索如何通过CoT增强知识蒸馏过程，以更有效地将大型语言模型的推理能力迁移给小型语言模型，从而提升后者在复杂任务上的表现。

Method: 采用Qwen和Llama2系列的大语言模型进行白盒知识蒸馏实验，使用CoT-Collection数据集中的思维链数据，并在BIG-Bench-Hard（BBH）基准的任务上评估蒸馏后模型的表现。

Result: 实验结果表明，利用CoT进行白盒知识蒸馏可显著提升小型语言模型在BBH基准上的平均性能，验证了CoT在知识蒸馏中对推理能力迁移的有效性。

Conclusion: CoT在白盒知识蒸馏中起到了积极作用，能够有效提升小型语言模型在自然语言推理与理解任务中的表现，为模型压缩与能力迁移提供了可行路径。

Abstract: Chain-of-Thought (CoT) prompting is a widely used method to improve the
reasoning capability of Large Language Models (LLMs). More recently, CoT has
been leveraged in Knowledge Distillation (KD) to transfer reasoning capability
from a larger LLM to a smaller one. This paper examines the role of CoT in
distilling the reasoning capability from larger LLMs to smaller LLMs using
white-box KD, analysing its effectiveness in improving the performance of the
distilled models for various natural language reasoning and understanding
tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2
families, employing CoT data from the CoT-Collection dataset. The distilled
models are then evaluated on natural language reasoning and understanding tasks
from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for
smaller LLMs. Experimental results demonstrate the role of CoT in improving
white-box KD effectiveness, enabling the distilled models to achieve better
average performance in natural language reasoning and understanding tasks from
BBH.

</details>


### [76] [Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese](https://arxiv.org/abs/2511.05239)
*Zilong Li,Jie Cao*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的注释管道，用于解决古典汉文到日文翻译中的低资源问题，并通过引入辅助中文自然语言处理任务提升序列标注性能。


<details>
  <summary>Details</summary>
Motivation: 由于古典汉文到日文的翻译数据稀缺，面临低资源挑战，因此需要开发有效的方法来提升此类翻译与注释系统的性能。

Method: 将古代汉文注释过程抽象为序列标注任务，构建基于大语言模型的注释管道，并利用数字化开源翻译数据构建新数据集；同时引入辅助中文NLP任务以增强模型训练效果。

Result: 在低资源设置下，引入辅助中文NLP任务能有效提升序列标注任务的训练效果；大语言模型在直接机器翻译中表现良好，但在字符注释任务上表现不佳。

Conclusion: 所提出的方法可作为大语言模型在古典汉文注释与翻译任务中的有效补充。

Abstract: Ancient people translated classical Chinese into Japanese by annotating
around each character. We abstract this process as sequence tagging tasks and
fit them into modern language technologies. The research of this annotation and
translation system is a facing low-resource problem. We release this problem by
introducing a LLM-based annotation pipeline and construct a new dataset from
digitalized open-source translation data. We show that under the low-resource
setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the
training of sequence tagging tasks. We also evaluate the performance of large
language models. They achieve high scores in direct machine translation, but
they are confused when being asked to annotate characters. Our method could
work as a supplement of LLMs.

</details>


### [77] [Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models](https://arxiv.org/abs/2511.05286)
*Teqi Hao,Xioayu Tan,Shaojie Shi,Yinghui Xu,Xihe Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Reflective Personalization Optimization (RPO)的新框架，通过将内容生成与个性化对齐解耦，提升黑盒大语言模型的个性化效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖上下文注入，在生成内容的同时需对齐用户风格，导致质量下降和控制不精确，难以兼顾生成质量和个性化。

Method: RPO分为两个阶段：首先由基础模型生成高质量通用回复，然后通过外部反思模块重写输出以匹配用户偏好；该模块先通过监督微调学习重写路径，再用强化学习优化个性化输出质量。

Result: 在LaMP基准上的实验表明，RPO显著优于现有最先进基线方法，验证了显式输出调整优于隐式上下文注入的有效性。

Conclusion: RPO提供了一种高效、模型无关的个性化层，可无缝集成到任何基础模型，为用户中心化生成任务开辟了新方向。

Abstract: The personalization of black-box large language models (LLMs) is a critical
yet challenging task. Existing approaches predominantly rely on context
injection, where user history is embedded into the prompt to directly guide the
generation process. However, this single-step paradigm imposes a dual burden on
the model: generating accurate content while simultaneously aligning with
user-specific styles. This often results in a trade-off that compromises output
quality and limits precise control. To address this fundamental tension, we
propose Reflective Personalization Optimization (RPO), a novel framework that
redefines the personalization paradigm by decoupling content generation from
alignment. RPO operates in two distinct stages: first, a base model generates a
high-quality, generic response; then, an external reflection module explicitly
rewrites this output to align with the user's preferences. This reflection
module is trained using a two-stage process. Initially, supervised fine-tuning
is employed on structured rewriting trajectories to establish a core
personalized reasoning policy that models the transformation from generic to
user-aligned responses. Subsequently, reinforcement learning is applied to
further refine and enhance the quality of the personalized outputs.
Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by
decoupling content generation from personalization, significantly outperforms
state-of-the-art baselines. These findings underscore the superiority of
explicit response shaping over implicit context injection. Moreover, RPO
introduces an efficient, model-agnostic personalization layer that can be
seamlessly integrated with any underlying base model, paving the way for a new
and effective direction in user-centric generation scenarios.

</details>


### [78] [Listening Between the Lines: Decoding Podcast Narratives with Language Modeling](https://arxiv.org/abs/2511.05310)
*Shreya Gupta,Ojasva Saxena,Arghodeep Nandi,Sarah Masud,Kiran Garimella,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出一种基于细粒度实体关联的叙事框架标注方法，通过微调BERT模型提升对播客等对话式文本中叙事框架的识别能力，并揭示话题与叙述方式之间的系统性关系。


<details>
  <summary>Details</summary>
Motivation: 播客作为影响公众舆论的重要媒介，其非正式、多主题、对话式的特性使得传统基于结构化文本训练的语言模型难以准确捕捉其中的叙事框架，亟需更贴近人类判断的自动化分析方法。

Method: 提出一种将叙事框架与对话中具体实体关联的细粒度标注方法，构建并微调BERT模型以识别播客中的叙事框架，进而将框架标签与高层话题相关联，分析话语趋势。

Result: 所提出的微调BERT模型在识别播客叙事框架方面优于现有大语言模型，能够更准确地反映人类对复杂对话数据的判断；并通过案例分析揭示了话题与叙述方式之间的系统性关联。

Conclusion: 该方法为分析非结构化、对话式媒体内容提供了更有效的框架，增强了对数字媒体中影响力传播机制的理解，具有广泛应用于舆论分析和媒体研究的潜力。

Abstract: Podcasts have become a central arena for shaping public opinion, making them
a vital source for understanding contemporary discourse. Their typically
unscripted, multi-themed, and conversational style offers a rich but complex
form of data. To analyze how podcasts persuade and inform, we must examine
their narrative structures -- specifically, the narrative frames they employ.
  The fluid and conversational nature of podcasts presents a significant
challenge for automated analysis. We show that existing large language models,
typically trained on more structured text such as news articles, struggle to
capture the subtle cues that human listeners rely on to identify narrative
frames. As a result, current approaches fall short of accurately analyzing
podcast narratives at scale.
  To solve this, we develop and evaluate a fine-tuned BERT model that
explicitly links narrative frames to specific entities mentioned in the
conversation, effectively grounding the abstract frame in concrete details. Our
approach then uses these granular frame labels and correlates them with
high-level topics to reveal broader discourse trends. The primary contributions
of this paper are: (i) a novel frame-labeling methodology that more closely
aligns with human judgment for messy, conversational data, and (ii) a new
analysis that uncovers the systematic relationship between what is being
discussed (the topic) and how it is being presented (the frame), offering a
more robust framework for studying influence in digital media.

</details>


### [79] [What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions](https://arxiv.org/abs/2511.05320)
*Klára Bendová,Tomáš Knap,Jan Černý,Vojtěch Pour,Jaromir Savelka,Ivana Kvapilíková,Jakub Drápal*

Main category: cs.CL

TL;DR: 本文研究了从斯洛伐克公开法院判决中提取犯罪行为描述的可行性，比较了正则表达式和大语言模型（LLM）两种方法。改进的正则表达式和LLM方法显著优于基线方法，结合使用时准确率高达99.5%。


<details>
  <summary>Details</summary>
Motivation: 刑事司法行政数据对犯罪行为的信息记录有限，而欧洲大陆法院判决书中包含大量未被利用的犯罪行为描述信息，因此需要有效方法从中提取结构化信息。

Method: 采用两种方法提取判决书中的犯罪行为描述：一是基于正则表达式的匹配，特别是针对“sparing”及其空格规范化特征；二是使用Gemini Flash 2.0大语言模型，通过预定义指令进行提示抽取。

Result: 基线方法（简单正则）仅在40.5%的判决中成功识别描述，改进的正则表达式达到97%，LLM达到98.75%，二者结合达99.5%。与法律专业学生标注对比显示，高级方法匹配率约90%，其中LLM完全匹配率达91.75%，组合方法达92%。

Conclusion: 改进的正则表达式和大语言模型均能高效、准确地从法院判决书中提取犯罪行为描述，结合使用效果最佳，为刑事司法数据分析提供了可行的技术路径。

Abstract: Criminal justice administrative data contain only a limited amount of
information about the committed offense. However, there is an unused source of
extensive information in continental European courts' decisions: descriptions
of criminal behaviors in verdicts by which offenders are found guilty. In this
paper, we study the feasibility of extracting these descriptions from publicly
available court decisions from Slovakia. We use two different approaches for
retrieval: regular expressions and large language models (LLMs). Our baseline
was a simple method employing regular expressions to identify typical words
occurring before and after the description. The advanced regular expression
approach further focused on "sparing" and its normalization (insertion of
spaces between individual letters), typical for delineating the description.
The LLM approach involved prompting the Gemini Flash 2.0 model to extract the
descriptions using predefined instructions. Although the baseline identified
descriptions in only 40.5% of verdicts, both methods significantly outperformed
it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and
99.5% when combined. Evaluation by law students showed that both advanced
methods matched human annotations in about 90% of cases, compared to just 34.5%
for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of
instances, and a combination of advanced regular expressions with LLMs reached
92%.

</details>


### [80] [Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE](https://arxiv.org/abs/2511.05324)
*Firoj Ahmmed Patwary,Abdullah Al Noman*

Main category: cs.CL

TL;DR: 本文提出了一种专为孟加拉语设计的字节对编码（BPE）分词器BengaliBPE，通过Unicode归一化、音素级初始化和形态感知合并规则，提升了对丰富形态语言的分词效果。实验表明，BengaliBPE在细分粒度和形态可解释性方面优于现有方法，适用于未来的孟加拉语NLP系统。


<details>
  <summary>Details</summary>
Motivation: 现有的子词分词器主要针对拉丁或多种语言语料库设计，在处理如孟加拉语等具有丰富形态的语言时表现不佳，因此需要一种语言适配的分词方案以提升模型的语言表示能力。

Method: 提出BengaliBPE，采用Unicode归一化、图素级初始化和形态感知的合并规则，使用大规模孟加拉语新闻分类数据集与空格、SentencePiece BPE和HuggingFace BPE三种基线方法进行对比，评估其在分词粒度、编码速度和下游分类准确率方面的表现。

Result: BengaliBPE在分词细粒度和形态可解释性方面表现最佳，尽管计算开销略高，但在下游任务中展现出优势。

Conclusion: 语言适配的分词策略对形态丰富的语言至关重要，BengaliBPE为孟加拉语NLP系统（包括上下文语言模型的大规模预训练）提供了坚实基础。

Abstract: Tokenization is an important first step in Natural Language Processing (NLP)
pipelines because it decides how models learn and represent linguistic
information. However, current subword tokenizers like SentencePiece or
HuggingFace BPE are mostly designed for Latin or multilingual corpora and do
not perform well on languages with rich morphology such as Bengali. To address
this limitation, we present BengaliBPE, a Byte Pair Encoding (BPE) tokenizer
specifically developed for the Bengali script. BengaliBPE applies Unicode
normalization, grapheme-level initialization, and morphology-aware merge rules
to maintain linguistic consistency and preserve subword integrity. We use a
large-scale Bengali news classification dataset to compare BengaliBPE with
three baselines: Whitespace, SentencePiece BPE, and HuggingFace BPE. The
evaluation considers tokenization granularity, encoding speed, and downstream
classification accuracy. While all methods perform reasonably well, BengaliBPE
provides the most detailed segmentation and the best morphological
interpretability, albeit with slightly higher computational cost. These
findings highlight the importance of language-aware tokenization for
morphologically rich scripts and establish BengaliBPE as a strong foundation
for future Bengali NLP systems, including large-scale pretraining of contextual
language models.

</details>


### [81] [Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning](https://arxiv.org/abs/2511.05407)
*Yahui Fu,Zi Haur Pang,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 提出了一种统一框架，通过建模个体和群体层面的偏好来提升对话系统中用户满意度估计的准确性，尤其改善了少数用户群体的表现。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法通常忽略少数用户群体和个体差异，导致满意度评估存在偏差，因此需要一种能够同时适应个体和群体偏好的框架。

Method: 提出了Chain-of-Personalized-Reasoning（CoPeR）捕捉个体偏好，设计基于期望最大化算法的Majority-Minority Preference-Aware Clustering（M2PC）进行无监督用户分组，并构建偏好自适应强化学习框架PAda-PPO实现联合优化。

Result: 在情感支持对话数据集上的实验表明，该方法在用户满意度估计方面表现一致提升，尤其对代表性不足的用户群体效果显著。

Conclusion: 所提出的框架能有效兼顾个体与群体偏好，提升了对话系统对多样化用户满意度的建模能力，特别是在少数用户群体上具有更好适应性。

Abstract: User satisfaction in dialogue systems is inherently subjective. When the same
response strategy is applied across users, minority users may assign different
satisfaction ratings than majority users due to variations in individual
intents and preferences. However, existing alignment methods typically train
one-size-fits-all models that aim for broad consensus, often overlooking
minority perspectives and user-specific adaptation. We propose a unified
framework that models both individual- and group-level preferences for user
satisfaction estimation. First, we introduce Chain-of-Personalized-Reasoning
(CoPeR) to capture individual preferences through interpretable reasoning
chains. Second, we propose an expectation-maximization-based Majority-Minority
Preference-Aware Clustering (M2PC) algorithm that discovers distinct user
groups in an unsupervised manner to learn group-level preferences. Finally, we
integrate these components into a preference-adaptive reinforcement learning
framework (PAda-PPO) that jointly optimizes alignment with both individual and
group preferences. Experiments on the Emotional Support Conversation dataset
demonstrate consistent improvements in user satisfaction estimation,
particularly for underrepresented user groups.

</details>


### [82] [Steering Language Models with Weight Arithmetic](https://arxiv.org/abs/2511.05408)
*Constanza Fierro,Fabien Roger*

Main category: cs.CL

TL;DR: 本文提出了一种名为对比权重引导（contrastive weight steering）的简单后训练方法，通过权重算术调整模型参数，以更有效地利用狭窄的训练数据来控制大语言模型的行为。


<details>
  <summary>Details</summary>
Motivation: 在狭窄的数据分布上提供反馈容易导致意外泛化，而在多样化数据上提供高质量反馈则成本高昂。因此，需要一种能有效利用有限训练数据来精确控制模型行为的方法。

Method: 通过两个小规模微调（一个诱导期望行为，另一个诱导相反行为）的权重变化之差，在权重空间中分离出行为方向，并通过加减该方向来修改模型权重。

Result: 权重引导在分布外行为控制方面优于激活引导，能有效缓解谄媚（sycophancy）和错位问题；在任务微调中可减少不良行为漂移，同时保持任务性能提升；初步证据显示可通过监测微调更新与“恶意”权重方向的相似性来检测新兴错位行为。

Conclusion: 对比权重引导是一种有效的后训练行为控制方法，能够在不显著损害通用能力的前提下实现更强的行为调节，并具备监测训练过程中潜在错位行为的潜力。

Abstract: Providing high-quality feedback to Large Language Models (LLMs) on a diverse
training distribution can be difficult and expensive, and providing feedback
only on a narrow distribution can result in unintended generalizations. To
better leverage narrow training data, we propose contrastive weight steering, a
simple post-training method that edits the model parameters using weight
arithmetic. We isolate a behavior direction in weight-space by subtracting the
weight deltas from two small fine-tunes -- one that induces the desired
behavior and another that induces its opposite -- and then add or remove this
direction to modify the model's weights. We apply this technique to mitigate
sycophancy and induce misalignment, and find that weight steering often
generalizes further than activation steering, achieving stronger
out-of-distribution behavioral control before degrading general capabilities.
We also show that, in the context of task-specific fine-tuning, weight steering
can partially mitigate undesired behavioral drift: it can reduce sycophancy and
under-refusals introduced during fine-tuning while preserving task performance
gains. Finally, we provide preliminary evidence that emergent misalignment can
be detected by measuring the similarity between fine-tuning updates and an
"evil" weight direction, suggesting that it may be possible to monitor the
evolution of weights during training and detect rare misaligned behaviors that
never manifest during training or evaluations.

</details>


### [83] [MIMIC-SR-ICD11: A Dataset for Narrative-Based Diagnosis](https://arxiv.org/abs/2511.05485)
*Yuexin Wu,Shiqi Wang,Vasile Rus*

Main category: cs.CL

TL;DR: 本文提出了MIMIC-SR-ICD11数据集和LL-Rank框架，用于基于临床报告的疾病诊断，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录常忽略患者自述中的关键细节，需要更有效的诊断支持工具。

Method: 构建了与ICD-11对齐的MIMIC-SR-ICD11数据集，并提出基于似然的重排序框架LL-Rank，利用语义兼容性评分提升诊断准确性。

Result: 在七个模型上，LL-Rank均优于生成加映射基线方法GenMap，消融实验表明其性能提升主要来自PMI评分机制。

Conclusion: LL-Rank能有效利用患者自述信息进行多标签疾病诊断，减少标签频率偏差，具有临床应用潜力。

Abstract: Disease diagnosis is a central pillar of modern healthcare, enabling early
detection and timely intervention for acute conditions while guiding lifestyle
adjustments and medication regimens to prevent or slow chronic disease.
Self-reports preserve clinically salient signals that templated electronic
health record (EHR) documentation often attenuates or omits, especially subtle
but consequential details. To operationalize this shift, we introduce
MIMIC-SR-ICD11, a large English diagnostic dataset built from EHR discharge
notes and natively aligned to WHO ICD-11 terminology. We further present
LL-Rank, a likelihood-based re-ranking framework that computes a
length-normalized joint likelihood of each label given the clinical report
context and subtracts the corresponding report-free prior likelihood for that
label. Across seven model backbones, LL-Rank consistently outperforms a strong
generation-plus-mapping baseline (GenMap). Ablation experiments show that
LL-Rank's gains primarily stem from its PMI-based scoring, which isolates
semantic compatibility from label frequency bias.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [84] [A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024](https://arxiv.org/abs/2511.04685)
*Daniela Guericke,Rolf van der Hulst,Asal Karimpour,Ieke Schrader,Matthias Walter*

Main category: cs.AI

TL;DR: 本文介绍了Team Twente在2024年综合医疗排班竞赛中获得第三名的解决方案，该方案结合了混合整数规划、约束规划和模拟退火算法，采用三阶段分解方法求解子问题，并首次提供了基准实例的最优解下界。


<details>
  <summary>Details</summary>
Motivation: 为了在集成医疗排班竞赛中取得优异成绩，同时探索高效求解复杂排班问题的方法。

Method: 采用混合整数规划、约束规划和模拟退火相结合的三阶段分解策略，将原问题分解为多个子问题进行求解。

Result: 在竞赛中获得第三名，并首次为基准实例提供了最优解的下界估计。

Conclusion: 所提出的方法有效且具有竞争力，未来通过解决文中指出的开放性问题有望进一步提升性能。

Abstract: We report about the algorithm, implementation and results submitted to the
Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored
third in the competition. Our approach combines mixed-integer programming,
constraint programming and simulated annealing in a 3-phase solution approach
based on decomposition into subproblems. Next to describing our approach and
describing our design decisions, we share our insights and, for the first time,
lower bounds on the optimal solution values for the benchmark instances. We
finally highlight open problems for which we think that addressing them could
improve our approach even further.

</details>


### [85] [Epistemic Reject Option Prediction](https://arxiv.org/abs/2511.04855)
*Vojtech Franc,Jakub Paplham*

Main category: cs.AI

TL;DR: 本文提出了一种新的拒绝选项预测器，用于处理由于数据不足导致的认知不确定性高的情况。该方法基于贝叶斯学习，通过最小化预期遗憾来定义最优预测器，并在遗憾超过指定拒绝成本时选择 abstain。据作者所知，这是首个能够识别训练数据不足以做出可靠决策的输入的原理性框架。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，模型不仅需要准确预测，还需要量化和传达其不确定性。传统方法仅关注偶然不确定性，在数据量大时有效，但在实际数据有限的情况下不现实。因此，需要一种能处理认知不确定性的拒绝选项预测方法。

Method: 基于贝叶斯学习，重新定义最优预测器为最小化预期遗憾的预测器。当某一输入的遗憾超过设定的拒绝成本时，模型选择 abstain。该方法专门针对由数据不足引起的高度认知不确定性区域进行拒绝。

Result: 提出了一个原则性的框架——认知拒绝选项预测器，能够在数据不足导致认知不确定性高的情况下进行 abstain 决策。实验表明该方法能有效识别出训练数据不足以支持可靠预测的输入。

Conclusion: 该研究首次提供了一个系统化的框架，使预测模型能够在面对因数据稀缺引起的高度认知不确定性时主动 abstain，从而提升高风险应用中的可靠性与安全性。

Abstract: In high-stakes applications, predictive models must not only produce accurate
predictions but also quantify and communicate their uncertainty. Reject-option
prediction addresses this by allowing the model to abstain when prediction
uncertainty is high. Traditional reject-option approaches focus solely on
aleatoric uncertainty, an assumption valid only when large training data makes
the epistemic uncertainty negligible. However, in many practical scenarios,
limited data makes this assumption unrealistic. This paper introduces the
epistemic reject-option predictor, which abstains in regions of high epistemic
uncertainty caused by insufficient data. Building on Bayesian learning, we
redefine the optimal predictor as the one that minimizes expected regret -- the
performance gap between the learned model and the Bayes-optimal predictor with
full knowledge of the data distribution. The model abstains when the regret for
a given input exceeds a specified rejection cost. To our knowledge, this is the
first principled framework that enables learning predictors capable of
identifying inputs for which the training data is insufficient to make reliable
decisions.

</details>


### [86] [DMA: Online RAG Alignment with Human Feedback](https://arxiv.org/abs/2511.04880)
*Yu Bai,Yukai Miao,Dawei Wang,Li Chen,Fei Long,Rundi Zhai,Dan Li,Yanyu Ren,Tianfeng Liu,Hongtao Xie,Ce Yang,Xuhui Cai*

Main category: cs.AI

TL;DR: 本文提出了Dynamic Memory Alignment (DMA)框架，通过多粒度人类反馈实现检索增强生成系统中的在线学习和实时排序优化。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统依赖静态检索，难以适应用户意图变化和内容漂移，因此需要一种能动态调整排序的机制。

Method: DMA将文档级、列表级和响应级反馈整合到一个统一的学习流程中，包括监督训练点对点和列表级排序器、基于响应偏好进行策略优化，并通过知识蒸馏构建轻量级打分模型以支持低延迟服务。

Result: 在线A/B测试显示显著提升用户参与度；离线测试表明在保持基础检索性能的同时，在TriviaQA和HotpotQA等问答任务上取得明显增益。

Conclusion: DMA为RAG系统提供了一种兼顾反馈驱动与实时适应性的有效解决方案，且不牺牲原有检索能力。

Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval,
limiting adaptation to evolving intent and content drift. We introduce Dynamic
Memory Alignment (DMA), an online learning framework that systematically
incorporates multi-granularity human feedback to align ranking in interactive
settings. DMA organizes document-, list-, and response-level signals into a
coherent learning pipeline: supervised training for pointwise and listwise
rankers, policy optimization driven by response-level preferences, and
knowledge distillation into a lightweight scorer for low-latency serving.
Throughout this paper, memory refers to the model's working memory, which is
the entire context visible to the LLM for In-Context Learning.
  We adopt a dual-track evaluation protocol mirroring deployment: (i)
large-scale online A/B ablations to isolate the utility of each feedback
source, and (ii) few-shot offline tests on knowledge-intensive benchmarks.
Online, a multi-month industrial deployment further shows substantial
improvements in human engagement. Offline, DMA preserves competitive
foundational retrieval while yielding notable gains on conversational QA
(TriviaQA, HotpotQA). Taken together, these results position DMA as a
principled approach to feedback-driven, real-time adaptation in RAG without
sacrificing baseline capability.

</details>


### [87] [Autonomous generation of different courses of action in mechanized combat operations](https://arxiv.org/abs/2511.05182)
*Johan Schubert,Patrik Hansen,Pontus Hörling,Ronnie Johansson*

Main category: cs.AI

TL;DR: 本文提出了一种支持军事地面作战执行阶段决策的方法，通过生成和评估机械化营的多种行动方案，结合敌方状态、兵力构成、攻防类型等因素，动态推荐最优行动路径。


<details>
  <summary>Details</summary>
Motivation: 在动态变化的战场环境中，传统决策方法难以快速生成并评估大量可行行动方案，因此需要一种能够实时支持指挥官决策的系统化方法。

Method: 该方法从初始行动集出发，系统生成数千种行动方案，并基于预期战果、兵力对比、攻防类型和推进速度等指标进行评估；生成与评估过程并行，结合战场手册评估战斗结果，并根据已评估行动迭代生成新方案。

Result: 该方法能够在作战过程中持续提供优化的行动建议，支持指挥官在不断变化的条件下做出序列化决策，提高了决策效率和适应性。

Conclusion: 所提出的决策支持方法能有效辅助机械化部队在复杂动态战场中进行实时、系统的行动规划与调整。

Abstract: In this paper, we propose a methodology designed to support decision-making
during the execution phase of military ground combat operations, with a focus
on one's actions. This methodology generates and evaluates recommendations for
various courses of action for a mechanized battalion, commencing with an
initial set assessed by their anticipated outcomes. It systematically produces
thousands of individual action alternatives, followed by evaluations aimed at
identifying alternative courses of action with superior outcomes. These
alternatives are appraised in light of the opponent's status and actions,
considering unit composition, force ratios, types of offense and defense, and
anticipated advance rates. Field manuals evaluate battle outcomes and
advancement rates. The processes of generation and evaluation work
concurrently, yielding a variety of alternative courses of action. This
approach facilitates the management of new course generation based on
previously evaluated actions. As the combat unfolds and conditions evolve,
revised courses of action are formulated for the decision-maker within a
sequential decision-making framework.

</details>


### [88] [Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance](https://arxiv.org/abs/2511.05311)
*Valeriu Dimidov,Faisal Hawlader,Sasan Jafarnejad,Raphaël Frank*

Main category: cs.AI

TL;DR: 本文探讨了基于大语言模型（LLM）的智能体在汽车预测性维护（PdM）数据清洗中的应用潜力，特别是在处理维护日志中的多种噪声类型方面表现良好，显示出推动PdM工业落地的前景。


<details>
  <summary>Details</summary>
Motivation: 由于经济限制、数据集可获得性低和专业人才短缺，汽车领域的预测性维护（PdM）发展受限，而大语言模型（LLM）的进步为克服这些障碍提供了新机遇。

Method: 研究聚焦于维护日志的清洗任务，评估LLM智能体在处理六种不同类型噪声（如拼写错误、缺失字段、近似重复条目和错误日期等）中的表现。

Result: 实验结果表明，LLM在处理通用数据清洗任务上是有效的，尤其适用于常见错误类型；但在领域特定错误上仍存在挑战。

Conclusion: LLM为PdM的数据清洗提供了有前景的基础，未来通过领域特化训练和增强智能体能力有望进一步提升其工业适用性。

Abstract: Economic constraints, limited availability of datasets for reproducibility
and shortages of specialized expertise have long been recognized as key
challenges to the adoption and advancement of predictive maintenance (PdM) in
the automotive sector. Recent progress in large language models (LLMs) presents
an opportunity to overcome these barriers and speed up the transition of PdM
from research to industrial practice. Under these conditions, we explore the
potential of LLM-based agents to support PdM cleaning pipelines. Specifically,
we focus on maintenance logs, a critical data source for training
well-performing machine learning (ML) models, but one often affected by errors
such as typos, missing fields, near-duplicate entries, and incorrect dates. We
evaluate LLM agents on cleaning tasks involving six distinct types of noise.
Our findings show that LLMs are effective at handling generic cleaning tasks
and offer a promising foundation for future industrial applications. While
domain-specific errors remain challenging, these results highlight the
potential for further improvements through specialized training and enhanced
agentic capabilities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [89] [ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling](https://arxiv.org/abs/2511.04758)
*Caelan Garrett,Fabio Ramos*

Main category: cs.RO

TL;DR: 本文提出了ScheduleStream，首个用于规划与调度的通用框架，通过引入混合持续性动作建模时间动态，实现双臂机器人任务与运动规划中的并行动作调度，并结合GPU加速提升效率。


<details>
  <summary>Details</summary>
Motivation: 控制双臂机器人同时运动在计算上具有挑战性，因为混合离散-连续动作空间的增长使得传统TAMP算法难以生成支持并行操作的调度方案。

Method: 提出ScheduleStream框架，采用混合持续性动作模型，支持异步启动和参数化持续时间；设计领域无关的算法，并在采样器中利用GPU加速进行任务与运动规划与调度（TAMPAS）。

Result: 在仿真中对比多种消融实验，结果显示所提算法能生成更高效的解决方案，并在多个真实双臂机器人任务中成功演示。

Conclusion: ScheduleStream是首个支持采样操作的通用规划与调度框架，能够有效扩展TAMP以支持双臂机器人的并行操作，提升任务执行效率。

Abstract: Bimanual and humanoid robots are appealing because of their human-like
ability to leverage multiple arms to efficiently complete tasks. However,
controlling multiple arms at once is computationally challenging due to the
growth in the hybrid discrete-continuous action space. Task and Motion Planning
(TAMP) algorithms can efficiently plan in hybrid spaces but generally produce
plans, where only one arm is moving at a time, rather than schedules that allow
for parallel arm motion. In order to extend TAMP to produce schedules, we
present ScheduleStream, the first general-purpose framework for planning &
scheduling with sampling operations. ScheduleStream models temporal dynamics
using hybrid durative actions, which can be started asynchronously and persist
for a duration that's a function of their parameters. We propose
domain-independent algorithms that solve ScheduleStream problems without any
application-specific mechanisms. We apply ScheduleStream to Task and Motion
Planning & Scheduling (TAMPAS), where we use GPU acceleration within samplers
to expedite planning. We compare ScheduleStream algorithms to several ablations
in simulation and find that they produce more efficient solutions. We
demonstrate ScheduleStream on several real-world bimanual robot tasks at
https://schedulestream.github.io.

</details>


### [90] [Unified Multimodal Diffusion Forcing for Forceful Manipulation](https://arxiv.org/abs/2511.04812)
*Zixuan Huang,Huaidian Hou,Dmitry Berenson*

Main category: cs.RO

TL;DR: 本文提出了一种名为多模态扩散强制（Multimodal Diffusion Forcing, MDF）的统一框架，用于从多模态机器人轨迹中学习，不仅限于动作生成，还能建模时间与跨模态依赖关系。


<details>
  <summary>Details</summary>
Motivation: 标准的模仿学习方法通常只学习从观测到动作的直接映射，忽略了感官输入、动作和奖励之间的丰富交互，而这种交互对建模机器人行为和理解任务结果至关重要。

Method: MDF采用随机部分掩码，并训练一个扩散模型来重建轨迹，从而鼓励模型学习时间和跨模态的依赖关系，例如预测动作对力信号的影响或从部分观测中推断状态。

Result: 在模拟和真实环境中的接触丰富、用力操作任务上评估表明，MDF不仅具备多功能性，而且在噪声观测下表现出强性能和鲁棒性。

Conclusion: MDF是一种有效的多模态学习框架，能够捕捉复杂的跨模态和时间依赖，适用于复杂机器人操作任务。

Abstract: Given a dataset of expert trajectories, standard imitation learning
approaches typically learn a direct mapping from observations (e.g., RGB
images) to actions. However, such methods often overlook the rich interplay
between different modalities, i.e., sensory inputs, actions, and rewards, which
is crucial for modeling robot behavior and understanding task outcomes. In this
work, we propose Multimodal Diffusion Forcing, a unified framework for learning
from multimodal robot trajectories that extends beyond action generation.
Rather than modeling a fixed distribution, MDF applies random partial masking
and trains a diffusion model to reconstruct the trajectory. This training
objective encourages the model to learn temporal and cross-modal dependencies,
such as predicting the effects of actions on force signals or inferring states
from partial observations. We evaluate MDF on contact-rich, forceful
manipulation tasks in simulated and real-world environments. Our results show
that MDF not only delivers versatile functionalities, but also achieves strong
performance, and robustness under noisy observations. More visualizations can
be found on our website https://unified-df.github.io

</details>


### [91] [Pixi: Unified Software Development and Distribution for Robotics and AI](https://arxiv.org/abs/2511.04827)
*Tobias Fischer,Wolf Vollprecht,Bas Zalmstra,Ruben Arts,Tim de Jager,Alejandro Fontan,Adam D Hines,Michael Milford,Silvio Traversaro,Daniel Claes,Scarlett Raine*

Main category: cs.RO

TL;DR: Pixi是一个统一的包管理框架，通过精确捕获项目依赖状态，实现跨平台的比特级可重现性，显著提升机器人学和AI研究的可重复性和协作效率。


<details>
  <summary>Details</summary>
Motivation: 解决科学计算中的可重复性危机，特别是机器人领域中由于多语言、软硬件工具链碎片化导致的高达70%算法无法复现或难以部署的问题。

Method: 开发Pixi框架，采用高性能SAT求解器实现快速依赖解析，生成项目级锁定文件，并整合conda-forge与PyPI生态系统，消除多管理器需求。

Result: Pixi使依赖解析速度提升达10倍，环境搭建时间从数小时缩短至几分钟，已被5300多个项目采用。

Conclusion: Pixi通过提供可扩展、可重现的协作研究基础设施，显著降低了全球研究人员的技术门槛，加速了机器人学与人工智能领域的研究进展。

Abstract: The reproducibility crisis in scientific computing constrains robotics
research. Existing studies reveal that up to 70% of robotics algorithms cannot
be reproduced by independent teams, while many others fail to reach deployment
because creating shareable software environments remains prohibitively complex.
These challenges stem from fragmented, multi-language, and hardware-software
toolchains that lead to dependency hell. We present Pixi, a unified
package-management framework that addresses these issues by capturing exact
dependency states in project-level lockfiles, ensuring bit-for-bit
reproducibility across platforms. Its high-performance SAT solver achieves up
to 10x faster dependency resolution than comparable tools, while integration of
the conda-forge and PyPI ecosystems removes the need for multiple managers.
Adopted in over 5,300 projects since 2023, Pixi reduces setup times from hours
to minutes and lowers technical barriers for researchers worldwide. By enabling
scalable, reproducible, collaborative research infrastructure, Pixi accelerates
progress in robotics and AI.

</details>


### [92] [Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning](https://arxiv.org/abs/2511.04831)
*NVIDIA,:,Mayank Mittal,Pascal Roth,James Tigue,Antoine Richard,Octi Zhang,Peter Du,Antonio Serrano-Muñoz,Xinjie Yao,René Zurbrügg,Nikita Rudin,Lukasz Wawrzyniak,Milad Rakhsha,Alain Denzler,Eric Heiden,Ales Borovicka,Ossama Ahmed,Iretiayo Akinola,Abrar Anwar,Mark T. Carlson,Ji Yuan Feng,Animesh Garg,Renato Gasoto,Lionel Gulich,Yijie Guo,M. Gussert,Alex Hansen,Mihir Kulkarni,Chenran Li,Wei Liu,Viktor Makoviychuk,Grzegorz Malczyk,Hammad Mazhar,Masoud Moghani,Adithyavairavan Murali,Michael Noseworthy,Alexander Poddubny,Nathan Ratliff,Welf Rehberg,Clemens Schwarke,Ritvik Singh,James Latham Smith,Bingjie Tang,Ruchik Thaker,Matthew Trepte,Karl Van Wyk,Fangzhou Yu,Alex Millane,Vikram Ramasamy,Remo Steiner,Sangeeta Subramanian,Clemens Volk,CY Chen,Neel Jawale,Ashwin Varghese Kuruttukulam,Michael A. Lin,Ajay Mandlekar,Karsten Patzwaldt,John Welsh,Huihua Zhao,Fatima Anes,Jean-Francois Lafleche,Nicolas Moënne-Loccoz,Soowan Park,Rob Stepinski,Dirk Van Gelder,Chris Amevor,Jan Carius,Jumyung Chang,Anka He Chen,Pablo de Heras Ciechomski,Gilles Daviet,Mohammad Mohajerani,Julia von Muralt,Viktor Reutskyy,Michael Sauter,Simon Schirm,Eric L. Shi,Pierre Terdiman,Kenny Vilella,Tobias Widmer,Gordon Yeoman,Tiffany Chen,Sergey Grizan,Cathy Li,Lotus Li,Connor Smith,Rafael Wiltz,Kostas Alexis,Yan Chang,David Chu,Linxi "Jim" Fan,Farbod Farshidian,Ankur Handa,Spencer Huang,Marco Hutter,Yashraj Narang,Soha Pouya,Shiwei Sheng,Yuke Zhu,Miles Macklin,Adam Moravanszky,Philipp Reist,Yunrong Guo,David Hoeller,Gavriel State*

Main category: cs.RO

TL;DR: Isaac Lab 是一个扩展了GPU原生机器人模拟的框架，支持大规模多模态学习，集成了高保真物理仿真、逼真渲染和模块化环境设计，适用于强化学习与模仿学习的大规模应用。


<details>
  <summary>Details</summary>
Motivation: 为了应对机器人学习中对高保真、大规模、多模态训练环境的需求，推动下一代机器人研究突破。

Method: 采用GPU并行物理仿真、光栅化渲染、可组合模块化架构，并集成执行器模型、多频传感器模拟、数据采集管道和域随机化工具。

Result: 成功应用于全身控制、跨形态移动、复杂接触操作、灵巧操作以及基于人类示范的技能学习等任务，并计划集成可微分物理引擎以支持梯度驱动方法。

Conclusion: Isaac Lab 提供了一个可扩展的高性能平台，有望显著推动大规模机器人学习的发展。

Abstract: We present Isaac Lab, the natural successor to Isaac Gym, which extends the
paradigm of GPU-native robotics simulation into the era of large-scale
multi-modal learning. Isaac Lab combines high-fidelity GPU parallel physics,
photorealistic rendering, and a modular, composable architecture for designing
environments and training robot policies. Beyond physics and rendering, the
framework integrates actuator models, multi-frequency sensor simulation, data
collection pipelines, and domain randomization tools, unifying best practices
for reinforcement and imitation learning at scale within a single extensible
platform. We highlight its application to a diverse set of challenges,
including whole-body control, cross-embodiment mobility, contact-rich and
dexterous manipulation, and the integration of human demonstrations for skill
acquisition. Finally, we discuss upcoming integration with the differentiable,
GPU-accelerated Newton physics engine, which promises new opportunities for
scalable, data-efficient, and gradient-based approaches to robot learning. We
believe Isaac Lab's combination of advanced simulation capabilities, rich
sensing, and data-center scale execution will help unlock the next generation
of breakthroughs in robotics research.

</details>


### [93] [Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning](https://arxiv.org/abs/2511.04835)
*Shubham Natraj,Bruno Sinopoli,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 提出了一种新的非均匀采样策略，通过将采样偏向“认证”区域来提升基于采样的运动规划器（SBMPs）的效率，并首次为采样区域提供了概率上正确的保证。


<details>
  <summary>Details</summary>
Motivation: 传统SBMP依赖均匀采样，在复杂环境中效率低、规划慢，因此需要一种更高效且具有理论保障的非均匀采样方法。

Method: 结合启发式路径预测器生成初始路径，并利用共形预测量化预测不确定性，构建包含最优解的认证区域，引导非均匀采样。

Result: 实验表明该方法能更快地找到可行路径，并在未见过的环境中表现出更好的泛化能力。

Conclusion: 所提出的非均匀采样策略显著提升了SBMP的规划效率和泛化性能，且首次为采样区域提供了概率保证。

Abstract: Sampling-based motion planners (SBMPs) are widely used to compute dynamically
feasible robot paths. However, their reliance on uniform sampling often leads
to poor efficiency and slow planning in complex environments. We introduce a
novel non-uniform sampling strategy that integrates into existing SBMPs by
biasing sampling toward `certified' regions. These regions are constructed by
(i) generating an initial, possibly infeasible, path using any heuristic path
predictor (e.g., A* or vision-language models) and (ii) applying conformal
prediction to quantify the predictor's uncertainty. This process yields
prediction sets around the initial-guess path that are guaranteed, with
user-specified probability, to contain the optimal solution. To our knowledge,
this is the first non-uniform sampling approach for SBMPs that provides such
probabilistically correct guarantees on the sampling regions. Extensive
evaluations demonstrate that our method consistently finds feasible paths
faster and generalizes better to unseen environments than existing baselines.

</details>


### [94] [Design Exploration for Protection and Cleaning of Solar Panels with Case Studies for Space Missions](https://arxiv.org/abs/2511.04837)
*Cameron Robinson,Ganghee Jang*

Main category: cs.RO

TL;DR: 设计并比较了两种太阳能板清洁机制（刮水器系统和轨道系统），同时测试了防护材料，发现聚碳酸酯具有良好的抗冲击性能，且软硬材料夹层结构更有效；刮水器系统在成本、清洁速度和功耗方面优于轨道系统。


<details>
  <summary>Details</summary>
Motivation: 解决太阳能板因积尘或太空碎片撞击导致性能下降或失效的问题，确保其在关键任务中的持续运行。

Method: 设计了刮水器和轨道两种清洁系统，并通过碰撞试验评估不同防护材料的性能，重点测试了聚碳酸酯及多层材料结构的效果。

Result: 聚碳酸酯表现出良好的防护潜力，软材料夹层结构对保护面板更为关键；刮水器系统在清洁效率、成本和能耗方面均优于轨道系统。

Conclusion: 刮水器系统是更优的清洁方案，而采用软硬材料复合层的防护设计能有效提升太阳能板的耐久性，适用于极端环境下的应用。

Abstract: Solar energy is used for many mission-critical applications including space
exploration, sensor systems to monitor wildfires, etc. Their operation can be
limited or even terminated if solar panels are covered with dust or hit by
space debris. To address this issue, we designed panel cleaning mechanisms and
tested protective materials. For cleaning mechanisms, we designed and compared
a wiper system and a rail system. For protective materials, we found through
collision tests that polycarbonate was very promising, though the most
important factor was layering a soft material between the panel's surface and a
hard material. In the cleaning system comparisons, the wiper-based system was
more efficient than the rail-based system in terms of cost, cleaning speed, and
total power consumption.

</details>


### [95] [iFlyBot-VLM Technical Report](https://arxiv.org/abs/2511.04976)
*Xin Nie,Zhiyuan Cheng,Yuan Zhang,Chao Ji,Jiajia Wu,Yuhan Zhang,Jia Pan*

Main category: cs.RO

TL;DR: iFlyBot-VLM 是一种通用的视觉-语言模型，旨在弥合环境感知与机器人动作控制之间的跨模态语义鸿沟，通过操作语言实现多平台的感知-行动闭环协调，并在多个基准上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升具身智能中视觉与动作控制之间的语义一致性，解决高维感知与低维控制间的跨模态鸿沟问题。

Method: 设计了一种系统化的VLM架构，将复杂视觉和空间信息抽象为身体无关的操作语言，支持空间理解、目标定位、动作控制生成和任务规划四大功能。

Result: 在10个主流具身智能VLM基准（如Blink、Where2Place）上达到最优性能，同时保持模型的通用性。

Conclusion: iFlyBot-VLM是一种可扩展且通用的具身AI基础模型，有望推动专用系统向通用智能体的发展，相关数据和模型权重将公开。

Abstract: We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used
to improve the domain of Embodied Intelligence. The central objective of
iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional
environmental perception and low-level robotic motion control. To this end, the
model abstracts complex visual and spatial information into a body-agnostic and
transferable Operational Language, thereby enabling seamless perception-action
closed-loop coordination across diverse robotic platforms. The architecture of
iFlyBot-VLM is systematically designed to realize four key functional
capabilities essential for embodied intelligence: 1) Spatial Understanding and
Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and
Control Parameter Generation; 4) Task Planning and Skill Sequencing. We
envision iFlyBot-VLM as a scalable and generalizable foundation model for
embodied AI, facilitating the progression from specialized task-oriented
systems toward generalist, cognitively capable agents. We conducted evaluations
on 10 current mainstream embodied intelligence-related VLM benchmark datasets,
such as Blink and Where2Place, and achieved optimal performance while
preserving the model's general capabilities. We will publicly release both the
training data and model weights to foster further research and development in
the field of Embodied Intelligence.

</details>


### [96] [A semi-analytical approach for computing the largest singularity-free spheres of a class of 6-6 Stewart-Gough platforms for specified orientation workspaces](https://arxiv.org/abs/2511.04992)
*Bibekananda Patra,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 本文提出了一种计算6-6 Stewart-Gough平台机构在指定姿态工作空间内最大无奇异球（SFS）的方法，通过在姿态空间采样并重复解析计算SFS，选取最小值作为目标结果，并通过数值实验比较了四种构型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了评估和优化Stewart-Gough平台机构在特定姿态工作空间内的运动性能，需有效识别并量化其奇异位形之外的工作空间范围。

Method: 针对固定姿态解析计算SFS，通过对姿态工作空间内的多个采样点重复该过程，取其中最小SFS作为最终结果。

Result: 在四种不同SGPM构型上进行了数值实验，比较了它们在同一姿态工作空间下的SFS体积，验证了方法的有效性。

Conclusion: 所提出的方法可用于6-6 Stewart-Gough平台的分析与设计，有助于提升其在指定工作空间内的运动性能和可靠性。

Abstract: This article presents a method for computing the largest singularity-free
sphere (SFS) of a 6-6 Stewart-Gough platform manipulator (SGPM) over a
specified orientation workspace. For a fixed orientation of the moving
platform, the SFS is computed analytically. This process is repeated over a set
of samples generated within the orientation workspace, and the smallest among
them is designated as the desired SFS for the given orientation workspace.
Numerical experiments are performed on four distinct architectures of the SGPM
to understand their relative performances w.r.t. SFS volumes over the same
orientation workspace. This study demonstrates the potential utility of the
proposed computational method both in analysis and design of SGPMs.

</details>


### [97] [MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery](https://arxiv.org/abs/2511.05007)
*Baiye Cheng,Tianhai Liang,Suning Huang,Maanping Shao,Feihong Zhang,Botian Xu,Zhengrong Xue,Huazhe Xu*

Main category: cs.RO

TL;DR: 本文提出了一种基于专家混合机制的扩散策略（MoE-DP），通过在视觉编码器和扩散模型之间引入MoE层，实现对长周期、多阶段任务中不同阶段的专家分工，显著提升了策略在扰动下的鲁棒性和可解释性，并在模拟和真实环境中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在处理长周期、多阶段任务时缺乏从子任务失败中恢复的能力，且策略表征难以解释。因此需要提升其鲁棒性和可解释性。

Method: 在扩散策略中引入专家混合（MoE）层，将策略知识分解为多个专业化专家，根据任务阶段动态激活相应专家，从而实现更鲁棒和可解释的决策。

Result: 在6个长周期模拟任务中，MoE-DP在扰动条件下平均成功率相对提升36%，并在真实场景中表现出显著性能增益；同时模型学习到可解释的技能分解，不同专家对应语义化的任务原语（如接近、抓取）。

Conclusion: MoE-DP通过引入专家混合机制，有效增强了扩散策略在复杂任务中的鲁棒性和可解释性，且支持无需再训练的子任务重排，具备实际应用潜力。

Abstract: Diffusion policies have emerged as a powerful framework for robotic
visuomotor control, yet they often lack the robustness to recover from subtask
failures in long-horizon, multi-stage tasks and their learned representations
of observations are often difficult to interpret. In this work, we propose the
Mixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is
to insert a Mixture of Experts (MoE) layer between the visual encoder and the
diffusion model. This layer decomposes the policy's knowledge into a set of
specialized experts, which are dynamically activated to handle different phases
of a task. We demonstrate through extensive experiments that MoE-DP exhibits a
strong capability to recover from disturbances, significantly outperforming
standard baselines in robustness. On a suite of 6 long-horizon simulation
tasks, this leads to a 36% average relative improvement in success rate under
disturbed conditions. This enhanced robustness is further validated in the real
world, where MoE-DP also shows significant performance gains. We further show
that MoE-DP learns an interpretable skill decomposition, where distinct experts
correspond to semantic task primitives (e.g., approaching, grasping). This
learned structure can be leveraged for inference-time control, allowing for the
rearrangement of subtasks without any re-training.Our video and code are
available at the https://moe-dp-website.github.io/MoE-DP-Website/.

</details>


### [98] [Epically Powerful: An open-source software and mechatronics infrastructure for wearable robotic systems](https://arxiv.org/abs/2511.05033)
*Jennifer K. Leestma,Siddharth R. Nathella,Christoph P. O. Nuesslein,Snehil Mathur,Gregory S. Sawicki,Aaron J. Young*

Main category: cs.RO

TL;DR: Epically Powerful是一个开源的机器人基础设施，旨在简化可穿戴机器人系统的开发与部署，支持多种商用驱动器和传感器，提供硬件选型、系统组装和控制器实现的全面指南。


<details>
  <summary>Details</summary>
Motivation: 降低开发和部署定制化可穿戴机器人系统的门槛，提升研究效率。

Method: 提供统一的代码框架（基于Python），集成通信协议管理、传感器数据采集、数据日志记录、实时可视化等功能，并配套硬件兼容性指南和详细文档。

Result: 实现了对多种QDD驱动器、单板计算机和传感器的无缝接口，支持快速从硬件原型到模块化系统的转化。

Conclusion: Epically Powerful不仅适用于可穿戴机器人，还可广泛应用于其他使用QDD执行器和闭环控制的机器人领域，具有良好的通用性和扩展性。

Abstract: Epically Powerful is an open-source robotics infrastructure that streamlines
the underlying framework of wearable robotic systems - managing communication
protocols, clocking, actuator commands, visualization, sensor data acquisition,
data logging, and more - while also providing comprehensive guides for hardware
selection, system assembly, and controller implementation. Epically Powerful
contains a code base enabling simplified user implementation via Python that
seamlessly interfaces with various commercial state-of-the-art quasi-direct
drive (QDD) actuators, single-board computers, and common sensors, provides
example controllers, and enables real-time visualization. To further support
device development, the package also includes a recommended parts list and
compatibility guide and detailed documentation on hardware and software
implementation. The goal of Epically Powerful is to lower the barrier to
developing and deploying custom wearable robotic systems without a
pre-specified form factor, enabling researchers to go from raw hardware to
modular, robust devices quickly and effectively. Though originally designed
with wearable robotics in mind, Epically Powerful is broadly applicable to
other robotic domains that utilize QDD actuators, single-board computers, and
sensors for closed-loop control.

</details>


### [99] [TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments](https://arxiv.org/abs/2511.05052)
*Zihao Li,Yiming Zhu,Zhe Zhong,Qinyuan Ren,Yijiang Huang*

Main category: cs.RO

TL;DR: 提出了一种名为TAPOM的拓扑感知规划方法，用于解决机器人在狭窄空间中操作细长物体的难题，通过任务空间拓扑分析提高规划效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有规划方法在低 Clearance 场景中常因采样困难或陷入局部极小值而失败，难以有效处理复杂受限空间中的物体操作问题。

Method: TAPOM结合高层任务空间拓扑分析，识别关键路径并生成引导关键帧，辅助底层规划器搜索可行的构型空间轨迹。

Result: 实验表明，TAPOM在低 Clearance 操作任务中相比当前先进方法具有显著更高的成功率和效率。

Conclusion: TAPOM能有效提升机器人在复杂真实环境中执行受限空间操作的能力，具有广泛的应用前景。

Abstract: Robotic manipulation in complex, constrained spaces is vital for widespread
applications but challenging, particularly when navigating narrow passages with
elongated objects. Existing planning methods often fail in these low-clearance
scenarios due to the sampling difficulties or the local minima. This work
proposes Topology-Aware Planning for Object Manipulation (TAPOM), which
explicitly incorporates task-space topological analysis to enable efficient
planning. TAPOM uses a high-level analysis to identify critical pathways and
generate guiding keyframes, which are utilized in a low-level planner to find
feasible configuration space trajectories. Experimental validation demonstrates
significantly high success rates and improved efficiency over state-of-the-art
methods on low-clearance manipulation tasks. This approach offers broad
implications for enhancing manipulation capabilities of robots in complex
real-world environments.

</details>


### [100] [Decomposed Object Manipulation via Dual-Actor Policy](https://arxiv.org/abs/2511.05129)
*Bin Fan,Jianjian Jiang,Zhuohao Li,Yixiang He,Xiaoming Wu,Yihan Yang,Shengbang Liu,Weishi Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种双演员策略（DAP），通过引入基于功能性和运动流的视觉先验分别优化物体操作任务中的接近和操作阶段，并构建了包含多种任务的仿真数据集Dual-Prior Object Manipulation Dataset，实验表明该方法在多个基准上显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 先前的研究通常忽略物体操作任务中接近和操作两个阶段的区别，采用单一策略直接学习整个过程，导致效率和性能受限。因此，需要一种能够显式区分并优化这两个阶段的方法。

Method: 提出双演员策略（DAP），包括一个基于功能性的演员用于定位操作部件以优化接近阶段，一个基于运动流的演员用于捕捉组件运动以优化操作阶段，并设计决策模块判断当前阶段并选择相应演员；同时构建了一个融合两种视觉先验的仿真数据集Dual-Prior Object Manipulation Dataset。

Result: 在所构建的数据集、RoboTwin基准和真实场景上的实验结果显示，该方法平均分别比现有最先进方法提升5.55%、14.7%和10.4%。

Conclusion: DAP通过显式建模物体操作的不同阶段并利用异构视觉先验，有效提升了操作性能，且在模拟与真实环境中均表现出优越性，验证了其有效性与泛化能力。

Abstract: Object manipulation, which focuses on learning to perform tasks on similar
parts across different types of objects, can be divided into an approaching
stage and a manipulation stage. However, previous works often ignore this
characteristic of the task and rely on a single policy to directly learn the
whole process of object manipulation. To address this problem, we propose a
novel Dual-Actor Policy, termed DAP, which explicitly considers different
stages and leverages heterogeneous visual priors to enhance each stage.
Specifically, we introduce an affordance-based actor to locate the functional
part in the manipulation task, thereby improving the approaching process.
Following this, we propose a motion flow-based actor to capture the movement of
the component, facilitating the manipulation process. Finally, we introduce a
decision maker to determine the current stage of DAP and select the
corresponding actor. Moreover, existing object manipulation datasets contain
few objects and lack the visual priors needed to support training. To address
this, we construct a simulated dataset, the Dual-Prior Object Manipulation
Dataset, which combines the two visual priors and includes seven tasks,
including two challenging long-term, multi-stage tasks. Experimental results on
our dataset, the RoboTwin benchmark and real-world scenarios illustrate that
our method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4%
on average respectively.

</details>


### [101] [Procedimiento de auditoría de ciberseguridad para sistemas autónomos: metodología, amenazas y mitigaciones](https://arxiv.org/abs/2511.05185)
*Adrián Campazas-Vega,Claudia Álvarez-Aparicio,David Sobrín-Hidalgo,Laura Inyesto-Alonso,Francisco Javier Rodríguez-Lera,Vicente Matellán-Olivera,Ángel Manuel Guerrero-Higueras*

Main category: cs.RO

TL;DR: 本文提出了一种针对自主系统的分层安全审计方法，结合机器人领域的威胁分类和具体缓解措施，并通过四种代表性机器人平台的案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在工业、医疗、物流和家庭等关键领域广泛应用，其面临的安全威胁日益增加，尤其是在人机交互环境中，亟需系统性的安全审计方法来应对复杂的攻击面。

Method: 采用分层结构化的方法，构建适应机器人环境的威胁分类体系，并提出具体的缓解措施，形成一套专门针对自主系统的安全审计流程。

Result: 该方法在Ghost Robotics的Vision 60、Unitree Robotics的A1、Universal Robots的UR3和Aldebaran Robotics的Pepper四个典型机器人平台上进行了四个实际案例研究，验证了所提安全审计方法的有效性。

Conclusion: 所提出的基于分层结构和威胁分类的安全审计方法能够有效识别和缓解自主系统的安全风险，适用于复杂且高风险的机器人系统。

Abstract: The deployment of autonomous systems has experienced remarkable growth in
recent years, driven by their integration into sectors such as industry,
medicine, logistics, and domestic environments. This expansion is accompanied
by a series of security issues that entail significant risks due to the
critical nature of autonomous systems, especially those operating in
human-interaction environments. Furthermore, technological advancement and the
high operational and architectural complexity of autonomous systems have
resulted in an increased attack surface. This article presents a specific
security auditing procedure for autonomous systems, based on a layer-structured
methodology, a threat taxonomy adapted to the robotic context, and a set of
concrete mitigation measures. The validity of the proposed approach is
demonstrated through four practical case studies applied to representative
robotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1
robot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,
and the Pepper social robot from Aldebaran Robotics.

</details>


### [102] [Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation](https://arxiv.org/abs/2511.05199)
*Yichen Zhu,Feifei Feng*

Main category: cs.RO

TL;DR: 本文提出一种基于人类演示视频的机器人策略学习方法（RfV），通过检索相关视频并提取中层信息（如物体可操作区域和手部运动轨迹）来增强机器人在复杂环境中的操作能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 机器人在复杂不确定环境中面临挑战，传统方法依赖大量训练数据，而人类可通过观看视频学习新任务，因此希望借鉴这一方式提升机器人学习效率与适应性。

Method: 构建包含人类日常操作视频的视频库，提取物体affordance掩码和手部运动轨迹等中层信息；设计双组件系统：视频检索器根据任务描述从外部视频库中检索相关视频，策略生成器将检索到的知识融入策略学习过程。

Result: 在多个模拟和真实场景测试中，该系统显著优于传统机器人系统，展现出更强的任务泛化能力和适应性。

Conclusion: 通过从人类演示视频中检索知识进行类比学习，能有效提升机器人在未知任务中的表现，为机器人操作学习提供了新范式。

Abstract: Robots operating in complex and uncertain environments face considerable
challenges. Advanced robotic systems often rely on extensive datasets to learn
manipulation tasks. In contrast, when humans are faced with unfamiliar tasks,
such as assembling a chair, a common approach is to learn by watching video
demonstrations. In this paper, we propose a novel method for learning robot
policies by Retrieving-from-Video (RfV), using analogies from human
demonstrations to address manipulation tasks. Our system constructs a video
bank comprising recordings of humans performing diverse daily tasks. To enrich
the knowledge from these videos, we extract mid-level information, such as
object affordance masks and hand motion trajectories, which serve as additional
inputs to enhance the robot model's learning and generalization capabilities.
We further feature a dual-component system: a video retriever that taps into an
external video bank to fetch task-relevant video based on task specification,
and a policy generator that integrates this retrieved knowledge into the
learning cycle. This approach enables robots to craft adaptive responses to
various scenarios and generalize to tasks beyond those in the training data.
Through rigorous testing in multiple simulated and real-world settings, our
system demonstrates a marked improvement in performance over conventional
robotic systems, showcasing a significant breakthrough in the field of
robotics.

</details>


### [103] [Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning](https://arxiv.org/abs/2511.05234)
*Philipp Dahlinger,Niklas Freymuth,Tai Hoang,Tobias Würth,Michael Volpp,Luise Kärger,Gerhard Neumann*

Main category: cs.RO

TL;DR: 提出了一种基于轨迹级元学习的网格模拟方法M3GN，利用条件神经过程和运动原语实现快速、稳定且准确的模拟，相较于现有GNS方法在精度和运行时间上均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有学习型模拟器依赖单步观测和自回归 rollout，难以捕捉时间上下文、推断材料属性，且长期轨迹误差累积严重。

Method: 将网格模拟视为轨迹级元学习问题，采用条件神经过程进行快速适应，并利用运动原语通过单次模型调用直接预测完整运动轨迹。

Result: M3GN在多个任务上相比当前最先进的GNS方法实现了更高的模拟精度和更低的运行时间开销。

Conclusion: M3GN通过引入元学习和运动原语，有效提升了学习型物理模拟器的准确性、稳定性与效率，适用于机器人操作和制造优化等需要快速精确模拟的应用场景。

Abstract: Simulating object deformations is a critical challenge across many scientific
domains, including robotics, manufacturing, and structural mechanics. Learned
Graph Network Simulators (GNSs) offer a promising alternative to traditional
mesh-based physics simulators. Their speed and inherent differentiability make
them particularly well suited for applications that require fast and accurate
simulations, such as robotic manipulation or manufacturing optimization.
However, existing learned simulators typically rely on single-step
observations, which limits their ability to exploit temporal context. Without
this information, these models fail to infer, e.g., material properties.
Further, they rely on auto-regressive rollouts, which quickly accumulate error
for long trajectories. We instead frame mesh-based simulation as a
trajectory-level meta-learning problem. Using Conditional Neural Processes, our
method enables rapid adaptation to new simulation scenarios from limited
initial data while capturing their latent simulation properties. We utilize
movement primitives to directly predict fast, stable and accurate simulations
from a single model call. The resulting approach, Movement-primitive
Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of
the runtime cost compared to state-of-the-art GNSs across several tasks.

</details>


### [104] [TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models](https://arxiv.org/abs/2511.05275)
*Hokyun Im,Euijin Jeong,Jianlong Fu,Andrey Kolobov,Youngwoon Lee*

Main category: cs.RO

TL;DR: TwinVLA是一种模块化框架，通过组合两个预训练的单臂视觉-语言-动作模型（VLA），实现高效的双手机器人操作，无需额外的双臂数据训练，在真实和仿真环境中均表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于大多数公开数据集集中在单臂操作上，直接将视觉-语言-动作模型应用于双手机械任务需要大量额外的双臂数据和微调。为解决这一数据效率问题，提出TwinVLA以避免对专有双臂数据的依赖。

Method: TwinVLA采用模块化设计，将两个相同的预训练单臂VLA模型进行协调组合，通过共享语义理解并引入协同机制实现双臂协作，无需在双臂数据上重新预训练。

Result: 在多种真实与仿真环境中的双手机械任务上，TwinVLA优于同等规模的单体RDT-1B模型，且性能接近依赖大量专有数据的先进模型π₀，同时完全基于公开的单臂数据训练。

Conclusion: TwinVLA提供了一种数据高效、可扩展的方法，利用现有单臂VLA模型实现高性能双手机器人操作，为跨形态机器人学习提供了新思路。

Abstract: Vision-language-action models (VLAs) trained on large-scale robotic datasets
have demonstrated strong performance on manipulation tasks, including bimanual
tasks. However, because most public datasets focus on single-arm
demonstrations, adapting VLAs for bimanual tasks typically requires substantial
additional bimanual data and fine-tuning. To address this challenge, we
introduce TwinVLA, a modular framework that composes two copies of a pretrained
single-arm VLA into a coordinated bimanual VLA. Unlike monolithic
cross-embodiment models trained on mixtures of single-arm and bimanual data,
TwinVLA improves both data efficiency and performance by composing pretrained
single-arm policies. Across diverse bimanual tasks in real-world and simulation
settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model
without requiring any bimanual pretraining. Furthermore, it narrows the gap to
state-of-the-art model, $\pi_0$ which rely on extensive proprietary bimanual
data and compute cost. These results establish our modular composition approach
as a data-efficient and scalable path toward high-performance bimanual
manipulation, leveraging public single-arm data.

</details>


### [105] [Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators](https://arxiv.org/abs/2511.05307)
*Akua K. Dickson,Juan C. Pacheco Garcia,Andrew P. Sabelhaus*

Main category: cs.RO

TL;DR: 本文提出了一种将任务空间中的力安全准则映射到构型空间的框架，用于软体机器人在复杂环境中实时检测和避免过度接触力。


<details>
  <summary>Details</summary>
Motivation: 现有的避障方法未考虑软体机器人与脆弱障碍物接触时的力限制，可能导致损坏。

Method: 通过正向运动学将任务空间中的允许接触力限制映射到构型空间，并结合力安全准则实现实时力安全检测。

Result: 在仿真和实物实验中验证了该方法能准确检测与可变形障碍物交互时的力安全性。

Conclusion: 所提方法能够实时判断软体机械臂的安全构型，为在复杂、脆弱环境中实现安全规划奠定了基础。

Abstract: Soft robot manipulators have the potential for deployment in delicate
environments to perform complex manipulation tasks. However, existing obstacle
detection and avoidance methods do not consider limits on the forces that
manipulators may exert upon contact with delicate obstacles. This work
introduces a framework that maps force safety criteria from task space (i.e.
positions along the robot's body) to configuration space (i.e. the robot's
joint angles) and enables real-time force safety detection. We incorporate
limits on allowable environmental contact forces for given task-space
obstacles, and map them into configuration space (C-space) through the
manipulator's forward kinematics. This formulation ensures that configurations
classified as safe are provably below the maximum force thresholds, thereby
allowing us to determine force-safe configurations of the soft robot
manipulator in real-time. We validate our approach in simulation and hardware
experiments on a two-segment pneumatic soft robot manipulator. Results
demonstrate that the proposed method accurately detects force safety during
interactions with deformable obstacles, thereby laying the foundation for
real-time safe planning of soft manipulators in delicate, cluttered
environments.

</details>


### [106] [ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality](https://arxiv.org/abs/2511.05379)
*Eric Godden,Jacquie Groenewegen,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: ETHOS是一种动态遭遇型触觉显示系统，能够在虚拟现实中的社交互动（如传递、击拳和击掌）中实现自然的物理接触。


<details>
  <summary>Details</summary>
Motivation: 为了在虚拟现实中重现具有社会意义的触觉交互，提升用户在虚拟环境中的沉浸感和真实感。

Method: 系统结合了扭矩控制的机器人操纵器、可更换的被动道具（硅胶手模型和短棒）、基于标记的物理-虚拟注册以及安全监控机制，并提出了静态和动态两种控制策略。

Result: 静态模式下的位置精度为5.09±0.94 mm，用户交互的平均接触延迟为28.53±31.21 ms，验证了系统在VR中实现高质量人际触觉交互的可行性。

Conclusion: ETHOS通过集成关键的安全与控制机制，为虚拟环境中高保真、动态的人际交互提供了实用基础。

Abstract: We present ETHOS (Encountered-Type Haptics for On-demand Social Interaction),
a dynamic encountered-type haptic display (ETHD) that enables natural physical
contact in virtual reality (VR) during social interactions such as handovers,
fist bumps, and high-fives. The system integrates a torque-controlled robotic
manipulator with interchangeable passive props (silicone hand replicas and a
baton), marker-based physical-virtual registration via a ChArUco board, and a
safety monitor that gates motion based on the user's head and hand pose. We
introduce two control strategies: (i) a static mode that presents a stationary
prop aligned with its virtual counterpart, consistent with prior ETHD
baselines, and (ii) a dynamic mode that continuously updates prop position by
exponentially blending an initial mid-point trajectory with real-time hand
tracking, generating a unique contact point for each interaction. Bench tests
show static colocation accuracy of 5.09 +/- 0.94 mm, while user interactions
achieved temporal alignment with an average contact latency of 28.53 +/- 31.21
ms across all interaction and control conditions. These results demonstrate the
feasibility of recreating socially meaningful haptics in VR. By incorporating
essential safety and control mechanisms, ETHOS establishes a practical
foundation for high-fidelity, dynamic interpersonal interactions in virtual
environments.

</details>


### [107] [EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation](https://arxiv.org/abs/2511.05397)
*Samarth Chopra,Alex McMoil,Ben Carnovale,Evan Sokolson,Rajkumar Kubendran,Samuel Dickerson*

Main category: cs.RO

TL;DR: EverydayVLA是一个低成本（低于300美元）、6自由度的机械臂系统，结合先进的视觉-语言-动作模型，具备自适应重规划能力，在标准基准和真实场景中表现优异，显著提升了机器人基础模型在家庭和研究实验室中的可及性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型通常依赖昂贵硬件，且在新环境或杂乱场景中表现不佳，限制了其广泛应用。因此，需要一种低成本、可靠且适应性强的解决方案，以推动机器人技术在日常环境中的普及。

Method: 提出EverydayVLA，一种低成本、6自由度的机械臂系统；采用单一统一模型联合输出离散与连续动作，并引入自适应时间范围集成方法监控运动不确定性，实现动态重规划，提升操作安全性与鲁棒性。

Result: 在LIBERO基准上达到最先进水平的成功率；在真实世界测试中，分布内任务性能超越先前方法49%，分布外任务提升34.9%。

Conclusion: 通过将先进的视觉-语言-动作模型与低成本硬件结合，EverydayVLA有效降低了机器人基础模型的使用门槛，为家庭和科研场景提供了经济可行的解决方案，推动了机器人技术的普及化。

Abstract: While Vision-Language-Action (VLA) models map visual inputs and language
instructions directly to robot actions, they often rely on costly hardware and
struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF
manipulator that can be assembled for under $300, capable of modest payloads
and workspace. A single unified model jointly outputs discrete and continuous
actions, and our adaptive-horizon ensemble monitors motion uncertainty to
trigger on-the-fly re-planning for safe, reliable operation. On LIBERO,
EverydayVLA matches state-of-the-art success rates, and in real-world tests it
outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.
By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA
democratizes access to a robotic foundation model and paves the way for
economical use in homes and research labs alike. Experiment videos and details:
https://everydayvla.github.io/

</details>


### [108] [Stable and Robust SLIP Model Control via Energy Conservation-Based Feedback Cancellation for Quadrupedal Applications](https://arxiv.org/abs/2511.05402)
*Muhammad Saud Ul Hassan,Derek Vasquez,Hamza Asif,Christian Hubicki*

Main category: cs.RO

TL;DR: 本文提出了一种基于能量守恒的控制架构，用于实现四足机器人稳定的动态运动。通过将机器人建模为弹簧负载倒立摆（SLIP），并利用能量守恒原理设计控制算法，实现了稳定弹跳步态的跟踪。仿真结果表明该方法在存在传感器误差的情况下仍具有良好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了实现四足机器人在奔跑等动态运动中的稳定性，受生物四足动物运动特性的启发，需要一种能够有效建模和控制弹跳运动的控制架构。

Method: 将四足机器人建模为弹簧负载倒立摆（SLIP）模型，在飞行阶段控制腿的方向，在支撑阶段控制腿长，并基于能量守恒原理计算支撑阶段的稳定抛物线样条轨迹进行跟踪。

Result: 在Ghost Robotics Minitaur机器人的仿真中，所提出的控制算法成功生成了稳定的弹跳步态，并且在传感器测量存在高达10%误差的情况下仍能保持稳定，表现出良好的鲁棒性。

Conclusion: 基于能量守恒的SLIP模型控制方法能有效实现四足机器人稳定动态运动，具备较强鲁棒性，适用于生物启发式四足机器人系统。

Abstract: In this paper, we present an energy-conservation based control architecture
for stable dynamic motion in quadruped robots. We model the robot as a
Spring-loaded Inverted Pendulum (SLIP), a model well-suited to represent the
bouncing motion characteristic of running gaits observed in various biological
quadrupeds and bio-inspired robotic systems. The model permits leg-orientation
control during flight and leg-length control during stance, a design choice
inspired by natural quadruped behaviors and prevalent in robotic quadruped
systems. Our control algorithm uses the reduced-order SLIP dynamics of the
quadruped to track a stable parabolic spline during stance, which is calculated
using the principle of energy conservation. Through simulations based on the
design specifications of an actual quadruped robot, Ghost Robotics Minitaur, we
demonstrate that our control algorithm generates stable bouncing gaits.
Additionally, we illustrate the robustness of our controller by showcasing its
ability to maintain stable bouncing even when faced with up to a 10% error in
sensor measurements.

</details>


### [109] [Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience](https://arxiv.org/abs/2511.05426)
*Luca Girardi,Gabriel Maquignaz,Stefano Mintchev*

Main category: cs.RO

TL;DR: 本文提出了一种受生物启发的柔性框架四旋翼无人机FlexiQuad，兼具高机动性、抗碰撞能力和可压缩性，显著提升了复杂环境中无人机的飞行能力。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼无人机依赖刚性结构，限制了其在密集环境中的抗碰撞能力和通过狭窄通道的能力。受自然界生物体各向异性刚度和分布式质量-能量结构的启发，作者旨在设计一种能够兼顾敏捷飞行与物理鲁棒性的新型无人机结构。

Method: 提出并实现了一种名为FlexiQuad的软体框架四旋翼设计方法，采用具有各向异性刚度的柔性结构，通过机械设计与材料选择实现高顺应性，并进行动力学建模与实验测试验证其飞行性能、抗撞能力及压缩通过性。

Result: FlexiQuad原型重405克，顺应性比传统四旋翼高三个数量级，可实现超过80 km/h的速度和3g以上线加速度及300 rad/s²角加速度；能承受5 m/s正面撞击而不损坏，斜向碰撞反冲力降低39倍；机身可完全压缩，可通过仅为标称宽度70%的狭缝；分析确定了0.006至0.77 N/mm的最佳结构软度范围。

Conclusion: FlexiQuad在保持高飞行性能的同时，显著提升了抗碰撞能力和 squeezability，实现了敏捷性、柔顺性和鲁棒性的协同优化，为复杂环境中无人机的应用提供了新的设计范式。

Abstract: Natural flyers use soft wings to seamlessly enable a wide range of flight
behaviours, including agile manoeuvres, squeezing through narrow passageways,
and withstanding collisions. In contrast, conventional quadrotor designs rely
on rigid frames that support agile flight but inherently limit collision
resilience and squeezability, thereby constraining flight capabilities in
cluttered environments. Inspired by the anisotropic stiffness and distributed
mass-energy structures observed in biological organisms, we introduce
FlexiQuad, a soft-frame quadrotor design approach that limits this trade-off.
We demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more
compliant than conventional quadrotors, yet capable of acrobatic manoeuvres
with peak speeds above 80 km/h and linear and angular accelerations exceeding 3
g and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate
accelerations of rigid counterparts up to a thrust-to-weight ratio of 8.
Simultaneously, FlexiQuad exhibits fourfold higher collision resilience,
surviving frontal impacts at 5 m/s without damage and reducing destabilising
forces in glancing collisions by a factor of 39. Its frame can fully compress,
enabling flight through gaps as narrow as 70% of its nominal width. Our
analysis identifies an optimal structural softness range, from 0.006 to 0.77
N/mm, comparable to that of natural flyers' wings, whereby agility,
squeezability, and collision resilience are jointly achieved for FlexiQuad
models from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in
complex environments, enabling robust physical interactions without
compromising flight performance.

</details>
