<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本研究评估了不同规模的视觉-语言模型（SmolVLM2）在为盲人和低视力用户生成无障碍视频描述时的表现，提出了两个新的评估框架，并在智能手机上测试了模型的实际部署性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型虽然能力强，但资源消耗高，不利于盲人和低视力用户的实际使用；因此需要研究较小模型在无障碍场景下的表现和可行性。

Method: 采用SmolVLM2的5亿和22亿参数版本，在AVCaps和Charades数据集上进行评估；提出多上下文BLV框架和导航辅助框架两种新评估方法；系统比较四种提示设计策略，并在智能手机上测试FP32和INT8精度模型的运行表现。

Result: 研究发现较小的模型在特定提示策略下能有效生成高质量的无障碍描述，且在移动设备上具备可行的推理速度与资源占用表现，INT8量化对性能影响较小。

Conclusion: 轻量级视觉-语言模型结合优化的提示设计和量化技术，可在资源受限设备上为盲人和低视力用户提供实用、高效的视频描述服务，具备良好的实际应用前景。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 提出Self-Consistency Sampling（SCS）方法，通过引入视觉扰动和轨迹重采样生成一致性得分，有效解决多模态大语言模型在结果奖励强化学习中因错误推理链猜对答案而获得奖励的问题，在多个基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在多选设置下，结果奖励强化学习会因错误推理链猜中正确选项而给予相同奖励，导致训练不可靠，这一问题在多模态大语言模型中被忽视。

Method: 提出Self-Consistency Sampling（SCS），通过引入微小视觉扰动并重复截断与重采样初始推理轨迹，利用多条轨迹之间的一致性生成可微分的一致性得分，从而在策略更新时降低不可靠轨迹的权重。

Result: 在Qwen2.5-VL-7B-Instruct等模型上结合RLOO、GRPO和REINFORCE++方法，六个多模态基准上的准确率最高提升7.7个百分点，且计算开销极小；在更小和更大的模型上也取得显著增益。

Conclusion: SCS是一种简单且通用的方法，能有效缓解结果奖励强化学习中推理轨迹不忠实的问题，显著提升多模态大语言模型的推理可靠性与性能。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文提出了一种编码器增强的因果解码器模型架构，能够在有限硬件条件下实现比传统因果Transformer更高的语言压缩效率，并通过逐token熵估计提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于语言内在的信息熵限制，语言预测存在准确率上限和压缩下限。现有最高效的压缩算法是基于因果大模型的下一词预测，但其用于精确估算语言熵在计算上不可行。因此需要更高效的模型架构来有效逼近语言熵。

Method: 引入编码器增强的因果解码器模型架构，结合编码器的上下文理解能力和因果解码器的生成特性，在训练效率和压缩性能上进行优化；采用逐token熵估计方法，指导模型训练使其接近而非超过估计的熵值。

Result: 新模型在有限硬件上训练时表现出优于传统因果Transformer的压缩性能；实验证明，基于逐token熵估计训练的模型具有更强的泛化能力；当模型被训练至接近但不超过熵值时，其泛化性能优于未考虑熵的模型。

Conclusion: 通过引入编码器增强结构和基于熵的训练策略，可以在较低资源消耗下更有效地逼近语言的理论压缩极限，并提升模型的泛化性能，为语言模型的设计提供了新的方向。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [4] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 本文提出了一种名为苏格拉底式自我精炼（SSR）的新框架，用于细粒度评估和精确改进大语言模型（LLM）的推理能力。该方法通过将回答分解为可验证的子问题-子答案对，并进行逐步置信度估计与迭代优化，显著提升了复杂任务上的推理准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM测试时推理框架多依赖粗粒度的自我验证与修正，难以有效应对复杂推理任务，因此需要一种更精细、可控的自我改进机制。

Method: 提出Socratic Self-Refine（SSR）框架：将模型输出分解为（子问题，子答案）对，通过受控重解和自洽性检查进行步骤级置信度评估，并针对不可靠步骤进行迭代精炼。

Result: 在五个推理基准和三个大语言模型上的实验表明，SSR持续优于当前最先进的自 refinement 基线方法，在准确性和可解释性方面均有提升。

Conclusion: SSR提供了一种有效的黑箱方法，不仅提升了LLM的推理性能，还为理解和评估其内部推理过程提供了原则性途径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [5] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella 是一个完全开源的三 billion 参数语言模型系列，基于公开数据和代码库训练，在性能上达到同类开源模型的领先水平，并发布了支持长上下文和数学推理的专用变体。


<details>
  <summary>Details</summary>
Motivation: 大多数高性能大语言模型仍为闭源或部分开源，限制了研究的透明度与可复现性，因此需要一个完全开源且高性能的语言模型来推动开放研究。

Method: 通过大规模预训练、通用指令微调以及基于人类偏好对齐的方法，在 AMD Instinct MI300X GPU 上完成模型训练，并使用公开可用的数据和代码库；同时推出两个专用变体：Instella-Long（支持最长 128K token 的上下文）和 Instella-Math（通过监督微调和强化学习优化数学推理能力）。

Result: Instella 在较少预训练 token 的情况下，性能仍优于当前其他完全开源的同规模模型，且与领先的开源权重模型具有竞争力；其专用变体在长上下文和数学推理任务中表现突出。

Conclusion: Instella 作为一个完全开源、高性能且多功能的语言模型家族，提升了开源语言模型的透明度与可复现性，为开放语言建模研究提供了有力支持。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [6] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了一种名为生成对抗蒸馏（GAD）的新方法，用于在黑盒环境下对大语言模型进行策略内蒸馏，实验表明该方法优于传统的序列级知识蒸馏，使学生模型性能接近教师模型。


<details>
  <summary>Details</summary>
Motivation: 为了在无法访问教师模型内部参数或logits的黑盒条件下，更有效地从专有教师模型中蒸馏知识，提升学生模型的性能。

Method: 将学生模型视为生成器，训练一个判别器来区分学生与教师模型的输出，构建一个极小极大博弈；判别器作为随学生模型共同演化的策略内奖励模型，提供稳定且自适应的反馈。

Result: 实验结果显示，使用GAD训练的Qwen2.5-14B-Instruct学生模型在LMSYS-Chat自动评估中表现接近其教师模型GPT-5-Chat，并持续优于传统的序列级知识蒸馏方法。

Conclusion: GAD是一种有前景且高效的黑盒大语言模型蒸馏范式，能够在无内部信息访问权限的情况下实现高质量的知识迁移。

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [7] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: 提出了一种名为ParoQuant的权重量化方法，通过结合独立Givens旋转和逐通道缩放来均衡通道间幅度并缩小量化组内的动态范围，从而在保持低推理开销的同时提升大语言模型在推理任务上的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有权重量化方法难以有效抑制权重和激活中的异常值，导致在复杂推理链中误差累积严重，影响模型精度，尤其是在需要长链推理的大语言模型中问题更为突出。

Method: 提出Pairwise Rotation Quantization (ParoQuant)，采用硬件友好的可优化独立Givens旋转与逐通道缩放相结合的方法，并协同设计推理内核以充分利用GPU并行性，确保旋转和缩放在运行时轻量高效。

Result: 在推理任务上平均比AWQ提升2.4%的准确率，且推理开销低于10%。

Conclusion: ParoQuant为推理型大语言模型的高效、高精度部署提供了可行方案。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [8] [Robot Crash Course: Learning Soft and Stylized Falling](https://arxiv.org/abs/2511.10635)
*Pascal Strauch,David Müller,Sammy Christen,Agon Serifi,Ruben Grandia,Espen Knoop,Moritz Bächer*

Main category: cs.RO

TL;DR: 本文提出了一种机器人无关的奖励函数，通过强化学习在双足机器人跌倒时平衡目标姿态达成与冲击最小化，保护关键部件，并通过仿真采样策略实现对任意末端姿态的控制，实验证明该方法可在模拟和真实环境中实现受控、轻柔的跌倒。


<details>
  <summary>Details</summary>
Motivation: 尽管双足机器人在稳健行走方面取得了进展，但在现实世界中仍存在跌倒风险。现有研究多集中于防跌倒，而本文则关注跌倒过程本身，旨在减少跌倒时的物理损伤并允许用户控制机器人的最终姿态。

Method: 提出一种与具体机器人无关的奖励函数，结合目标末端姿态达成、冲击力最小化和关键部件保护，用于强化学习训练；引入基于仿真的初始和目标姿态采样策略，以增强策略对不同起始状态和任意末端姿态的鲁棒性。

Result: 在模拟和真实双足机器人上的实验表明，所提方法能够实现受控且柔和的跌倒，有效降低冲击并准确达到指定末端姿态。

Conclusion: 通过合理的奖励设计和仿真训练策略，双足机器人可以在跌倒过程中实现安全控制，为未来机器人在失衡情况下的安全应对提供了可行方案。

Abstract: Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.

</details>
